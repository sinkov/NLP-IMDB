{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_FCP_Group16.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "leBO0rM5Ujft",
        "exE0eZSwU8rU",
        "7NJSlOpB7_Kj",
        "AiAsTuKtXPKJ",
        "ZLKrO8epaJBG",
        "mxUqkXLXaJ2w",
        "8asGeptCbZNS",
        "qqSMIfJfbdey",
        "C5_1VbaDbtG0",
        "XzBiKAU6b1Oy",
        "VcutYwnMcVQo",
        "4nvYAZBRci8P",
        "oAySsr41ifuL",
        "seZ7_2Jzij1a",
        "GSfKqDDcizxQ",
        "T8Y79tgIjSJG",
        "ZBfCktKNjkPr",
        "UWHRomKJqFHa",
        "HrW7pgOKrEfi",
        "2LvDk6FrrI5r",
        "DmreQywisT9T"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Importing Libraries"
      ],
      "metadata": {
        "id": "leBO0rM5Ujft"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from nltk.tokenize import wordpunct_tokenize\n",
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import spacy\n",
        "from gensim import corpora\n",
        "import gensim\n",
        "from sklearn.metrics import f1_score\n",
        "import gensim.downloader as api\n",
        "from gensim.models import CoherenceModel\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "import re\n",
        "\n",
        "from requests import get\n",
        "from bs4 import BeautifulSoup as bs\n",
        "\n",
        "import codecs\n",
        "\n",
        "#Install missing libraries\n",
        "\n",
        "#!pip install torchtext\n",
        "!pip install ftfy\n",
        "#!pip install pyLDAvis\n",
        "import ftfy\n",
        "#import pyLDAvis.gensim_models as gensimvis\n",
        "#import pyLDAvis\n",
        "\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import time\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from torch.utils.data.dataset import random_split\n",
        "from torchtext.data.functional import to_map_style_dataset"
      ],
      "metadata": {
        "id": "oHcI63-JUqzx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b95150f3-a1ce-48ad-9215-ffa251d61426"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting ftfy\n",
            "  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n",
            "\u001b[K     |████████████████████████████████| 53 kB 1.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.7/dist-packages (from ftfy) (0.2.5)\n",
            "Installing collected packages: ftfy\n",
            "Successfully installed ftfy-6.1.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Clean the textual data and explore reviews\n"
      ],
      "metadata": {
        "id": "exE0eZSwU8rU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1 Opening the file and performing initial analysis\n",
        "* Open the file\n",
        "* Clean the unicode texts\n",
        "* Check the classes distribution\n",
        "* Check length distrubution of reviews"
      ],
      "metadata": {
        "id": "7NJSlOpB7_Kj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_uvu3LtUvyYp",
        "outputId": "e9998d62-668b-427c-d645-f13342dfad9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Paths\n",
        "train_data = 'drive/MyDrive/MSC/train.csv'\n",
        "test_data = 'drive/MyDrive/MSC/test.csv'"
      ],
      "metadata": {
        "id": "XimQTvCuUq50"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_reviews=pd.read_csv(train_data,encoding='utf-8',)"
      ],
      "metadata": {
        "id": "-e4VgmBOUq8s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining a function to deal with inconsistent unicode\n",
        "def unicoderemoval(x):\n",
        "  s= x.encode(\"utf-8\").decode(\"unicode_escape\")\n",
        "  t= ftfy.fix_text(s)\n",
        "  u = t.replace(\"\\n\",\" \")\n",
        "  return u"
      ],
      "metadata": {
        "id": "PZhUE9GbUq_n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Running the function\n",
        "pd.options.display.max_colwidth = 200\n",
        "df_reviews[\"reviews_clean\"] = df_reviews[\"imdb_user_review\"].apply(lambda x: unicoderemoval(x))\n",
        "df_reviews.head()"
      ],
      "metadata": {
        "id": "U-u5YOR0UrGF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337
        },
        "outputId": "921016d9-9b26-470c-96ac-4931ec7caf35"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   helpfulness_cat  \\\n",
              "0              1.0   \n",
              "1              1.0   \n",
              "2              1.0   \n",
              "3              1.0   \n",
              "4              1.0   \n",
              "\n",
              "                                                                                                                                                                                          imdb_user_review  \\\n",
              "0  It is hard to find such delightful and adorable movies these days as \\\"The Kid\\\". It is a silent movie but so rich, winning and beautiful that you don\\u00b4t nead words. Chaplin and 6-year old Coo...   \n",
              "1  They don't make films like this faded, haunting masterpiece of silent cinema anymore.When Dracula was first put on sale for movie rights; the one of the first men to grab it was F.W.Murnau one of ...   \n",
              "2  I first viewed this movie in 1924 at age 6 yrs--probably the first movie I ever saw.  I thought it was terrific then, and after viewing it again now(2003) I still think it is an exceptionally fine...   \n",
              "3  I doubt that I'd ever seen anything resembling a \\\"complete\\\" version of METROPOLIS before, though certain of its scenes were familiar to me, if only as used and abused in such films as Diane Keat...   \n",
              "4  I was shocked to find myself riveted to this movie. This is without a doubt the best sci-fi movie I've ever seen! Let me explain my position. We have all seen modern sci-fi movies, and argued over...   \n",
              "\n",
              "                                                                                                                                                                                             reviews_clean  \n",
              "0  It is hard to find such delightful and adorable movies these days as \"The Kid\". It is a silent movie but so rich, winning and beautiful that you don´t nead words. Chaplin and 6-year old Coogan are...  \n",
              "1  They don't make films like this faded, haunting masterpiece of silent cinema anymore.When Dracula was first put on sale for movie rights; the one of the first men to grab it was F.W.Murnau one of ...  \n",
              "2  I first viewed this movie in 1924 at age 6 yrs--probably the first movie I ever saw.  I thought it was terrific then, and after viewing it again now(2003) I still think it is an exceptionally fine...  \n",
              "3  I doubt that I'd ever seen anything resembling a \"complete\" version of METROPOLIS before, though certain of its scenes were familiar to me, if only as used and abused in such films as Diane Keaton...  \n",
              "4  I was shocked to find myself riveted to this movie. This is without a doubt the best sci-fi movie I've ever seen! Let me explain my position. We have all seen modern sci-fi movies, and argued over...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-23519cbe-f56c-4aab-8822-226bf74fbe53\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>helpfulness_cat</th>\n",
              "      <th>imdb_user_review</th>\n",
              "      <th>reviews_clean</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.0</td>\n",
              "      <td>It is hard to find such delightful and adorable movies these days as \\\"The Kid\\\". It is a silent movie but so rich, winning and beautiful that you don\\u00b4t nead words. Chaplin and 6-year old Coo...</td>\n",
              "      <td>It is hard to find such delightful and adorable movies these days as \"The Kid\". It is a silent movie but so rich, winning and beautiful that you don´t nead words. Chaplin and 6-year old Coogan are...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.0</td>\n",
              "      <td>They don't make films like this faded, haunting masterpiece of silent cinema anymore.When Dracula was first put on sale for movie rights; the one of the first men to grab it was F.W.Murnau one of ...</td>\n",
              "      <td>They don't make films like this faded, haunting masterpiece of silent cinema anymore.When Dracula was first put on sale for movie rights; the one of the first men to grab it was F.W.Murnau one of ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.0</td>\n",
              "      <td>I first viewed this movie in 1924 at age 6 yrs--probably the first movie I ever saw.  I thought it was terrific then, and after viewing it again now(2003) I still think it is an exceptionally fine...</td>\n",
              "      <td>I first viewed this movie in 1924 at age 6 yrs--probably the first movie I ever saw.  I thought it was terrific then, and after viewing it again now(2003) I still think it is an exceptionally fine...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.0</td>\n",
              "      <td>I doubt that I'd ever seen anything resembling a \\\"complete\\\" version of METROPOLIS before, though certain of its scenes were familiar to me, if only as used and abused in such films as Diane Keat...</td>\n",
              "      <td>I doubt that I'd ever seen anything resembling a \"complete\" version of METROPOLIS before, though certain of its scenes were familiar to me, if only as used and abused in such films as Diane Keaton...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.0</td>\n",
              "      <td>I was shocked to find myself riveted to this movie. This is without a doubt the best sci-fi movie I've ever seen! Let me explain my position. We have all seen modern sci-fi movies, and argued over...</td>\n",
              "      <td>I was shocked to find myself riveted to this movie. This is without a doubt the best sci-fi movie I've ever seen! Let me explain my position. We have all seen modern sci-fi movies, and argued over...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-23519cbe-f56c-4aab-8822-226bf74fbe53')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-23519cbe-f56c-4aab-8822-226bf74fbe53 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-23519cbe-f56c-4aab-8822-226bf74fbe53');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.hist(df_reviews['helpfulness_cat'])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "O41w4aJoUrI7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "a409a94b-8c70-408c-a0b5-77282bfefde3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#check the lenght distribution per review\n",
        "len_reviews_list=[len(rev.split()) for rev in df_reviews['reviews_clean']]\n",
        "plt.figure(figsize=(15,5))\n",
        "x=len_reviews_list\n",
        "hist, bins = np.histogram(x, bins=10)\n",
        "logbins = np.logspace(np.log10(bins[0]),np.log10(bins[-1]),len(bins))\n",
        "plt.hist(x, bins=logbins)\n",
        "plt.xscale('log')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3yG3ZHu9UrMH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 324
        },
        "outputId": "41d38a41-71ff-4823-b1b1-c4ef9964116e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1080x360 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_reviews['len_review']=x"
      ],
      "metadata": {
        "id": "yMuF-z4sUrPA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Average len of useful reviews is')\n",
        "print(np.average(df_reviews.loc[df_reviews.helpfulness_cat==1,'len_review']))\n",
        "print('Average len of unuseful reviews is')\n",
        "print(np.average(df_reviews.loc[df_reviews.helpfulness_cat==0,'len_review']))"
      ],
      "metadata": {
        "id": "JHPhXwm1UrRu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac805fd8-ef78-4116-a62c-b1708b744e9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average len of useful reviews is\n",
            "187.99963476990504\n",
            "Average len of unuseful reviews is\n",
            "134.19756001574183\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2 Display some of helpful and unhelpful reviews"
      ],
      "metadata": {
        "id": "nfdlkggDZ5CZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "k=0\n",
        "for i in df_reviews.loc[df_reviews.helpfulness_cat==0,'reviews_clean']:\n",
        "    print(i)\n",
        "    print('NEXT REVIEW')\n",
        "    k+=1\n",
        "    if k ==3:\n",
        "        break"
      ],
      "metadata": {
        "id": "GVo1amSOXDet",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fda156c6-bf3c-465c-cbd5-32ae3f89b44b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The movie is best described as pathetic. It may be good when people had nothing else, but not today. Not even after sound appeared in films. I wonder what the film is doing so high up in the ratings when it was a galactic effort to even sit through it. Metropolis is a complete waste of time. Ever since dialogues appeared in the films, there is not a single film that is considered good without good dialogues. Take any of the top films, even the IMDb top 250 list. There is not a single film that has won its place with run of the mill dialogues. And this one is without sound. Today's audience wants a story, strong dialogue, good acting and effects. The acting is stiff and out of fashion in Metroplis. The excessive use of expressions to make up for lack of sound may be understandable back then, but today it just looks a bit too funny. My recommendation: Don't watch it!\n",
            "NEXT REVIEW\n",
            "Well i was watching this movie for whatever reason. And it was so boring and the people don't even talk in it. and i don't see how film was entertaining in 1927. i never fall asleep on the couch but i did while watching metropolis. A rich guy wants to be a slave so he can be with a girl. Thats about the time i fell asleep when i woke up i put in Edward Scissorhands instead and That was a good movie and i didn't fall asleep to it. I don't know what else to type for ten lines i hate this stupid rule do i have ten lines yet no i do not ahhh! I need ten lines to finish this review i think i almost have ten lines i think i do!Botom Line In 1927 maybeNow a days Terrible movie who watches these kind of movies anymore!\n",
            "NEXT REVIEW\n",
            "This is A Film of A Conservative Man Who Is Stuck In The Past - Unlike Harold Lloyd who was open-minded and progressive and who eagerly produced his first talkie in 1929 - Chaplin continued to live in his fantasy world of the past and released a silent film in 1931 which could have been made in 1890 - The Synchronized Score could well have been replaced with a violin playing \"Hearts And Flowers\" because the story is as interesting as a revival of \"Uncle Tom's Cabin\" I give this a 1 - Don't bother watching this film - read a good book instead.\n",
            "NEXT REVIEW\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "k=0\n",
        "for i in df_reviews.loc[df_reviews.helpfulness_cat==1,'reviews_clean']:\n",
        "    print(i)\n",
        "    print('NEXT REVIEW')\n",
        "    k+=1\n",
        "    if k ==3:\n",
        "        break"
      ],
      "metadata": {
        "id": "Jhk9mVJraB7D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aedb2ca5-5dc2-4d1f-8111-1fc01ee5b73d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It is hard to find such delightful and adorable movies these days as \"The Kid\". It is a silent movie but so rich, winning and beautiful that you don´t nead words. Chaplin and 6-year old Coogan are possibly the most charming pair I have ever seen in my life. The film succees to be nicely light and full of joy but also overwhelmingly sad and sentimental. I always get my eyes full with tears of saddness and happiness. And I really never cry in movies. And the music...its simply Oscar-level! The movie is sympathetic, full of feeling, touchingly funny moments. It is truly a masterpiece showing how extraordinary talented person Charlie Chaplin was considering this film is respectively over 80 years old! So don´t you miss it because of it´s age, don´t miss it.\n",
            "NEXT REVIEW\n",
            "They don't make films like this faded, haunting masterpiece of silent cinema anymore.When Dracula was first put on sale for movie rights; the one of the first men to grab it was F.W.Murnau one of the most of the famous German directors of his time. By the time word got back to them about using the rights of the name and storyline of Dracula (Owned by the rights of Florence's widow.) Murnau had alread started production on the film; so to get around it they cut out the name 'Dracula' and replaced it with Count Orlok, Jonathan Harker became Hutter and Ban Helsing became Professor Bulwer; Orlock stalks the gothic streets of Bremen instead of Vistorian London.What is so different from Nosferatu and many of the others films of the time was that most of the film was shot on actually locations around Eastern Europe; the production hardly used any studio sets. What makes the most haunting feature tho is the sense of realism and the expressionism (most evident in the interiors od Orlok's Castle) that gives the film its hypnotic visual power.If there is any film a film student would need to have in his/her collection, it's this film. Although it is a hard task to find any surving copies. The reason for this is when the film was released Florence Stoker (widow of the author of Dracula) noticed the comparsion; she pursued the case relentlessly and in July 1925 a German court ordered all prints of the film to be destroyed. Luckily for us several prints of the film survived; a few in which have still been lost over the last few 8 decades.But thanks to the 2000 release of 'Shadow of a Vampire' a film  which looks behind the filming of Nosferatu and starring John Malkovich (F.W.Murnau) and Willem Dafoe (Count Orlok) the film was released for the first time on DVD in it's full original length of 94 minutes.Sadly soon after the film hit America in 1929; at the age of 43; Murnau was killed in a car crash.\"Men must die. Nosferatu does not die!\" proclaimed the original publcity for the film. We can only hope it's the truth for this film.\n",
            "NEXT REVIEW\n",
            "I first viewed this movie in 1924 at age 6 yrs--probably the first movie I ever saw.  I thought it was terrific then, and after viewing it again now(2003) I still think it is an exceptionally fine movie.  Many special effects without benefit of computers.  A very ambitious movie for that time.\n",
            "NEXT REVIEW\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.3 Visualising a Scattertext diagram \n",
        "This diagram shows words most used associated with the helpful and unhelpful review categories. This is an interactive visualisation"
      ],
      "metadata": {
        "id": "AiAsTuKtXPKJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scattertext"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 693
        },
        "id": "HPkXacKUXC9p",
        "outputId": "a27cf286-42a4-49a0-bf05-0b708da3b0fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting scattertext\n",
            "  Downloading scattertext-0.1.6-py3-none-any.whl (7.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.3 MB 3.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from scattertext) (1.0.2)\n",
            "Requirement already satisfied: statsmodels in /usr/local/lib/python3.7/dist-packages (from scattertext) (0.10.2)\n",
            "Collecting gensim>=4.0.0\n",
            "  Downloading gensim-4.2.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (24.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 24.1 MB 1.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from scattertext) (1.21.6)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from scattertext) (1.3.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from scattertext) (1.15.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from scattertext) (1.7.3)\n",
            "Collecting mock\n",
            "  Downloading mock-4.0.3-py3-none-any.whl (28 kB)\n",
            "Collecting flashtext\n",
            "  Downloading flashtext-2.7.tar.gz (14 kB)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.7/dist-packages (from gensim>=4.0.0->scattertext) (5.2.1)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->scattertext) (2022.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->scattertext) (2.8.2)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->scattertext) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->scattertext) (3.1.0)\n",
            "Requirement already satisfied: patsy>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from statsmodels->scattertext) (0.5.2)\n",
            "Building wheels for collected packages: flashtext\n",
            "  Building wheel for flashtext (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for flashtext: filename=flashtext-2.7-py2.py3-none-any.whl size=9309 sha256=4d9edf7b3499cdc10c2beadc673b0b0f27a4ca29c7ee008eafa9c94ec1743ebf\n",
            "  Stored in directory: /root/.cache/pip/wheels/cb/19/58/4e8fdd0009a7f89dbce3c18fff2e0d0fa201d5cdfd16f113b7\n",
            "Successfully built flashtext\n",
            "Installing collected packages: mock, gensim, flashtext, scattertext\n",
            "  Attempting uninstall: gensim\n",
            "    Found existing installation: gensim 3.6.0\n",
            "    Uninstalling gensim-3.6.0:\n",
            "      Successfully uninstalled gensim-3.6.0\n",
            "Successfully installed flashtext-2.7 gensim-4.2.0 mock-4.0.3 scattertext-0.1.6\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "gensim"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import scattertext as st\n",
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "from pprint import pprint"
      ],
      "metadata": {
        "id": "KTGaZnzhXDA3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df_reviews.iloc[: , [0, 1]].copy()"
      ],
      "metadata": {
        "id": "KWx8xj_aXDEq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"Cat\"] = df[\"helpfulness_cat\"].apply(lambda x: str(x))\n",
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 589
        },
        "id": "CBYjfGDkXDKH",
        "outputId": "ee56c6fb-1292-422f-dc79-68a02e050cd5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       helpfulness_cat  \\\n",
              "0                  1.0   \n",
              "1                  1.0   \n",
              "2                  1.0   \n",
              "3                  1.0   \n",
              "4                  1.0   \n",
              "...                ...   \n",
              "10750              0.0   \n",
              "10751              0.0   \n",
              "10752              0.0   \n",
              "10753              0.0   \n",
              "10754              0.0   \n",
              "\n",
              "                                                                                                                                                                                              imdb_user_review  \\\n",
              "0      It is hard to find such delightful and adorable movies these days as \\\"The Kid\\\". It is a silent movie but so rich, winning and beautiful that you don\\u00b4t nead words. Chaplin and 6-year old Coo...   \n",
              "1      They don't make films like this faded, haunting masterpiece of silent cinema anymore.When Dracula was first put on sale for movie rights; the one of the first men to grab it was F.W.Murnau one of ...   \n",
              "2      I first viewed this movie in 1924 at age 6 yrs--probably the first movie I ever saw.  I thought it was terrific then, and after viewing it again now(2003) I still think it is an exceptionally fine...   \n",
              "3      I doubt that I'd ever seen anything resembling a \\\"complete\\\" version of METROPOLIS before, though certain of its scenes were familiar to me, if only as used and abused in such films as Diane Keat...   \n",
              "4      I was shocked to find myself riveted to this movie. This is without a doubt the best sci-fi movie I've ever seen! Let me explain my position. We have all seen modern sci-fi movies, and argued over...   \n",
              "...                                                                                                                                                                                                        ...   \n",
              "10750  The makers of this movie really touched a sore spot in the grumpy old mans mind, namely the semioffficiaal corrupt system of professional legal guardianship in the united states of america, wher t...   \n",
              "10751  I Care A Lot is an exhilarating black comedy with a head-turning performance from Rosamund Pike. If I had to, I would pay 10 dollars for a ticket to see this in the theaters. Also, expect the unex...   \n",
              "10752                         Really loved this. This film is masterful in the way it makes you feel about all characters and the message about our westernised systems with plenty of laugh out loud moments.   \n",
              "10753                                                   The story, direction and acting across the board are superb. BUT Peter Dinklage is THE absolute star here IMHO.\\nLoved every minute, scene and nuance.   \n",
              "10754  This movie ruled! It had such a unique premise. The acting was awesome. Rosemund Pike channeled her inner Jordan Belforte throughout the whole movie. Dinklage was amazing. It's dark and funny. Hig...   \n",
              "\n",
              "       Cat  \n",
              "0      1.0  \n",
              "1      1.0  \n",
              "2      1.0  \n",
              "3      1.0  \n",
              "4      1.0  \n",
              "...    ...  \n",
              "10750  0.0  \n",
              "10751  0.0  \n",
              "10752  0.0  \n",
              "10753  0.0  \n",
              "10754  0.0  \n",
              "\n",
              "[10755 rows x 3 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-97538bc7-d545-4ec2-97ad-353a6d362bef\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>helpfulness_cat</th>\n",
              "      <th>imdb_user_review</th>\n",
              "      <th>Cat</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.0</td>\n",
              "      <td>It is hard to find such delightful and adorable movies these days as \\\"The Kid\\\". It is a silent movie but so rich, winning and beautiful that you don\\u00b4t nead words. Chaplin and 6-year old Coo...</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.0</td>\n",
              "      <td>They don't make films like this faded, haunting masterpiece of silent cinema anymore.When Dracula was first put on sale for movie rights; the one of the first men to grab it was F.W.Murnau one of ...</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.0</td>\n",
              "      <td>I first viewed this movie in 1924 at age 6 yrs--probably the first movie I ever saw.  I thought it was terrific then, and after viewing it again now(2003) I still think it is an exceptionally fine...</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.0</td>\n",
              "      <td>I doubt that I'd ever seen anything resembling a \\\"complete\\\" version of METROPOLIS before, though certain of its scenes were familiar to me, if only as used and abused in such films as Diane Keat...</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.0</td>\n",
              "      <td>I was shocked to find myself riveted to this movie. This is without a doubt the best sci-fi movie I've ever seen! Let me explain my position. We have all seen modern sci-fi movies, and argued over...</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10750</th>\n",
              "      <td>0.0</td>\n",
              "      <td>The makers of this movie really touched a sore spot in the grumpy old mans mind, namely the semioffficiaal corrupt system of professional legal guardianship in the united states of america, wher t...</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10751</th>\n",
              "      <td>0.0</td>\n",
              "      <td>I Care A Lot is an exhilarating black comedy with a head-turning performance from Rosamund Pike. If I had to, I would pay 10 dollars for a ticket to see this in the theaters. Also, expect the unex...</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10752</th>\n",
              "      <td>0.0</td>\n",
              "      <td>Really loved this. This film is masterful in the way it makes you feel about all characters and the message about our westernised systems with plenty of laugh out loud moments.</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10753</th>\n",
              "      <td>0.0</td>\n",
              "      <td>The story, direction and acting across the board are superb. BUT Peter Dinklage is THE absolute star here IMHO.\\nLoved every minute, scene and nuance.</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10754</th>\n",
              "      <td>0.0</td>\n",
              "      <td>This movie ruled! It had such a unique premise. The acting was awesome. Rosemund Pike channeled her inner Jordan Belforte throughout the whole movie. Dinklage was amazing. It's dark and funny. Hig...</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10755 rows × 3 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-97538bc7-d545-4ec2-97ad-353a6d362bef')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-97538bc7-d545-4ec2-97ad-353a6d362bef button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-97538bc7-d545-4ec2-97ad-353a6d362bef');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Turn data into a Scattertext Corpus \n",
        "corpus = st.CorpusFromPandas(df, \n",
        "                              category_col='Cat', \n",
        "                              text_col='imdb_user_review',\n",
        "                              nlp=nlp).build()"
      ],
      "metadata": {
        "id": "evKBDRhHXDM-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking the most frequent words/phrases for helpful reviews\n",
        "term_freq_df = corpus.get_term_freq_df()\n",
        "term_freq_df['Helpful Score'] = corpus.get_scaled_f_scores('1.0')\n",
        "pprint(list(term_freq_df.sort_values(by='Helpful Score', ascending=False).index[:10]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tLVA1jOAXDPn",
        "outputId": "6ea2055a-06c5-406b-aa60-c23cfb01bed6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/scattertext/termscoring/ScaledFScore.py:27: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  scores = np.zeros(len(cat_scores)).astype(np.float)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['performance',\n",
            " 'gives',\n",
            " 'perfect',\n",
            " 'wonderful',\n",
            " 'beautiful',\n",
            " 'role',\n",
            " 'brilliant',\n",
            " 'and his',\n",
            " 'of his',\n",
            " 'excellent']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking the most frequent words/phrases for unhelpful reviews\n",
        "term_freq_df['Unhelpful Score'] = corpus.get_scaled_f_scores('0.0')\n",
        "pprint(list(term_freq_df.sort_values(by='Unhelpful Score', ascending=False).index[:10]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T6FcxIHVXDSt",
        "outputId": "40acaf81-108e-49ae-8ca9-55ad05355d5a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/scattertext/termscoring/ScaledFScore.py:27: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  scores = np.zeros(len(cat_scores)).astype(np.float)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['bella',\n",
            " 'twilight',\n",
            " 'edward',\n",
            " 'vampires',\n",
            " 'boring',\n",
            " 'vampire',\n",
            " 'waste',\n",
            " 'waste of',\n",
            " 'oldboy',\n",
            " 'horrible']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "html = st.produce_scattertext_explorer(corpus,\n",
        "...          category='1.0',\n",
        "...          category_name='Helpful Reviews',\n",
        "...          not_category_name='Unhelpful Reviews',\n",
        "...          width_in_pixels=1000)"
      ],
      "metadata": {
        "id": "wy9K9_5CXDVo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "open(\"Convention-Visualization.html\", 'wb').write(html.encode('utf-8'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fSYAlHeCXDYN",
        "outputId": "6cd496ce-237a-42af-f9b9-f65b15dc54b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "15343900"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.4 Cleaning data"
      ],
      "metadata": {
        "id": "ZLKrO8epaJBG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 1: Evaluating whether to remove foreign language rows\n",
        "We decided not to, since some more poorly formatted reviews (generally classified as unhelpful) were getting misclassified as non-English, which could skew our results"
      ],
      "metadata": {
        "id": "mxUqkXLXaJ2w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langid"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "omiyhUYEaB-S",
        "outputId": "c47c5a57-e44b-4d76-b61d-57de9042bf8e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting langid\n",
            "  Downloading langid-1.1.6.tar.gz (1.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.9 MB 3.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from langid) (1.21.6)\n",
            "Building wheels for collected packages: langid\n",
            "  Building wheel for langid (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langid: filename=langid-1.1.6-py3-none-any.whl size=1941188 sha256=b04e5e02d58a5dbb46762630d7c8a91a92e709b70a13fa4b11df10db9b3882f1\n",
            "  Stored in directory: /root/.cache/pip/wheels/2b/bb/7f/11e4db39477278161e882eadc46fb558949a28b13470fc74b8\n",
            "Successfully built langid\n",
            "Installing collected packages: langid\n",
            "Successfully installed langid-1.1.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Identifying rows which are not in English\n",
        "\n",
        "import langid\n",
        "# Defining a function\n",
        "def lng(text):\n",
        "  a = langid.classify(text)[0]\n",
        "  return a\n",
        "\n",
        "df_reviews[\"imdb_user_review_lang\"] = df_reviews[\"reviews_clean\"].apply(lambda x: lng(x))"
      ],
      "metadata": {
        "id": "Q7nzs1n0aCBk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import langid\n",
        "# Defining a function\n",
        "def lng1(text):\n",
        "  b = langid.classify(text)[1]\n",
        "  return b\n",
        "\n",
        "df_reviews[\"imdb_user_review_lang_valn\"] = df_reviews[\"reviews_clean\"].apply(lambda x: lng1(x))"
      ],
      "metadata": {
        "id": "oLUXcQVSaCEg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.options.display.max_colwidth = 50\n",
        "df_reviews[df_reviews[\"imdb_user_review_lang\"]!='en']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 550
        },
        "id": "bWBUhYFtaCHE",
        "outputId": "0c785fe9-eef9-4ab1-8a08-275775b26798"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       helpfulness_cat                                   imdb_user_review  \\\n",
              "3819               1.0  Sarah ali khan n varun dhawan u morons....\\ud8...   \n",
              "3887               1.0  \\ud83d\\udd2a\\ud83d\\udd2a\\ud83d\\udd2a\\ud83d\\udd...   \n",
              "4102               1.0  Odlican film, istorijski tacan, Odlican film, ...   \n",
              "4167               1.0  The best movie ever, i like it a lot. Film je ...   \n",
              "5131               1.0  If your want to die see the movie \\ud83d\\udc4e...   \n",
              "8798               0.0  Netflix do it again, boring pseudo feminist no...   \n",
              "8843               1.0  An enjoyable movie going experience. Three che...   \n",
              "9152               1.0  A fun brain candy movie...good action...fun di...   \n",
              "9265               0.0  Finally a movie that says the truth \\u2764\\u27...   \n",
              "9385               1.0  Worst ever super hero movie\\ngal gadot cannot ...   \n",
              "9771               0.0  Superb movie, Akshay Kumar fabulous acting. Ri...   \n",
              "9996               0.0  Worst movie ever ,no acting ,no drama ,no stor...   \n",
              "10190              0.0  Tiger Shroff Hard Works, Best Thill, Great Act...   \n",
              "10670              0.0  Really fun addictive fun fun addictive game fu...   \n",
              "\n",
              "                                           reviews_clean  len_review  \\\n",
              "3819   Sarah ali khan n varun dhawan u morons....😡😡😡😡...          25   \n",
              "3887   🔪🔪🔪🔪🔪 DON'T GIVE YOUR PRECIOUS TIME TO THIS MO...           9   \n",
              "4102   Odlican film, istorijski tacan, Odlican film, ...           8   \n",
              "4167   The best movie ever, i like it a lot. Film je ...          21   \n",
              "5131   If your want to die see the movie 👎🏼👎🏼👎🏼👎🏼👎🏼👎🏼...           9   \n",
              "8798   Netflix do it again, boring pseudo feminist no...           8   \n",
              "8843   An enjoyable movie going experience. Three che...          10   \n",
              "9152   A fun brain candy movie...good action...fun di...          11   \n",
              "9265   Finally a movie that says the truth ❤❤❤❤❤ ❤❤❤❤...           9   \n",
              "9385   Worst ever super hero movie gal gadot cannot a...          13   \n",
              "9771   Superb movie, Akshay Kumar fabulous acting. Ri...          14   \n",
              "9996   Worst movie ever ,no acting ,no drama ,no stor...          10   \n",
              "10190  Tiger Shroff Hard Works, Best Thill, Great Act...          20   \n",
              "10670  Really fun addictive fun fun addictive game fu...          12   \n",
              "\n",
              "      imdb_user_review_lang  imdb_user_review_lang_valn  \n",
              "3819                     mt                 -257.906256  \n",
              "3887                     gu                   -1.082748  \n",
              "4102                     bs                  -50.347835  \n",
              "4167                     hr                 -239.128387  \n",
              "5131                     km                 -505.226149  \n",
              "8798                     pt                  -41.686171  \n",
              "8843                     af                 -115.578607  \n",
              "9152                     eu                  -78.615788  \n",
              "9265                     mr                 -299.976626  \n",
              "9385                     br                 -179.839990  \n",
              "9771                     id                 -109.300433  \n",
              "9996                     it                  -88.597518  \n",
              "10190                    rw                  -97.854469  \n",
              "10670                    cy                  -94.081874  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-2d337244-c92e-4a8c-8cea-6f21153e35fc\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>helpfulness_cat</th>\n",
              "      <th>imdb_user_review</th>\n",
              "      <th>reviews_clean</th>\n",
              "      <th>len_review</th>\n",
              "      <th>imdb_user_review_lang</th>\n",
              "      <th>imdb_user_review_lang_valn</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3819</th>\n",
              "      <td>1.0</td>\n",
              "      <td>Sarah ali khan n varun dhawan u morons....\\ud8...</td>\n",
              "      <td>Sarah ali khan n varun dhawan u morons....😡😡😡😡...</td>\n",
              "      <td>25</td>\n",
              "      <td>mt</td>\n",
              "      <td>-257.906256</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3887</th>\n",
              "      <td>1.0</td>\n",
              "      <td>\\ud83d\\udd2a\\ud83d\\udd2a\\ud83d\\udd2a\\ud83d\\udd...</td>\n",
              "      <td>🔪🔪🔪🔪🔪 DON'T GIVE YOUR PRECIOUS TIME TO THIS MO...</td>\n",
              "      <td>9</td>\n",
              "      <td>gu</td>\n",
              "      <td>-1.082748</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4102</th>\n",
              "      <td>1.0</td>\n",
              "      <td>Odlican film, istorijski tacan, Odlican film, ...</td>\n",
              "      <td>Odlican film, istorijski tacan, Odlican film, ...</td>\n",
              "      <td>8</td>\n",
              "      <td>bs</td>\n",
              "      <td>-50.347835</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4167</th>\n",
              "      <td>1.0</td>\n",
              "      <td>The best movie ever, i like it a lot. Film je ...</td>\n",
              "      <td>The best movie ever, i like it a lot. Film je ...</td>\n",
              "      <td>21</td>\n",
              "      <td>hr</td>\n",
              "      <td>-239.128387</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5131</th>\n",
              "      <td>1.0</td>\n",
              "      <td>If your want to die see the movie \\ud83d\\udc4e...</td>\n",
              "      <td>If your want to die see the movie 👎🏼👎🏼👎🏼👎🏼👎🏼👎🏼...</td>\n",
              "      <td>9</td>\n",
              "      <td>km</td>\n",
              "      <td>-505.226149</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8798</th>\n",
              "      <td>0.0</td>\n",
              "      <td>Netflix do it again, boring pseudo feminist no...</td>\n",
              "      <td>Netflix do it again, boring pseudo feminist no...</td>\n",
              "      <td>8</td>\n",
              "      <td>pt</td>\n",
              "      <td>-41.686171</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8843</th>\n",
              "      <td>1.0</td>\n",
              "      <td>An enjoyable movie going experience. Three che...</td>\n",
              "      <td>An enjoyable movie going experience. Three che...</td>\n",
              "      <td>10</td>\n",
              "      <td>af</td>\n",
              "      <td>-115.578607</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9152</th>\n",
              "      <td>1.0</td>\n",
              "      <td>A fun brain candy movie...good action...fun di...</td>\n",
              "      <td>A fun brain candy movie...good action...fun di...</td>\n",
              "      <td>11</td>\n",
              "      <td>eu</td>\n",
              "      <td>-78.615788</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9265</th>\n",
              "      <td>0.0</td>\n",
              "      <td>Finally a movie that says the truth \\u2764\\u27...</td>\n",
              "      <td>Finally a movie that says the truth ❤❤❤❤❤ ❤❤❤❤...</td>\n",
              "      <td>9</td>\n",
              "      <td>mr</td>\n",
              "      <td>-299.976626</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9385</th>\n",
              "      <td>1.0</td>\n",
              "      <td>Worst ever super hero movie\\ngal gadot cannot ...</td>\n",
              "      <td>Worst ever super hero movie gal gadot cannot a...</td>\n",
              "      <td>13</td>\n",
              "      <td>br</td>\n",
              "      <td>-179.839990</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9771</th>\n",
              "      <td>0.0</td>\n",
              "      <td>Superb movie, Akshay Kumar fabulous acting. Ri...</td>\n",
              "      <td>Superb movie, Akshay Kumar fabulous acting. Ri...</td>\n",
              "      <td>14</td>\n",
              "      <td>id</td>\n",
              "      <td>-109.300433</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9996</th>\n",
              "      <td>0.0</td>\n",
              "      <td>Worst movie ever ,no acting ,no drama ,no stor...</td>\n",
              "      <td>Worst movie ever ,no acting ,no drama ,no stor...</td>\n",
              "      <td>10</td>\n",
              "      <td>it</td>\n",
              "      <td>-88.597518</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10190</th>\n",
              "      <td>0.0</td>\n",
              "      <td>Tiger Shroff Hard Works, Best Thill, Great Act...</td>\n",
              "      <td>Tiger Shroff Hard Works, Best Thill, Great Act...</td>\n",
              "      <td>20</td>\n",
              "      <td>rw</td>\n",
              "      <td>-97.854469</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10670</th>\n",
              "      <td>0.0</td>\n",
              "      <td>Really fun addictive fun fun addictive game fu...</td>\n",
              "      <td>Really fun addictive fun fun addictive game fu...</td>\n",
              "      <td>12</td>\n",
              "      <td>cy</td>\n",
              "      <td>-94.081874</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2d337244-c92e-4a8c-8cea-6f21153e35fc')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-2d337244-c92e-4a8c-8cea-6f21153e35fc button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-2d337244-c92e-4a8c-8cea-6f21153e35fc');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Trying another package\n",
        "!pip install langdetect\n",
        "from langdetect import detect  \n",
        "\n",
        "languages = []\n",
        "\n",
        "for x in range(len(df_reviews[\"reviews_clean\"])):\n",
        "    languages.append(detect(df_reviews[\"reviews_clean\"][x]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5alhGpn0aCKF",
        "outputId": "52b481a0-9ee3-45f9-e0ee-fd3e8d34f701"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting langdetect\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[K     |████████████████████████████████| 981 kB 3.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from langdetect) (1.15.0)\n",
            "Building wheels for collected packages: langdetect\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993242 sha256=d7d8435ed0e2c6114f145586e6d5765e8c96c68b354fff6b21a7478315aa9004\n",
            "  Stored in directory: /root/.cache/pip/wheels/c5/96/8a/f90c59ed25d75e50a8c10a1b1c2d4c402e4dacfa87f3aff36a\n",
            "Successfully built langdetect\n",
            "Installing collected packages: langdetect\n",
            "Successfully installed langdetect-1.0.9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_reviews[\"imdb_user_review_langnew\"] = languages"
      ],
      "metadata": {
        "id": "UY1MP7uAaCNJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_reviews[df_reviews[\"imdb_user_review_langnew\"]!='en']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 726
        },
        "id": "fCChO8dRaCQL",
        "outputId": "add64773-990c-4ce7-e654-05d633ca6ac7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       helpfulness_cat                                   imdb_user_review  \\\n",
              "1992               1.0  HOW DARE YOU DISRESPECT A MOVIE WITH SO MANY I...   \n",
              "3819               1.0  Sarah ali khan n varun dhawan u morons....\\ud8...   \n",
              "4102               1.0  Odlican film, istorijski tacan, Odlican film, ...   \n",
              "4152               1.0  Great and amazing film.\\nSjajan film. Veoma va...   \n",
              "4167               1.0  The best movie ever, i like it a lot. Film je ...   \n",
              "4348               0.0  Very good movie i like it very much make sure ...   \n",
              "4490               0.0  Complete nonsense. Conspiracy at it's finest. ...   \n",
              "6336               0.0  Hahahaha omg the cgi is awful ahahahahahahaha ...   \n",
              "7591               0.0  That's very boring , and awful story,,,,,,,,z....   \n",
              "9331               1.0  Albert Einstein\\nIssac Newton\\nGalileo Galilei...   \n",
              "10046              1.0  Not relatable unless your parents own millions...   \n",
              "10472              0.0  Well... i liked it! Alot.. i mean, whats not t...   \n",
              "10670              0.0  Really fun addictive fun fun addictive game fu...   \n",
              "\n",
              "                                           reviews_clean  len_review  \\\n",
              "1992   HOW DARE YOU DISRESPECT A MOVIE WITH SO MANY I...          16   \n",
              "3819   Sarah ali khan n varun dhawan u morons....😡😡😡😡...          25   \n",
              "4102   Odlican film, istorijski tacan, Odlican film, ...           8   \n",
              "4152   Great and amazing film. Sjajan film. Veoma vaz...           9   \n",
              "4167   The best movie ever, i like it a lot. Film je ...          21   \n",
              "4348   Very good movie i like it very much make sure ...          13   \n",
              "4490   Complete nonsense. Conspiracy at it's finest. ...           7   \n",
              "6336   Hahahaha omg the cgi is awful ahahahahahahaha ...          10   \n",
              "7591   That's very boring , and awful story,,,,,,,,z....           7   \n",
              "9331   Albert Einstein Issac Newton Galileo Galilei S...          23   \n",
              "10046  Not relatable unless your parents own millions...           9   \n",
              "10472  Well... i liked it! Alot.. i mean, whats not t...          11   \n",
              "10670  Really fun addictive fun fun addictive game fu...          12   \n",
              "\n",
              "      imdb_user_review_lang  imdb_user_review_lang_valn  \\\n",
              "1992                     en                   -9.864063   \n",
              "3819                     mt                 -257.906256   \n",
              "4102                     bs                  -50.347835   \n",
              "4152                     en                  -66.773998   \n",
              "4167                     hr                 -239.128387   \n",
              "4348                     en                  -88.902386   \n",
              "4490                     en                  -36.642973   \n",
              "6336                     en                 -129.955987   \n",
              "7591                     en                  -47.131999   \n",
              "9331                     en                 -336.830939   \n",
              "10046                    en                  -92.136443   \n",
              "10472                    en                  -89.898836   \n",
              "10670                    cy                  -94.081874   \n",
              "\n",
              "      imdb_user_review_langnew  \n",
              "1992                        de  \n",
              "3819                        id  \n",
              "4102                        hr  \n",
              "4152                        hr  \n",
              "4167                        hr  \n",
              "4348                        af  \n",
              "4490                        fr  \n",
              "6336                        so  \n",
              "7591                        sk  \n",
              "9331                        id  \n",
              "10046                       fr  \n",
              "10472                       no  \n",
              "10670                       es  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-dc49e617-48a7-46ea-8043-67eaf5146fa7\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>helpfulness_cat</th>\n",
              "      <th>imdb_user_review</th>\n",
              "      <th>reviews_clean</th>\n",
              "      <th>len_review</th>\n",
              "      <th>imdb_user_review_lang</th>\n",
              "      <th>imdb_user_review_lang_valn</th>\n",
              "      <th>imdb_user_review_langnew</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1992</th>\n",
              "      <td>1.0</td>\n",
              "      <td>HOW DARE YOU DISRESPECT A MOVIE WITH SO MANY I...</td>\n",
              "      <td>HOW DARE YOU DISRESPECT A MOVIE WITH SO MANY I...</td>\n",
              "      <td>16</td>\n",
              "      <td>en</td>\n",
              "      <td>-9.864063</td>\n",
              "      <td>de</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3819</th>\n",
              "      <td>1.0</td>\n",
              "      <td>Sarah ali khan n varun dhawan u morons....\\ud8...</td>\n",
              "      <td>Sarah ali khan n varun dhawan u morons....😡😡😡😡...</td>\n",
              "      <td>25</td>\n",
              "      <td>mt</td>\n",
              "      <td>-257.906256</td>\n",
              "      <td>id</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4102</th>\n",
              "      <td>1.0</td>\n",
              "      <td>Odlican film, istorijski tacan, Odlican film, ...</td>\n",
              "      <td>Odlican film, istorijski tacan, Odlican film, ...</td>\n",
              "      <td>8</td>\n",
              "      <td>bs</td>\n",
              "      <td>-50.347835</td>\n",
              "      <td>hr</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4152</th>\n",
              "      <td>1.0</td>\n",
              "      <td>Great and amazing film.\\nSjajan film. Veoma va...</td>\n",
              "      <td>Great and amazing film. Sjajan film. Veoma vaz...</td>\n",
              "      <td>9</td>\n",
              "      <td>en</td>\n",
              "      <td>-66.773998</td>\n",
              "      <td>hr</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4167</th>\n",
              "      <td>1.0</td>\n",
              "      <td>The best movie ever, i like it a lot. Film je ...</td>\n",
              "      <td>The best movie ever, i like it a lot. Film je ...</td>\n",
              "      <td>21</td>\n",
              "      <td>hr</td>\n",
              "      <td>-239.128387</td>\n",
              "      <td>hr</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4348</th>\n",
              "      <td>0.0</td>\n",
              "      <td>Very good movie i like it very much make sure ...</td>\n",
              "      <td>Very good movie i like it very much make sure ...</td>\n",
              "      <td>13</td>\n",
              "      <td>en</td>\n",
              "      <td>-88.902386</td>\n",
              "      <td>af</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4490</th>\n",
              "      <td>0.0</td>\n",
              "      <td>Complete nonsense. Conspiracy at it's finest. ...</td>\n",
              "      <td>Complete nonsense. Conspiracy at it's finest. ...</td>\n",
              "      <td>7</td>\n",
              "      <td>en</td>\n",
              "      <td>-36.642973</td>\n",
              "      <td>fr</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6336</th>\n",
              "      <td>0.0</td>\n",
              "      <td>Hahahaha omg the cgi is awful ahahahahahahaha ...</td>\n",
              "      <td>Hahahaha omg the cgi is awful ahahahahahahaha ...</td>\n",
              "      <td>10</td>\n",
              "      <td>en</td>\n",
              "      <td>-129.955987</td>\n",
              "      <td>so</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7591</th>\n",
              "      <td>0.0</td>\n",
              "      <td>That's very boring , and awful story,,,,,,,,z....</td>\n",
              "      <td>That's very boring , and awful story,,,,,,,,z....</td>\n",
              "      <td>7</td>\n",
              "      <td>en</td>\n",
              "      <td>-47.131999</td>\n",
              "      <td>sk</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9331</th>\n",
              "      <td>1.0</td>\n",
              "      <td>Albert Einstein\\nIssac Newton\\nGalileo Galilei...</td>\n",
              "      <td>Albert Einstein Issac Newton Galileo Galilei S...</td>\n",
              "      <td>23</td>\n",
              "      <td>en</td>\n",
              "      <td>-336.830939</td>\n",
              "      <td>id</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10046</th>\n",
              "      <td>1.0</td>\n",
              "      <td>Not relatable unless your parents own millions...</td>\n",
              "      <td>Not relatable unless your parents own millions...</td>\n",
              "      <td>9</td>\n",
              "      <td>en</td>\n",
              "      <td>-92.136443</td>\n",
              "      <td>fr</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10472</th>\n",
              "      <td>0.0</td>\n",
              "      <td>Well... i liked it! Alot.. i mean, whats not t...</td>\n",
              "      <td>Well... i liked it! Alot.. i mean, whats not t...</td>\n",
              "      <td>11</td>\n",
              "      <td>en</td>\n",
              "      <td>-89.898836</td>\n",
              "      <td>no</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10670</th>\n",
              "      <td>0.0</td>\n",
              "      <td>Really fun addictive fun fun addictive game fu...</td>\n",
              "      <td>Really fun addictive fun fun addictive game fu...</td>\n",
              "      <td>12</td>\n",
              "      <td>cy</td>\n",
              "      <td>-94.081874</td>\n",
              "      <td>es</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-dc49e617-48a7-46ea-8043-67eaf5146fa7')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-dc49e617-48a7-46ea-8043-67eaf5146fa7 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-dc49e617-48a7-46ea-8043-67eaf5146fa7');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_reviews = df_reviews.drop(['len_review', 'imdb_user_review_lang', 'imdb_user_review_lang_valn', 'imdb_user_review_langnew' ], axis=1)\n",
        "df_reviews.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "jZSj4Y81aCTC",
        "outputId": "7a5a1dfc-535f-4774-9d88-5c1444bf9631"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   helpfulness_cat                                   imdb_user_review  \\\n",
              "0              1.0  It is hard to find such delightful and adorabl...   \n",
              "1              1.0  They don't make films like this faded, hauntin...   \n",
              "2              1.0  I first viewed this movie in 1924 at age 6 yrs...   \n",
              "3              1.0  I doubt that I'd ever seen anything resembling...   \n",
              "4              1.0  I was shocked to find myself riveted to this m...   \n",
              "\n",
              "                                       reviews_clean  \n",
              "0  It is hard to find such delightful and adorabl...  \n",
              "1  They don't make films like this faded, hauntin...  \n",
              "2  I first viewed this movie in 1924 at age 6 yrs...  \n",
              "3  I doubt that I'd ever seen anything resembling...  \n",
              "4  I was shocked to find myself riveted to this m...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b76fb2a8-2b6e-4c7f-b246-724a60640f3f\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>helpfulness_cat</th>\n",
              "      <th>imdb_user_review</th>\n",
              "      <th>reviews_clean</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.0</td>\n",
              "      <td>It is hard to find such delightful and adorabl...</td>\n",
              "      <td>It is hard to find such delightful and adorabl...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.0</td>\n",
              "      <td>They don't make films like this faded, hauntin...</td>\n",
              "      <td>They don't make films like this faded, hauntin...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.0</td>\n",
              "      <td>I first viewed this movie in 1924 at age 6 yrs...</td>\n",
              "      <td>I first viewed this movie in 1924 at age 6 yrs...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.0</td>\n",
              "      <td>I doubt that I'd ever seen anything resembling...</td>\n",
              "      <td>I doubt that I'd ever seen anything resembling...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.0</td>\n",
              "      <td>I was shocked to find myself riveted to this m...</td>\n",
              "      <td>I was shocked to find myself riveted to this m...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b76fb2a8-2b6e-4c7f-b246-724a60640f3f')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-b76fb2a8-2b6e-4c7f-b246-724a60640f3f button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-b76fb2a8-2b6e-4c7f-b246-724a60640f3f');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 2: Defining functions to clean aspects of the data"
      ],
      "metadata": {
        "id": "8asGeptCbZNS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1) Splitting up contractions to full words \n",
        "Eg- It's to it is"
      ],
      "metadata": {
        "id": "qqSMIfJfbdey"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install contractions\n",
        "import contractions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vq1qh3S2aCV8",
        "outputId": "1b70eedb-3ce4-479e-fa96-a431634e6013"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting contractions\n",
            "  Downloading contractions-0.1.72-py2.py3-none-any.whl (8.3 kB)\n",
            "Collecting textsearch>=0.0.21\n",
            "  Downloading textsearch-0.0.21-py2.py3-none-any.whl (7.5 kB)\n",
            "Collecting pyahocorasick\n",
            "  Downloading pyahocorasick-1.4.4-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (106 kB)\n",
            "\u001b[K     |████████████████████████████████| 106 kB 4.3 MB/s \n",
            "\u001b[?25hCollecting anyascii\n",
            "  Downloading anyascii-0.3.1-py3-none-any.whl (287 kB)\n",
            "\u001b[K     |████████████████████████████████| 287 kB 29.4 MB/s \n",
            "\u001b[?25hInstalling collected packages: pyahocorasick, anyascii, textsearch, contractions\n",
            "Successfully installed anyascii-0.3.1 contractions-0.1.72 pyahocorasick-1.4.4 textsearch-0.0.21\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cleaning up contractions\n",
        "def decontract(x):\n",
        "  return contractions.fix(x)"
      ],
      "metadata": {
        "id": "SlMQF8_eaCYz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "gSmPuNCQbswT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2) Removing URLs"
      ],
      "metadata": {
        "id": "C5_1VbaDbtG0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EQj4kEplJMum",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd10d349-8055-4bcf-edbb-e21cec69e9ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<input>:3: DeprecationWarning: invalid escape sequence \\S\n",
            "<input>:3: DeprecationWarning: invalid escape sequence \\S\n",
            "<input>:3: DeprecationWarning: invalid escape sequence \\S\n",
            "<ipython-input-34-e9520b07ce8f>:3: DeprecationWarning: invalid escape sequence \\S\n",
            "  to_match = [\"http\\S+\",\n"
          ]
        }
      ],
      "source": [
        "# Looking for strings containing http.// or https.//\n",
        "def urls(x):\n",
        "  to_match = [\"http\\S+\",\n",
        "            \"http?://.*?\\\\s+\"]\n",
        "  text = re.sub('|'.join(to_match), '', x, flags=re.MULTILINE)\n",
        "  return text"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3) Slang replacement"
      ],
      "metadata": {
        "id": "XzBiKAU6b1Oy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating list of links to scrape from Noslang site which is a dictionary of slangs\n",
        "link1 = 'https://www.noslang.com/dictionary/'\n",
        "extensions = '1abcdefghijklmnopqrstuvwxyz'\n",
        "\n",
        "link_list=[]\n",
        "for i in extensions:\n",
        "  a = link1+i\n",
        "  link_list.append(a)"
      ],
      "metadata": {
        "id": "tgR9jBp0aCek"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Scraping the webpages\n",
        "headers = {'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:79.0) Gecko/20100101 Firefox/79.0'}\n",
        "allpg = []\n",
        "for i in link_list:\n",
        "    url = i\n",
        "    response = get(url, headers=headers)\n",
        "    html_soup = bs(response.text, 'html.parser')\n",
        "    allpg.append(html_soup)"
      ],
      "metadata": {
        "id": "iZG9SS-vaCh_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Selecting the slangs\n",
        "slangs = {}\n",
        "for i in allpg:\n",
        "    for j in i.findAll('div', {'class':'dictonary-word'}):\n",
        "      title = j.find('a') ['name']\n",
        "      abbr = j.find('abbr')['title']\n",
        "      slangs.update({title : abbr})\n",
        "\n",
        "for i in list(slangs.items())[200:220]:\n",
        "  print(i)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cZXlbKmTaCk6",
        "outputId": "4bef1284-8f6f-4b30-e3d7-8f9f066e9507"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('afaicr4', 'as far as i can remember for')\n",
            "('afaics', 'as far as I can see')\n",
            "('afaict', 'as far as I can tell')\n",
            "('afaik', 'as far as I know')\n",
            "('afair', 'as far as I recall')\n",
            "('afaiu', 'As far as I understand')\n",
            "('afc', 'away from computer')\n",
            "('afcpmgo', 'away from computer parents may go on')\n",
            "('afg', 'away from game')\n",
            "('afk', 'away from keyboard')\n",
            "('afkb', 'away from keyboard')\n",
            "('agn', 'again')\n",
            "('ah', 'ass hole')\n",
            "('ahole', 'a**h**e')\n",
            "('ai', 'Artificial Intelligence')\n",
            "('aiadw', 'ALL IN A DAYS WORK')\n",
            "('aiamu', \"and I'm a monkey's uncle\")\n",
            "('aicmfp', 'and I claim my five pounds')\n",
            "('aight', 'Alright')\n",
            "('aightz', 'alright')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking for any 1 letter slang words/slang words that are real words so we don't accidentally replace anything that's not slang\n",
        "# 1- 1 letter slangs\n",
        "one_let = [i for i in slangs.keys() if len(i)==1]\n",
        "print(one_let)\n",
        "\n",
        "# 2- Checking if any of the words are real. If yes, checking if they need to be removed/replaced\n",
        "import nltk\n",
        "nltk.download('words')\n",
        "from nltk.corpus import words\n",
        "setofwords = set(words.words())\n",
        "\n",
        "words_real = [i for i in slangs if i in setofwords]\n",
        "print(words_real)\n",
        "\n",
        "dictsm = {}\n",
        "for i in range(len(words_real)):\n",
        "    dictsm.update({words_real[i]:[slangs[x] for x in words_real][i]} )\n",
        "print(dictsm)\n",
        "\n",
        "# Replacing few words from above logic\n",
        "slangs.pop('chronic')\n",
        "slangs.pop('dos')\n",
        "slangs.pop('hijack')\n",
        "slangs.pop('tomorrow')\n",
        "slangs.pop('2')\n",
        "slangs.pop('4')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "id": "sfnHex-aaCoN",
        "outputId": "f9e11ccb-f29f-488c-da33-bfc5b7adff07"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['2', '4', '@', 'c', 'k', 'm', 'n', 'o', 'r', 'u', 'y']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['ah', 'ai', 'aim', 'aka', 'alright', 'ama', 'Amos', 'anon', 'apod', 'ar', 'arse', 'ase', 'aslop', 'atop', 'bae', 'bah', 'beech', 'bibi', 'bion', 'biz', 'bo', 'bogo', 'bord', 'bout', 'bur', 'c', 'cam', 'cawk', 'char', 'chronic', 'chut', 'cig', 'cob', 'cod', 'col', 'coo', 'cos', 'coz', 'crunk', 'cubi', 'cud', 'curn', 'cwm', 'da', 'dah', 'dey', 'dick', 'din', 'dirl', 'dis', 'dit', 'dob', 'dod', 'don', 'dont', 'dos', 'dum', 'dun', 'dupe', 'em', 'ere', 'eta', 'ey', 'fam', 'fe', 'feat', 'feck', 'fi', 'fifo', 'flamer', 'fob', 'foo', 'fu', 'fub', 'fud', 'fut', 'ga', 'gaj', 'gal', 'gaw', 'gey', 'gim', 'goi', 'gol', 'grog', 'gud', 'gurl', 'guru', 'hak', 'han', 'hau', 'hijack', 'ho', 'homey', 'homo', 'hubby', 'hud', 'huggle', 'idly', 'imu', 'iso', 'iyo', 'k', 'ka', 'kay', 'kit', 'ko', 'kos', 'lak', 'lam', 'lan', 'lat', 'lata', 'leet', 'legit', 'lifo', 'ligas', 'lim', 'lof', 'luff', 'lug', 'lulab', 'lurker', 'ly', 'm', 'mao', 'mho', 'min', 'mir', 'mite', 'mor', 'moro', 'moss', 'mu', 'n', 'na', 'naa', 'natch', 'naw', 'nawt', 'ne', 'nei', 'ni', 'nib', 'nip', 'nu', 'nub', 'doge', 'o', 'oar', 'od', 'oe', 'orgy', 'os', 'ova', 'pah', 'paw', 'pic', 'pino', 'pir', 'pita', 'pix', 'plomb', 'po', 'pob', 'poi', 'pol', 'pow', 'pro', 'prob', 'prod', 'pst', 'r', 're', 'reefer', 'rep', 'rite', 'roger', 'roi', 'rox', 'rut', 'sab', 'sah', 'sec', 'seg', 'sh', 'shag', 'sho', 'shure', 'sic', 'sig', 'siol', 'sis', 'skeet', 'skewl', 'snafu', 'snew', 'snog', 'sob', 'soc', 'soe', 'sok', 'sol', 'sool', 'sop', 'sot', 'sow', 'st', 'sup', 'supa', 'supposably', 'ta', 'tai', 'tal', 'tat', 'tau', 'taw', 'tay', 'te', 'tha', 'thar', 'tho', 'tigger', 'til', 'tomorrow', 'tonite', 'tou', 'troll', 'tu', 'tuff', 'tut', 'twat', 'typo', 'tyt', 'u', 'ur', 'ura', 'ure', 'ut', 'uta', 'vag', 'wab', 'wad', 'wap', 'wat', 'waw', 'webby', 'wen', 'weve', 'wha', 'whit', 'wid', 'wit', 'woe', 'woft', 'wombat', 'wot', 'wud', 'wut', 'wy', 'wyn', 'y', 'ya', 'yapa', 'yarly', 'yas', 'yaw', 'ye', 'yea', 'yer', 'yew', 'ym', 'yn', 'yo', 'yor', 'yoy', 'yr', 'zig']\n",
            "{'ah': 'ass hole', 'ai': 'Artificial Intelligence', 'aim': 'AOL instant messanger', 'aka': 'also known as', 'alright': 'all right', 'ama': 'ask me anything', 'Amos': 'Add me on snapchat', 'anon': 'anonymous', 'apod': 'Another Point Of Discussion', 'ar': 'are', 'arse': 'ass', 'ase': 'age, sex, ethnicity', 'aslop': 'Age Sex Location Orientation Picture', 'atop': 'at time of posting', 'bae': 'before anyone else', 'bah': \"I don't really care\", 'beech': 'b***h', 'bibi': 'bye bye', 'bion': 'Believe it or not.', 'biz': 'Business', 'bo': 'body odour', 'bogo': 'buy one get one', 'bord': 'bored', 'bout': 'about', 'bur': 'p***y', 'c': 'see', 'cam': 'camera', 'cawk': 'c**k', 'char': 'character', 'chronic': 'marijuana', 'chut': 'p***y', 'cig': 'cigarette', 'cob': 'close of business', 'cod': 'Call of Duty', 'col': 'crying out loud', 'coo': 'cool', 'cos': 'because', 'coz': 'because', 'crunk': 'combination of crazy and drunk', 'cubi': 'can you believe it', 'cud': 'could', 'curn': 'calling you right now', 'cwm': 'come with me', 'da': 'the', 'dah': 'dumb as hell', 'dey': 'they', 'dick': 'penis', 'din': \"didn't\", 'dirl': 'Die in real life', 'dis': 'this', 'dit': 'Details in Thread', 'dob': 'date of birth', 'dod': 'Day of Defeat', 'don': 'denial of normal', 'dont': \"don't\", 'dos': 'denial of service', 'dum': 'dumb', 'dun': \"don't\", 'dupe': 'duplicate', 'em': 'them', 'ere': 'here', 'eta': 'Estimated Time of Arrival', 'ey': 'hey', 'fam': 'family', 'fe': 'fatal error', 'feat': 'Featuring', 'feck': 'f**k', 'fi': 'f**k it', 'fifo': 'first in, first out', 'flamer': 'angry poster', 'fob': 'fresh off the boat', 'foo': 'fool', 'fu': 'f**k you', 'fub': 'f**k you b***h', 'fud': 'fear, uncertainty and doubt', 'fut': 'f**k You Too', 'ga': 'go ahead', 'gaj': 'get a job', 'gal': 'get a life', 'gaw': 'grandparents are watching', 'gey': 'gay', 'gim': 'google instant messanger', 'goi': 'get over it ', 'gol': 'giggle out loud', 'grog': 'beer', 'gud': 'good', 'gurl': 'girl', 'guru': 'expert', 'hak': \"here's a kiss\", 'han': 'how about now', 'hau': 'How Are You', 'hijack': 'start an off topic discussion', 'ho': 'hold on', 'homey': 'Friend', 'homo': 'homosexual', 'hubby': 'husband', 'hud': 'Heads Up Display', 'huggle': 'hug and cuddle', 'idly': \"I don't like you\", 'imu': 'i miss you ', 'iso': 'In Search Of', 'iyo': 'in your opinion', 'k': 'ok', 'ka': 'Kick Ass', 'kay': 'okay', 'kit': 'keep in touch', 'ko': 'knock out', 'kos': 'kid over shoulder', 'lak': 'love and kisses', 'lam': 'leave a message', 'lan': 'local area network', 'lat': 'laugh at that', 'lata': 'later', 'leet': 'elite', 'legit': 'legitimate', 'lifo': 'last in first out', 'ligas': 'like I give a s**t', 'lim': 'Like it Matters', 'lof': 'Laughing on floor', 'luff': 'Love', 'lug': 'lesbian until graduation', 'lulab': 'Love you like a brother', 'lurker': \"one who reads but doesn't reply\", 'ly': 'love you', 'm': 'am', 'mao': 'my ass off', 'mho': 'My Humble Opinion', 'min': 'minute', 'mir': 'mom in room', 'mite': 'might', 'mor': 'more', 'moro': 'tomorrow', 'moss': 'member of same sex', 'mu': 'miss you', 'n': 'and', 'na': 'not applicable', 'naa': 'not at all', 'natch': 'naturally', 'naw': 'no', 'nawt': 'not', 'ne': 'any', 'nei': 'Not enough information', 'ni': 'no idea', 'nib': 'new in box', 'nip': 'nothing in particular', 'nu': 'new', 'nub': 'inexperienced person', 'doge': 'dog', 'o': 'Oh', 'oar': 'on a roll', 'od': 'over dose', 'oe': 'or else', 'orgy': 'orgasm', 'os': 'operating system', 'ova': 'over', 'pah': 'parents at home', 'paw': 'parents are watching', 'pic': 'picture', 'pino': 'Filipino', 'pir': 'parents in room', 'pita': 'pain in the ass', 'pix': 'pictures', 'plomb': 'parents looking over my back', 'po': 'piss off', 'pob': 'parent over back', 'poi': 'point of interest', 'pol': 'parent over looking', 'pow': 'prisoner of war', 'pro': 'professional', 'prob': 'problem', 'prod': 'product', 'pst': 'please send tell', 'r': 'are', 're': 'reply', 'reefer': 'marijuana', 'rep': 'to represent', 'rite': 'right', 'roger': 'affirmative', 'roi': 'Return On Investment', 'rox': 'rocks', 'rut': 'are you there', 'sab': 'slap a b***h', 'sah': 'sexy as hell', 'sec': 'second', 'seg': 's**t Eatin Grin', 'sh': 's**t happens', 'shag': 'f**k', 'sho': 'sure', 'shure': 'sure', 'sic': 'said in context', 'sig': 'Signature', 'siol': 'Shout It Out Loud', 'sis': 'sister', 'skeet': 'ejaculate', 'skewl': 'school', 'snafu': 'situation normal all f**ked up', 'snew': \"what's new\", 'snog': 'kiss', 'sob': 'son of a b***h', 'soc': 'Same old crap', 'soe': 'service oriented enterprise', 'sok': \"It's ok\", 'sol': 's**t outta luck', 'sool': 's**t out of luck', 'sop': 'same old place', 'sot': 'suck on this', 'sow': 'statement of work', 'st': 'Stop That', 'sup': \"what's up\", 'supa': 'super', 'supposably': 'Supposedly', 'ta': 'thanks again', 'tai': 'think about it', 'tal': 'thanks a lot', 'tat': 'that', 'tau': 'thinking about you', 'taw': 'Teachers are Watching', 'tay': 'thinking about you', 'te': 'Team effort', 'tha': 'the', 'thar': 'there', 'tho': 'though', 'tigger': 'tiger', 'til': 'until', 'tomorrow': '2moro', 'tonite': 'tonight', 'tou': 'thinking of you', 'troll': 'person who diliberately stirs up trouble', 'tu': 'thank you', 'tuff': 'tough', 'tut': 'take your time', 'twat': 'vagina', 'typo': 'typing mistake', 'tyt': 'take your time', 'u': 'you', 'ur': 'your', 'ura': 'you are a', 'ure': 'you are', 'ut': 'you there', 'uta': 'up the ass', 'vag': 'vagina', 'wab': 'what a b***h', 'wad': 'without a doubt', 'wap': 'wet ass p***y', 'wat': 'what', 'waw': 'what a w***e', 'webby': 'webcam', 'wen': 'when', 'weve': 'what ever', 'wha': 'what?', 'whit': 'with', 'wid': 'with', 'wit': 'with', 'woe': 'what on earth', 'woft': 'Waste of f**king time', 'wombat': 'waste of money, brains, and time', 'wot': 'what', 'wud': 'would', 'wut': 'what', 'wy': 'Why?', 'wyn': \"What's your name\", 'y': 'why', 'ya': 'yeah', 'yapa': 'yet another pointless acronym', 'yarly': 'yeah really', 'yas': 'you are stupid', 'yaw': 'you are welcome', 'ye': 'yeah', 'yea': 'yeah', 'yer': \"you're\", 'yew': 'you', 'ym': 'your mom', 'yn': 'why not', 'yo': 'year old', 'yor': 'your', 'yoy': 'why oh why', 'yr': 'year', 'zig': 'cigarette'}\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'for'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining a function to replace slangs\n",
        "def slang_words(text):\n",
        "    text = ' '.join([slangs.get(i,i) for i in text.split()])\n",
        "    return text"
      ],
      "metadata": {
        "id": "fAEA5jeuaCrT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 3: Creating a function to do all the cleaning, and running it"
      ],
      "metadata": {
        "id": "VcutYwnMcVQo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function\n",
        "def datacleaning(x):\n",
        "  x = x.lower()\n",
        "  x = x.replace('´',\"\")\n",
        "  x = decontract(x)\n",
        "  x = urls(x)\n",
        "  x = slang_words(x)\n",
        "  x = re.sub(r'[^\\w\\s]', '', x)\n",
        "  return x"
      ],
      "metadata": {
        "id": "A1Wn3kVpaCul"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Running the cleaning process\n",
        "df_reviews[\"reviews_clean\"] = df_reviews[\"reviews_clean\"].apply(lambda x: datacleaning(x))\n",
        "pd.options.display.max_colwidth = 200\n",
        "df_reviews.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 372
        },
        "id": "I5BtwSCbaCx2",
        "outputId": "dae09599-8435-4a43-8dc4-efa4579ca182"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   helpfulness_cat  \\\n",
              "0              1.0   \n",
              "1              1.0   \n",
              "2              1.0   \n",
              "3              1.0   \n",
              "4              1.0   \n",
              "\n",
              "                                                                                                                                                                                          imdb_user_review  \\\n",
              "0  It is hard to find such delightful and adorable movies these days as \\\"The Kid\\\". It is a silent movie but so rich, winning and beautiful that you don\\u00b4t nead words. Chaplin and 6-year old Coo...   \n",
              "1  They don't make films like this faded, haunting masterpiece of silent cinema anymore.When Dracula was first put on sale for movie rights; the one of the first men to grab it was F.W.Murnau one of ...   \n",
              "2  I first viewed this movie in 1924 at age 6 yrs--probably the first movie I ever saw.  I thought it was terrific then, and after viewing it again now(2003) I still think it is an exceptionally fine...   \n",
              "3  I doubt that I'd ever seen anything resembling a \\\"complete\\\" version of METROPOLIS before, though certain of its scenes were familiar to me, if only as used and abused in such films as Diane Keat...   \n",
              "4  I was shocked to find myself riveted to this movie. This is without a doubt the best sci-fi movie I've ever seen! Let me explain my position. We have all seen modern sci-fi movies, and argued over...   \n",
              "\n",
              "                                                                                                                                                                                             reviews_clean  \n",
              "0  it is hard to find such delightful and adorable movies these days as the kid it is a silent movie but so rich winning and beautiful that you do not nead words chaplin and 6year old coogan are poss...  \n",
              "1  they do not make films like this faded haunting masterpiece of silent cinema anymorewhen dracula was first put on sale for movie rights the one of the first men to grab it was fwmurnau one of the ...  \n",
              "2  i first viewed this movie in 1924 at age 6 yrsprobably the first movie i ever saw i thought it was terrific then and after viewing it again now2003 i still think it is an exceptionally fine movie ...  \n",
              "3  i doubt that i would ever seen anything resembling a complete version of metropolis before though certain of its scenes were familiar to me if only as used and abused in such films as diane keaton...  \n",
              "4  i was shocked to find myself riveted to this movie this is without a doubt the best scifi movie i have ever seen let me explain my position we have all seen modern scifi movies and argued over whi...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-7c3d6627-26c4-4b22-97ef-86c6a329c767\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>helpfulness_cat</th>\n",
              "      <th>imdb_user_review</th>\n",
              "      <th>reviews_clean</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.0</td>\n",
              "      <td>It is hard to find such delightful and adorable movies these days as \\\"The Kid\\\". It is a silent movie but so rich, winning and beautiful that you don\\u00b4t nead words. Chaplin and 6-year old Coo...</td>\n",
              "      <td>it is hard to find such delightful and adorable movies these days as the kid it is a silent movie but so rich winning and beautiful that you do not nead words chaplin and 6year old coogan are poss...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.0</td>\n",
              "      <td>They don't make films like this faded, haunting masterpiece of silent cinema anymore.When Dracula was first put on sale for movie rights; the one of the first men to grab it was F.W.Murnau one of ...</td>\n",
              "      <td>they do not make films like this faded haunting masterpiece of silent cinema anymorewhen dracula was first put on sale for movie rights the one of the first men to grab it was fwmurnau one of the ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.0</td>\n",
              "      <td>I first viewed this movie in 1924 at age 6 yrs--probably the first movie I ever saw.  I thought it was terrific then, and after viewing it again now(2003) I still think it is an exceptionally fine...</td>\n",
              "      <td>i first viewed this movie in 1924 at age 6 yrsprobably the first movie i ever saw i thought it was terrific then and after viewing it again now2003 i still think it is an exceptionally fine movie ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.0</td>\n",
              "      <td>I doubt that I'd ever seen anything resembling a \\\"complete\\\" version of METROPOLIS before, though certain of its scenes were familiar to me, if only as used and abused in such films as Diane Keat...</td>\n",
              "      <td>i doubt that i would ever seen anything resembling a complete version of metropolis before though certain of its scenes were familiar to me if only as used and abused in such films as diane keaton...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.0</td>\n",
              "      <td>I was shocked to find myself riveted to this movie. This is without a doubt the best sci-fi movie I've ever seen! Let me explain my position. We have all seen modern sci-fi movies, and argued over...</td>\n",
              "      <td>i was shocked to find myself riveted to this movie this is without a doubt the best scifi movie i have ever seen let me explain my position we have all seen modern scifi movies and argued over whi...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7c3d6627-26c4-4b22-97ef-86c6a329c767')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-7c3d6627-26c4-4b22-97ef-86c6a329c767 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-7c3d6627-26c4-4b22-97ef-86c6a329c767');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.5 Visualising similarity of items classified as helpful and unhelpful"
      ],
      "metadata": {
        "id": "4nvYAZBRci8P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1- Splitting data to train-test\n",
        "train, test = train_test_split(df_reviews, test_size=0.5, random_state=42)"
      ],
      "metadata": {
        "id": "rjvkcZyzaC0v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2- (1) Doc2Vec modelling: Tokenizing the articles and storing them in a list\n",
        "tkn_quotes = [wordpunct_tokenize(quote.lower()) for quote in train[\"reviews_clean\"]] \n",
        "tgd_quotes = [TaggedDocument(d, [i]) for i, d in enumerate(tkn_quotes)] "
      ],
      "metadata": {
        "id": "2vSVTmfecd7m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2- (2) Running the Doc2Vec model\n",
        "model = Doc2Vec(\n",
        "        tgd_quotes, vector_size=20, window=2, min_count=1, workers=4, epochs=100)\n",
        "model.save(\"quote_embedding.model\")\n",
        "model = Doc2Vec.load(\"quote_embedding.model\")"
      ],
      "metadata": {
        "id": "hAWlvIFKcd-s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2- (3) For each of the test documents, retrieving most similar document\n",
        "inferred_vectors = []\n",
        "tkn_quotes1 = [wordpunct_tokenize(quote.lower()) for quote in test[\"reviews_clean\"]] \n",
        "for i in tkn_quotes1:\n",
        "  v1 = model.infer_vector(i)\n",
        "  inferred_vectors.append(v1)"
      ],
      "metadata": {
        "id": "aHidcDJlceBW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2- (4) Converting to a dataframe\n",
        "inferred_vectors_df = pd.DataFrame(inferred_vectors)\n",
        "inferred_vectors_df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 270
        },
        "id": "ZzXhMHH0ceD8",
        "outputId": "2b3d9736-b6cb-48fc-f0dd-0e0e78532286"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         0         1         2         3         4         5         6   \\\n",
              "0 -0.247469 -0.180134 -0.229946 -0.611609 -0.275779  0.112298  1.129198   \n",
              "1  0.694537  0.407601  0.184554 -0.442458 -0.932213 -0.489336  0.646730   \n",
              "2 -0.375806 -0.627245  0.134382  3.948321 -0.078158 -0.881517 -1.589320   \n",
              "3 -0.664103  0.696402 -2.413806  0.871140 -0.194502  2.184351  0.328963   \n",
              "4 -0.021593 -1.047626  0.018786  0.426711  0.913319 -1.052881 -0.018104   \n",
              "\n",
              "         7         8         9         10        11        12        13  \\\n",
              "0  0.262561  1.071896  0.889572 -1.730346  1.396811 -1.609798 -1.058972   \n",
              "1 -1.788073  0.253727  0.415946  0.726573  1.785896 -0.023968 -2.842075   \n",
              "2  2.441573 -0.123849 -0.958821 -0.984414  3.007344  0.337775 -2.316378   \n",
              "3 -1.662486  1.295628 -1.531772  0.075926  3.263682 -1.340128 -2.463036   \n",
              "4 -1.611532 -0.360593 -2.458270 -0.272210  2.779272  1.249830 -0.747383   \n",
              "\n",
              "         14        15        16        17        18        19  \n",
              "0 -0.535566  3.594897 -0.839244 -1.474938  0.017007 -2.276265  \n",
              "1  0.470695  1.782314  2.301076 -2.094283 -0.614423 -1.126409  \n",
              "2 -0.507108  2.808970  4.165591 -0.488247 -0.500836 -2.015362  \n",
              "3  1.062065  3.531630  0.600726 -1.744627 -0.167427  0.014745  \n",
              "4  0.171139  1.566171  2.803674 -2.539784 -1.109939 -2.431795  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-0bc82b2d-ebbd-411c-af7c-76532b513b97\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-0.247469</td>\n",
              "      <td>-0.180134</td>\n",
              "      <td>-0.229946</td>\n",
              "      <td>-0.611609</td>\n",
              "      <td>-0.275779</td>\n",
              "      <td>0.112298</td>\n",
              "      <td>1.129198</td>\n",
              "      <td>0.262561</td>\n",
              "      <td>1.071896</td>\n",
              "      <td>0.889572</td>\n",
              "      <td>-1.730346</td>\n",
              "      <td>1.396811</td>\n",
              "      <td>-1.609798</td>\n",
              "      <td>-1.058972</td>\n",
              "      <td>-0.535566</td>\n",
              "      <td>3.594897</td>\n",
              "      <td>-0.839244</td>\n",
              "      <td>-1.474938</td>\n",
              "      <td>0.017007</td>\n",
              "      <td>-2.276265</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.694537</td>\n",
              "      <td>0.407601</td>\n",
              "      <td>0.184554</td>\n",
              "      <td>-0.442458</td>\n",
              "      <td>-0.932213</td>\n",
              "      <td>-0.489336</td>\n",
              "      <td>0.646730</td>\n",
              "      <td>-1.788073</td>\n",
              "      <td>0.253727</td>\n",
              "      <td>0.415946</td>\n",
              "      <td>0.726573</td>\n",
              "      <td>1.785896</td>\n",
              "      <td>-0.023968</td>\n",
              "      <td>-2.842075</td>\n",
              "      <td>0.470695</td>\n",
              "      <td>1.782314</td>\n",
              "      <td>2.301076</td>\n",
              "      <td>-2.094283</td>\n",
              "      <td>-0.614423</td>\n",
              "      <td>-1.126409</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-0.375806</td>\n",
              "      <td>-0.627245</td>\n",
              "      <td>0.134382</td>\n",
              "      <td>3.948321</td>\n",
              "      <td>-0.078158</td>\n",
              "      <td>-0.881517</td>\n",
              "      <td>-1.589320</td>\n",
              "      <td>2.441573</td>\n",
              "      <td>-0.123849</td>\n",
              "      <td>-0.958821</td>\n",
              "      <td>-0.984414</td>\n",
              "      <td>3.007344</td>\n",
              "      <td>0.337775</td>\n",
              "      <td>-2.316378</td>\n",
              "      <td>-0.507108</td>\n",
              "      <td>2.808970</td>\n",
              "      <td>4.165591</td>\n",
              "      <td>-0.488247</td>\n",
              "      <td>-0.500836</td>\n",
              "      <td>-2.015362</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-0.664103</td>\n",
              "      <td>0.696402</td>\n",
              "      <td>-2.413806</td>\n",
              "      <td>0.871140</td>\n",
              "      <td>-0.194502</td>\n",
              "      <td>2.184351</td>\n",
              "      <td>0.328963</td>\n",
              "      <td>-1.662486</td>\n",
              "      <td>1.295628</td>\n",
              "      <td>-1.531772</td>\n",
              "      <td>0.075926</td>\n",
              "      <td>3.263682</td>\n",
              "      <td>-1.340128</td>\n",
              "      <td>-2.463036</td>\n",
              "      <td>1.062065</td>\n",
              "      <td>3.531630</td>\n",
              "      <td>0.600726</td>\n",
              "      <td>-1.744627</td>\n",
              "      <td>-0.167427</td>\n",
              "      <td>0.014745</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-0.021593</td>\n",
              "      <td>-1.047626</td>\n",
              "      <td>0.018786</td>\n",
              "      <td>0.426711</td>\n",
              "      <td>0.913319</td>\n",
              "      <td>-1.052881</td>\n",
              "      <td>-0.018104</td>\n",
              "      <td>-1.611532</td>\n",
              "      <td>-0.360593</td>\n",
              "      <td>-2.458270</td>\n",
              "      <td>-0.272210</td>\n",
              "      <td>2.779272</td>\n",
              "      <td>1.249830</td>\n",
              "      <td>-0.747383</td>\n",
              "      <td>0.171139</td>\n",
              "      <td>1.566171</td>\n",
              "      <td>2.803674</td>\n",
              "      <td>-2.539784</td>\n",
              "      <td>-1.109939</td>\n",
              "      <td>-2.431795</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0bc82b2d-ebbd-411c-af7c-76532b513b97')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-0bc82b2d-ebbd-411c-af7c-76532b513b97 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-0bc82b2d-ebbd-411c-af7c-76532b513b97');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2- (5) Reducing dimensions to 2 using Multidimensional scaling\n",
        "from sklearn.manifold import MDS\n",
        "embedding = MDS(n_components=2, random_state=5)\n",
        "inferred_vectors_df_transformed = embedding.fit_transform(inferred_vectors_df)\n",
        "inferred_vectors_df_transformed.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "cKzl1MuZceGw",
        "outputId": "f0bfbd7e-7e48-4776-cd26-14437ae9fcee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-47-4270b208dae6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmanifold\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMDS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0membedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMDS\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0minferred_vectors_df_transformed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membedding\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minferred_vectors_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0minferred_vectors_df_transformed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/manifold/_mds.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y, init)\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0meps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m             \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 545\u001b[0;31m             \u001b[0mreturn_n_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    546\u001b[0m         )\n\u001b[1;32m    547\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/manifold/_mds.py\u001b[0m in \u001b[0;36msmacof\u001b[0;34m(dissimilarities, metric, n_components, init, n_init, n_jobs, max_iter, verbose, eps, random_state, return_n_iter)\u001b[0m\n\u001b[1;32m    275\u001b[0m                 \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m                 \u001b[0meps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m                 \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m             )\n\u001b[1;32m    279\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbest_stress\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mstress\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mbest_stress\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/manifold/_mds.py\u001b[0m in \u001b[0;36m_smacof_single\u001b[0;34m(dissimilarities, metric, n_components, init, max_iter, verbose, eps, random_state)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0;31m# Compute stress\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m         \u001b[0mstress\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mdisparities\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;31m# Update X using the Guttman transform\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2- (6) Converting the above to a dataframe and adding outlet and topic details\n",
        "inferred_vectors_df1 = pd.DataFrame(inferred_vectors_df_transformed)\n",
        "inferred_vectors_df1[\"cat\"]= test[\"helpfulness_cat\"].values\n",
        "inferred_vectors_df1.columns = [\"x\", \"y\", \"cat\"]\n",
        "inferred_vectors_df1.head()"
      ],
      "metadata": {
        "id": "UIXhWfYIceJQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3- Visualising the similarities\n",
        "import plotly.express as px\n",
        "\n",
        "fig = px.scatter(inferred_vectors_df1, x=\"x\", y=\"y\", color=\"cat\", title=\"Review similarity\", )\n",
        "\n",
        "fig.update_traces(opacity=0.6)\n",
        "\n",
        "fig.update_layout(\n",
        "    autosize=False,\n",
        "    width=700,\n",
        "    height=700,)\n",
        "\n",
        "fig.update_layout(\n",
        "    xaxis_title=\"Dimension 1\", yaxis_title=\"Dimension 2\")\n",
        "\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "zHg14rkhceL3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Applying different tokenization and vectorization techniques to use obtained vectors as inputs for predicting useful / unuseful labels using Logistic Regression"
      ],
      "metadata": {
        "id": "oAySsr41ifuL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1 Defining functions for text tokenization and vectorization\n",
        "* tokenize_wordpunkt\n",
        "* vectorize_docs\n",
        "\n"
      ],
      "metadata": {
        "id": "seZ7_2Jzij1a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Function to tokenize sentences using spacy with lemmas\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "def tokenize_spacy_lemma(df_text,string=True): \n",
        "    #string allows us to generate either list of tokens or tokenized string\n",
        "    #tokenized string is used for TF IDF sklearn vectorization\n",
        "    docs_tokens, tmp_tokens,docs_tokens_str = [], [],[]\n",
        "    for item in df_text:\n",
        "        tmp_tokens = [\n",
        "            token.lemma_\n",
        "            for token in nlp(item)\n",
        "            if not token.is_stop and not token.is_punct and not token.like_num\n",
        "        ]\n",
        "        docs_tokens.append(tmp_tokens)\n",
        "        tmp_tokens = []\n",
        "    if string == True:\n",
        "        for k in range(len(docs_tokens)):\n",
        "            joint=' '.join([\" \".join(list([str(i) for i in docs_tokens[k] ]))])\n",
        "            docs_tokens_str.append(joint)\n",
        "        return docs_tokens_str\n",
        "    else:\n",
        "        return docs_tokens"
      ],
      "metadata": {
        "id": "Zz-pJtUDceOr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Tokenize using NLTK\n",
        "#very fast, but no lemmas\n",
        "def tokenize_wordpunkt(df_text,string=True):\n",
        "    tkn_reviews = [wordpunct_tokenize(rev.lower()) for rev in df_text] \n",
        "    if string==True:\n",
        "      list_docs_str=[]\n",
        "      for k in range(len(tkn_reviews)):\n",
        "          joint=' '.join([\" \".join(list([str(i) for i in tkn_reviews[k] ]))])\n",
        "          list_docs_str.append(joint)\n",
        "      return list_docs_str\n",
        "    else:\n",
        "      return tkn_reviews"
      ],
      "metadata": {
        "id": "KIhlAvSOceR-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Function to vectorize docs using sklear\n",
        "def vectorize_docs(x_train,x_test,sklearn_vectorizer):\n",
        "    vectorized_docs_train=sklearn_vectorizer.fit_transform(x_train)\n",
        "    vectorized_docs_test=sklearn_vectorizer.transform(x_test)\n",
        "    return (vectorized_docs_train,vectorized_docs_test)"
      ],
      "metadata": {
        "id": "Ed0a9TQFceUt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2 Model 1: Running Logistic Regression with TF IDF features as inputs"
      ],
      "metadata": {
        "id": "GSfKqDDcizxQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Split to test and training to  \n",
        "#nltk or spacy vectorication - comment or uncomment\n",
        "\n",
        "x_nltk=tokenize_wordpunkt(df_reviews.reviews_clean)\n",
        "#x_spacy=tokenize_spacy_lemma(df_reviews.reviews_clean)\n",
        "y=df_reviews.helpfulness_cat\n",
        "x_train, x_test, y_train, y_test = train_test_split(x_nltk, \n",
        "                                                    #x_spacy,\n",
        "                                                    y, \n",
        "                                                    stratify=y,\n",
        "                                                    test_size=0.20, \n",
        "                                                    random_state=0) "
      ],
      "metadata": {
        "id": "mAotPZgaceXX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Lets vectorize out datasets and fit Log Regression\n",
        "#after splitting train and test datasets\n",
        "vectorized_test_and_train_sets=vectorize_docs(x_train,\n",
        "               x_test,\n",
        "               TfidfVectorizer(max_features=100))\n",
        "\n",
        "x_train_vec=vectorized_test_and_train_sets[0]\n",
        "x_test_vec=vectorized_test_and_train_sets[1]\n",
        "\n",
        "logisticRegr = LogisticRegression()\n",
        "logisticRegr.fit(x_train_vec, y_train)\n",
        "\n",
        "y_pred=logisticRegr.predict(x_test_vec)\n",
        "score = f1_score(y_pred, y_test)\n",
        "score"
      ],
      "metadata": {
        "id": "dUYTRMZPcead"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Check overfitting\n",
        "print(logisticRegr.score(x_train_vec, y_train))\n",
        "print(logisticRegr.score(x_test_vec, y_test))"
      ],
      "metadata": {
        "id": "79D7elo5i7iZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What F1 score will we obtain on the test set if we predict all reviews as 1 ('useful')? We can use this score as an indicator of usefullness of the classification model."
      ],
      "metadata": {
        "id": "IFAoDJffjD9_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "naive_score = f1_score(np.repeat(1,len(y_test)), y_test)\n",
        "naive_score"
      ],
      "metadata": {
        "id": "FP-n3_wNi7k6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Lemma Spacy tokenization did not affect the f1_score although it took much more time to perform tokenization\n",
        "* There is no overfit (train and test accuracy score is the same.)\n",
        "* Naive '1' label assignment is performing better than the trained model. If we assign 1 to each review in the test set – the resulting f1 score is 86.6"
      ],
      "metadata": {
        "id": "JrafALjmjM_T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Tune the number of max_features parameter for TF IDF vectorization"
      ],
      "metadata": {
        "id": "T8Y79tgIjSJG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Let's play with tuning the max features\n",
        "def tune_max_features(range_):\n",
        "    scores=[]\n",
        "    for i in range_:\n",
        "        vectorized_test_and_train_sets=vectorize_docs(x_train,\n",
        "                   x_test,\n",
        "                   TfidfVectorizer(max_features=i))\n",
        "        x_train_vec=vectorized_test_and_train_sets[0]\n",
        "        x_test_vec=vectorized_test_and_train_sets[1]\n",
        "        logisticRegr = LogisticRegression()\n",
        "        logisticRegr.fit(x_train_vec, y_train)\n",
        "        y_pred=logisticRegr.predict(x_test_vec)\n",
        "        score = f1_score(y_pred, y_test)\n",
        "        scores.append(score)\n",
        "    return (scores)"
      ],
      "metadata": {
        "id": "4_ZeRVFqi7n0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tune_max_features([100,500,1000,3000,10000])"
      ],
      "metadata": {
        "id": "5ilfl9gii7qS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* With the substantial increase of the number of features the f1 accuracy increases very slightly (less then 1% per step) , but using 3000 features with only 5000 sententes can result in overfit."
      ],
      "metadata": {
        "id": "QiiZ4uCZjhpu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.3 Model 2: Topic modelling vectorization\n",
        "\n",
        "1.   Tokenize text extracting lemmas from words to improve the 'readability' of topics \n",
        "2.   Perform LDA vectorization\n",
        "3. Visualize individual topics"
      ],
      "metadata": {
        "id": "ZBfCktKNjkPr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Split train and test for spacy \n",
        "#to perform spacy tokenization\n",
        "x_spacy=tokenize_spacy_lemma(df_reviews.reviews_clean,string=False)\n",
        "\n",
        "y=df_reviews.helpfulness_cat\n",
        "x_train, x_test, y_train, y_test = train_test_split(x_spacy, \n",
        "                                                    y, \n",
        "                                                    stratify=y,\n",
        "                                                    test_size=0.20, \n",
        "                                                    random_state=0)    "
      ],
      "metadata": {
        "id": "T-YjKBzhi7sv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Function to vectorize docs (reviews) using LDA topic modelling\n",
        "def LDA_vectorization_and_log_reg(train_tokenized_docs,test_tokenized_docs,\n",
        "                     n_topics,\n",
        "                     n_passes=15):\n",
        "    dictionary = corpora.Dictionary(train_tokenized_docs)\n",
        "    corpus_train = [dictionary.doc2bow(text) for text in train_tokenized_docs]\n",
        "    ldamodel = gensim.models.ldamodel.LdaModel(corpus_train,\n",
        "                                                 num_topics = n_topics,\n",
        "                                                 id2word=dictionary,\n",
        "                                                 passes=n_passes)\n",
        "    lda_vecs_train = []\n",
        "    for i in range(len(corpus_train)):\n",
        "        top_topics = ldamodel.get_document_topics(corpus_train[i], minimum_probability=0.0)\n",
        "        topic_vec = [top_topics[i][1] for i in range(n_topics)]\n",
        "        lda_vecs_train.append(topic_vec)\n",
        "    corpus_test = [dictionary.doc2bow(text) for text in test_tokenized_docs]\n",
        "    lda_vecs_test = []\n",
        "    for i in range(len(corpus_test)):\n",
        "        top_topics = ldamodel.get_document_topics(corpus_test[i], minimum_probability=0.0)\n",
        "        topic_vec = [top_topics[i][1] for i in range(n_topics)]\n",
        "        lda_vecs_test.append(topic_vec)   \n",
        "    logisticRegr = LogisticRegression()\n",
        "    logisticRegr.fit(lda_vecs_train, y_train)\n",
        "    y_pred=logisticRegr.predict(lda_vecs_test)\n",
        "    score = f1_score(y_pred, y_test)\n",
        "    viz = gensimvis.prepare(ldamodel, corpus_train, dictionary)\n",
        "    return {'score':score,\n",
        "            'viz':viz,\n",
        "            'model':ldamodel,\n",
        "            'dictionary':dictionary,\n",
        "            'corpus':corpus_train}\n",
        "        "
      ],
      "metadata": {
        "id": "CunjO1Hxi7vV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "GJ5ZOGQCBsbu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results_LDA=LDA_vectorization_and_log_reg(x_train,\n",
        "                              x_test,\n",
        "                              n_topics=30,\n",
        "                              n_passes=20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IGKAO4EWi7x4",
        "outputId": "77a308e3-c901-4ab2-939c-de9cd8fe8a8d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pyLDAvis/_prepare.py:247: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
            "  by='saliency', ascending=False).head(R).drop('saliency', 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(results_LDA['score'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kcyMAdoAi70c",
        "outputId": "68e18cb6-ca73-45d7-ab36-3cf5d862e294"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8636726016476216\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "coherence_model_lda = CoherenceModel(\n",
        "\n",
        "   model=results_LDA['model'], texts=x_train, dictionary=results_LDA['dictionary'], coherence='c_v')\n",
        "\n",
        "coherence_lda = coherence_model_lda.get_coherence()\n",
        "\n",
        "print('\\nCoherence Score: ', coherence_lda)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yrc0T8JSi73b",
        "outputId": "16ee9e38-73da-4d49-d20b-763a11d4ddb8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Coherence Score:  0.5467063115439466\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The coherence score improved here from earlier (with uncleaned reviews)"
      ],
      "metadata": {
        "id": "ecnZ9rBrpzct"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Let's visualize produced topic\n",
        "pyLDAvis.enable_notebook()\n",
        "results_LDA['viz']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 862
        },
        "id": "Ty_HaMMdi79F",
        "outputId": "888f1acb-b168-472e-84d4-08fd09ae4d81"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PreparedData(topic_coordinates=              x         y  topics  cluster       Freq\n",
              "topic                                                \n",
              "8      0.409678  0.047695       1        1  21.663411\n",
              "19     0.438507 -0.000442       2        1  18.833525\n",
              "21     0.382477  0.158380       3        1  17.700234\n",
              "27     0.339613  0.119737       4        1  13.360438\n",
              "12     0.377109  0.026286       5        1  13.309551\n",
              "1      0.308039 -0.129423       6        1   5.105922\n",
              "11     0.221305 -0.252829       7        1   2.418567\n",
              "24     0.030465 -0.180569       8        1   0.972996\n",
              "5     -0.072110  0.019958       9        1   0.691209\n",
              "22    -0.072970 -0.006732      10        1   0.637847\n",
              "13    -0.065835 -0.077992      11        1   0.614528\n",
              "20    -0.092276  0.029724      12        1   0.487841\n",
              "17    -0.097954  0.022030      13        1   0.456348\n",
              "6     -0.102075 -0.029179      14        1   0.436606\n",
              "25    -0.100237  0.012312      15        1   0.436079\n",
              "10    -0.108426  0.007163      16        1   0.397390\n",
              "23    -0.102471  0.041419      17        1   0.282405\n",
              "16    -0.123658  0.017452      18        1   0.249429\n",
              "2     -0.122371  0.010907      19        1   0.247982\n",
              "0     -0.128201  0.015361      20        1   0.221399\n",
              "15    -0.121903  0.003438      21        1   0.213794\n",
              "26    -0.130340  0.014353      22        1   0.202897\n",
              "7     -0.130064  0.016890      23        1   0.202652\n",
              "9     -0.130197  0.019834      24        1   0.171693\n",
              "4     -0.132695  0.016479      25        1   0.159708\n",
              "18    -0.133592  0.016093      26        1   0.145078\n",
              "28    -0.134730  0.015840      27        1   0.115958\n",
              "3     -0.135028  0.015639      28        1   0.102338\n",
              "29    -0.135107  0.014823      29        1   0.088832\n",
              "14    -0.134954  0.015351      30        1   0.073342, topic_info=         Term          Freq         Total Category  logprob  loglift\n",
              "62      movie  17421.000000  17421.000000  Default  30.0000  30.0000\n",
              "18       film  13093.000000  13093.000000  Default  29.0000  29.0000\n",
              "988      book    870.000000    870.000000  Default  28.0000  28.0000\n",
              "283    action   1336.000000   1336.000000  Default  27.0000  27.0000\n",
              "686      star   1022.000000   1022.000000  Default  26.0000  26.0000\n",
              "...       ...           ...           ...      ...      ...      ...\n",
              "124   fivethe      0.009126      0.705847  Topic30 -10.8920   2.8695\n",
              "125   fleeing      0.009126      0.705847  Topic30 -10.8920   2.8695\n",
              "4977   gender      0.148792     51.571076  Topic30  -8.1006   1.3696\n",
              "261    switch      0.009126     71.033945  Topic30 -10.8920  -1.7420\n",
              "5172   reboot      0.009126     32.541247  Topic30 -10.8920  -0.9614\n",
              "\n",
              "[1515 rows x 6 columns], token_table=       Topic      Freq      Term\n",
              "term                            \n",
              "0          1  0.129446          \n",
              "0          2  0.257158          \n",
              "0          3  0.030435          \n",
              "0          4  0.527799          \n",
              "0          5  0.049698          \n",
              "...      ...       ...       ...\n",
              "13494     30  0.708063   zambian\n",
              "13075     18  0.980004  zemeckis\n",
              "3530       1  0.289227    zombie\n",
              "3530       4  0.706382    zombie\n",
              "13495     30  0.714097      zulu\n",
              "\n",
              "[2753 rows x 3 columns], R=30, lambda_step=0.01, plot_opts={'xlab': 'PC1', 'ylab': 'PC2'}, topic_order=[9, 20, 22, 28, 13, 2, 12, 25, 6, 23, 14, 21, 18, 7, 26, 11, 24, 17, 3, 1, 16, 27, 8, 10, 5, 19, 29, 4, 30, 15])"
            ],
            "text/html": [
              "\n",
              "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.3.1/pyLDAvis/js/ldavis.v1.0.0.css\">\n",
              "\n",
              "\n",
              "<div id=\"ldavis_el5521406547846652963392946508\"></div>\n",
              "<script type=\"text/javascript\">\n",
              "\n",
              "var ldavis_el5521406547846652963392946508_data = {\"mdsDat\": {\"x\": [0.4096782051997111, 0.43850723725174434, 0.3824767256879838, 0.3396133542052971, 0.377109109222406, 0.30803920629358894, 0.22130466613673921, 0.03046515604077958, -0.07210963255674831, -0.07297015659530444, -0.06583518404536082, -0.09227630886938051, -0.09795408568043958, -0.10207537812346823, -0.100237173284313, -0.10842572506861829, -0.10247124971851737, -0.12365822949656767, -0.12237072717140363, -0.12820078270834848, -0.12190271982729974, -0.13033968087736195, -0.13006448672869228, -0.13019709481720496, -0.13269489560981249, -0.13359179199865182, -0.1347299115743515, -0.13502771639947625, -0.13510700715553245, -0.13495372173139583], \"y\": [0.04769473166828534, -0.00044153397818428615, 0.15838030767670763, 0.11973727561110745, 0.026285522411029594, -0.1294232962783461, -0.2528294763139823, -0.1805686434495865, 0.019957872593778948, -0.006731581784330299, -0.07799236503831303, 0.029724425349340024, 0.022030083452822015, -0.029178665281612035, 0.012311500347207115, 0.007163137798085746, 0.04141923909251978, 0.017452308643931685, 0.010907437855484177, 0.015360848153048123, 0.003438374779877877, 0.01435311405997495, 0.0168899374981595, 0.019834179794293055, 0.016479262417447415, 0.016093276208583862, 0.015839709717609476, 0.015639225424036063, 0.014822880320786647, 0.01535091125023794], \"topics\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30], \"cluster\": [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], \"Freq\": [21.6634114308189, 18.833524977900005, 17.700233873613566, 13.360438253052397, 13.309551041018977, 5.105921695119334, 2.4185669456107517, 0.9729959010741571, 0.6912092645202117, 0.6378469764770238, 0.6145284158029096, 0.48784114018495667, 0.45634822075434595, 0.4366059930292683, 0.4360790453340584, 0.39739036140338924, 0.2824053120174998, 0.24942920923977935, 0.24798155437140446, 0.22139930782085931, 0.21379380542860046, 0.20289667460142288, 0.20265218254395156, 0.17169267345505623, 0.159707637123587, 0.14507820076071223, 0.11595815520826776, 0.10233764764396747, 0.08883245761390701, 0.0733416464567342]}, \"tinfo\": {\"Term\": [\"movie\", \"film\", \"book\", \"action\", \"star\", \" \", \"war\", \"read\", \"scene\", \"bad\", \"watch\", \"new\", \"plot\", \"world\", \"great\", \"good\", \"true\", \"horror\", \"force\", \"story\", \"original\", \"people\", \"alien\", \"person\", \"think\", \"kill\", \"man\", \"love\", \"performance\", \"car\", \"waste\", \"vampire\", \"horrible\", \"bella\", \"dumb\", \"twilight\", \"lame\", \"awful\", \"redeem\", \"cliche\", \"quit\", \"willis\", \"seed\", \"confusing\", \"dreadful\", \"bland\", \"exposition\", \"screw\", \"wooden\", \"moron\", \"idiotic\", \"loser\", \"lazy\", \"horribly\", \"cullen\", \"trash\", \"pattinson\", \"ritual\", \"useless\", \"sand\", \"rubbish\", \"pointless\", \"dude\", \"bad\", \"crap\", \"terrible\", \"stupid\", \"poorly\", \"mediocre\", \"money\", \"ridiculous\", \"garbage\", \"mess\", \"poor\", \"disappointing\", \"boring\", \"hole\", \"guy\", \"suppose\", \"minute\", \"plot\", \"like\", \"half\", \"fake\", \"movie\", \"hour\", \"thing\", \"character\", \"go\", \"know\", \"try\", \"time\", \"want\", \"let\", \"get\", \"think\", \"watch\", \"look\", \"people\", \"well\", \"write\", \"good\", \"act\", \"end\", \"actually\", \"scene\", \"feel\", \"film\", \"way\", \"story\", \" \", \"see\", \"awesome\", \"mel\", \"sandler\", \"gibson\", \"tina\", \"piss\", \"saturday\", \"yesterday\", \"jackie\", \"biblical\", \"manga\", \"historic\", \"afternoon\", \"magnificently\", \"heartwarme\", \"sci\", \"flip\", \"reynold\", \"hearted\", \"ant\", \"chan\", \"hooked\", \"steady\", \"adjust\", \"hopkin\", \"kurt\", \"bridesmaid\", \"almodovar\", \"clap\", \"welldone\", \"humour\", \"comedy\", \"pleasantly\", \"hilarious\", \"funny\", \"laugh\", \"enjoy\", \"glad\", \"hater\", \"enjoyable\", \"definitely\", \"recommend\", \"movie\", \"great\", \"cute\", \"cry\", \"watch\", \"adore\", \"amazing\", \"good\", \"expect\", \"fun\", \"love\", \"see\", \"worth\", \"song\", \"think\", \"expectation\", \"humor\", \"wonderful\", \"lot\", \"acting\", \"theater\", \"overall\", \"review\", \"story\", \"like\", \"time\", \"feel\", \"film\", \" \", \"actor\", \"end\", \"go\", \"sure\", \"people\", \"want\", \"character\", \"way\", \"year\", \"know\", \"look\", \"religious\", \"innocence\", \"serial\", \"holocaust\", \"bell\", \"being\", \"naive\", \"shyamalan\", \"hatred\", \"maid\", \"bergman\", \"homosexual\", \"catholic\", \"anton\", \"uncle\", \"gender\", \"spiritual\", \"sexually\", \"document\", \"stair\", \"compassion\", \"lawsuit\", \"tolerance\", \"japan\", \"explicit\", \"farm\", \"preserve\", \"grasp\", \"universal\", \"complement\", \"rape\", \"abuse\", \"condition\", \"ron\", \"consequence\", \"inner\", \"mother\", \"religion\", \"belief\", \"commit\", \"child\", \"human\", \"represent\", \"truth\", \"son\", \"depict\", \"life\", \"father\", \"murder\", \"reality\", \"victim\", \"nature\", \"woman\", \"parent\", \"innocent\", \"world\", \"killer\", \"message\", \"people\", \"family\", \"question\", \"live\", \"show\", \"fact\", \"case\", \"relationship\", \"boy\", \"young\", \"man\", \"tell\", \"present\", \"person\", \"daughter\", \"change\", \"way\", \"film\", \"believe\", \"real\", \"find\", \"leave\", \"girl\", \"story\", \"time\", \"love\", \"come\", \"year\", \"make\", \"know\", \"think\", \"point\", \"end\", \"try\", \"like\", \"see\", \"bomb\", \"taxi\", \"harvey\", \"subconscious\", \"cigarette\", \"population\", \"butler\", \"rocky\", \"tunnel\", \"float\", \"upper\", \"imprison\", \"snap\", \"ironic\", \"drain\", \"birthday\", \"signal\", \"helicopter\", \"deaf\", \"aaron\", \"secretary\", \"helmet\", \"min\", \"gerard\", \"poetic\", \"maria\", \"karin\", \"milk\", \"claustrophobic\", \"safety\", \"native\", \"mike\", \"hotel\", \"officer\", \"crash\", \"accident\", \"drown\", \"  \", \"anna\", \"threaten\", \"scientist\", \"morning\", \"paris\", \" \", \"doctor\", \"smoke\", \"car\", \"escape\", \"mission\", \"adam\", \"club\", \"lock\", \"driver\", \"door\", \"arm\", \"meet\", \"wear\", \"city\", \"drive\", \"police\", \"die\", \"man\", \"death\", \"later\", \"zombie\", \"scene\", \"come\", \"gun\", \"room\", \"face\", \"take\", \"new\", \"help\", \"run\", \"fight\", \"shoot\", \"find\", \"head\", \"go\", \"use\", \"look\", \"know\", \"kill\", \"day\", \"old\", \"woman\", \"leave\", \"happen\", \"year\", \"friend\", \"end\", \"little\", \"good\", \"start\", \"like\", \"time\", \"get\", \"west\", \"howard\", \"franco\", \"toronto\", \"caine\", \"imagery\", \"oliver\", \"riveting\", \"vincent\", \"damon\", \"ronin\", \"jean\", \"excel\", \"punk\", \"surrounding\", \"wine\", \"comprehend\", \"grim\", \"economic\", \"elizabeth\", \"ambitious\", \"coen\", \"intimate\", \"redmayne\", \"joel\", \"backdrop\", \"increasingly\", \"shawshank\", \"democracy\", \"understate\", \"international\", \"vietnam\", \"tackle\", \"kubrick\", \"festival\", \"filmmake\", \"extraordinary\", \"portrayal\", \"performance\", \"historical\", \"redemption\", \"debut\", \"career\", \"film\", \"united\", \"ed\", \"role\", \"filmmaker\", \"americans\", \"beauty\", \"award\", \"capture\", \"david\", \"beautifully\", \"play\", \"american\", \"director\", \"compelling\", \"success\", \"actor\", \"character\", \"aspect\", \"cast\", \"viewer\", \"audience\", \"screen\", \"beautiful\", \"powerful\", \"story\", \"work\", \"emotional\", \"lead\", \"score\", \"set\", \"act\", \"great\", \"truly\", \"scene\", \"give\", \"life\", \"good\", \"make\", \"take\", \"man\", \"moment\", \"true\", \"time\", \"feel\", \" \", \"julie\", \"heartfelt\", \"denzel\", \"wink\", \"mesmerize\", \"gratuitous\", \"fonda\", \"newman\", \"von\", \"leto\", \"mode\", \"applause\", \"emmerich\", \"turmoil\", \"management\", \"prophet\", \"gothic\", \"amazon\", \"lowbudget\", \"jewel\", \"inspirational\", \"moody\", \"butcher\", \"goosebump\", \"privilege\", \"farhadi\", \"timothy\", \"thor\", \"deer\", \"paranoid\", \"porn\", \"gore\", \"horror\", \"scare\", \"hitchcock\", \"supernatural\", \"gory\", \"suspenseful\", \"suspense\", \"rabbit\", \"thriller\", \"genre\", \"jane\", \"scary\", \"version\", \"originality\", \"original\", \"ghost\", \"atmosphere\", \"element\", \"refreshing\", \"effect\", \"film\", \"thrill\", \"tension\", \"scifi\", \"special\", \"visual\", \"creepy\", \"remake\", \"modern\", \"scene\", \"blood\", \"story\", \"style\", \"fan\", \"good\", \"director\", \"see\", \"new\", \"black\", \"great\", \"dark\", \"like\", \"little\", \"character\", \"shark\", \"smith\", \"oldboy\", \"matrix\", \"mutant\", \"bourne\", \"stallone\", \"spiderman\", \"hardy\", \"tolkien\", \"hobbit\", \"elf\", \"chanwook\", \"arnold\", \"schwarzenegger\", \"tauriel\", \"rambo\", \"overuse\", \"ghostbuster\", \"felt\", \"jam\", \"liner\", \"lauren\", \"legola\", \"paltrow\", \"dolphin\", \"lebron\", \"alright\", \"magnetic\", \"contest\", \"vengeance\", \"jackson\", \"action\", \"korean\", \"stunt\", \"undoubtedly\", \"eastwood\", \"vacation\", \"park\", \"warrior\", \"fight\", \"trilogy\", \"villain\", \"sequence\", \"fairy\", \"revenge\", \"scene\", \"cut\", \"cool\", \"ring\", \"fun\", \"plot\", \"original\", \"chase\", \"team\", \"film\", \"comic\", \"smart\", \"man\", \"well\", \"fan\", \"love\", \"good\", \"great\", \"release\", \"hero\", \"movie\", \"john\", \"luke\", \"rey\", \"jedi\", \"lucas\", \"kylo\", \"chinese\", \"palpatine\", \"finn\", \"solo\", \"leia\", \"prequel\", \"abrams\", \"lightsaber\", \"anakin\", \"skywalker\", \"ren\", \"resistant\", \"galaxy\", \"poe\", \"yoda\", \"k\", \"droid\", \"fleet\", \"sith\", \"falcon\", \"han\", \"trooper\", \"inept\", \"rehash\", \"saber\", \"empire\", \"disney\", \"reboot\", \"awaken\", \"star\", \"war\", \"force\", \"planet\", \"episode\", \"trilogy\", \"emperor\", \"new\", \"resistance\", \"original\", \"destroy\", \"george\", \"power\", \"order\", \"liberal\", \"racism\", \"norton\", \"percy\", \"polish\", \"patton\", \"determined\", \"mixture\", \"policy\", \"russians\", \"robber\", \"moviegoe\", \"historian\", \"americas\", \"streep\", \"distinctive\", \"overblown\", \"remade\", \"bubble\", \"romanian\", \"meryl\", \"grown\", \"jazz\", \"oskar\", \"noteworthy\", \"unimaginative\", \"apollo\", \"smolder\", \"bradley\", \"lisbeth\", \"book\", \"conservative\", \"function\", \"read\", \"edward\", \"novel\", \"field\", \"unstoppable\", \"invasion\", \"russian\", \"part\", \"project\", \"important\", \"computer\", \"alien\", \"rupert\", \"ripley\", \"jesus\", \"protocol\", \"kat\", \"unaware\", \"weaver\", \"fincher\", \"brosnan\", \"newt\", \"z\", \"alien3\", \"terry\", \"sigourney\", \"letdown\", \"hood\", \"retelling\", \"inmate\", \"bacon\", \"rollercoaster\", \"romania\", \"encompass\", \"subtly\", \"narcissist\", \"revolution\", \"penguin\", \"smug\", \"alliance\", \"brothers\", \"predator\", \"dog\", \"egg\", \"internet\", \"pierce\", \"robin\", \"jewish\", \"prison\", \"creature\", \"scott\", \"jews\", \"wolf\", \"christ\", \"furious\", \"blake\", \"shaw\", \"statham\", \"walker\", \"pine\", \"owen\", \"administration\", \"dwayne\", \"diesel\", \"larry\", \"vin\", \"underwhelme\", \"drinking\", \"roughly\", \"hitchcockian\", \"dominic\", \"deckard\", \"chernobyl\", \"condense\", \"dom\", \"dimitri\", \"vapid\", \"semi\", \"discomfort\", \"politician\", \"snipe\", \"ninety\", \"hobbs\", \"nerd\", \"jason\", \"paul\", \"fast\", \"car\", \"russell\", \"wan\", \"wilson\", \"brian\", \"johnson\", \"uk\", \"action\", \"chase\", \"franchise\", \"rock\", \"jack\", \"pacino\", \"carrey\", \"danish\", \"cary\", \"heath\", \"pandemic\", \"ledger\", \"jay\", \"grass\", \"al\", \"gibney\", \"whistle\", \"stack\", \"activist\", \"voyage\", \"informative\", \"actorsactresse\", \"infectious\", \"lamb\", \"romp\", \"bernard\", \"strangeness\", \"walken\", \"arkin\", \"lex\", \"slim\", \"incoherent\", \"eisenberg\", \"denmark\", \"documentary\", \"hitler\", \"nolan\", \"leo\", \"carl\", \"jim\", \"titanic\", \"interview\", \"depression\", \"saint\", \"knight\", \"footage\", \"grant\", \"topic\", \"information\", \"truth\", \"seal\", \"rise\", \"christopher\", \"baldwin\", \"cage\", \"wave\", \"del\", \"toro\", \"celebrity\", \"liam\", \"neeson\", \"el\", \"santa\", \"oldman\", \"redford\", \"peck\", \"exaggeration\", \"brokeback\", \"enni\", \"h\", \"ang\", \"hulk\", \"las\", \"sting\", \"peek\", \"nader\", \"finch\", \"sierra\", \"hathaway\", \"beatle\", \"minded\", \"spout\", \"lyric\", \"ennis\", \"mum\", \"gary\", \"anne\", \"educational\", \"moore\", \"nicolas\", \"nick\", \"health\", \"race\", \"person\", \"firm\", \"mountain\", \"chris\", \"german\", \"germany\", \"wwii\", \"jolie\", \"angelina\", \"walle\", \"ajla\", \"danijel\", \"purge\", \"fluff\", \"bosnian\", \"grumpy\", \"donna\", \"lloyd\", \"brazilian\", \"screenshot\", \"spear\", \"pill\", \"innuendo\", \"congratulation\", \"nazi\", \"revert\", \"ava\", \"lara\", \"documentarian\", \"relevance\", \"pistol\", \"ma\", \"ace\", \"borat\", \"war\", \"mythology\", \"soldier\", \"jews\", \"feminist\", \"s\", \"greek\", \"army\", \"marine\", \"military\", \"historical\", \"scientology\", \"reeve\", \"blah\", \"lol\", \"mermaid\", \"ewan\", \"mcgregor\", \"navy\", \"avenger\", \"hateful\", \"implausible\", \"likeable\", \"undeniably\", \"tourist\", \"gere\", \"cheese\", \"impactful\", \"skeptical\", \"proposition\", \"propose\", \"mormon\", \"wahlberg\", \"alain\", \"slut\", \"distortion\", \"animator\", \"bribe\", \"prevail\", \"wilder\", \"perry\", \"football\", \"noah\", \"keanu\", \"todd\", \"game\", \"atrocious\", \"sport\", \"lastly\", \"coach\", \"video\", \"hot\", \"carter\", \"team\", \"bollywood\", \"daesu\", \"misery\", \"boyle\", \"iris\", \"bride\", \"kathy\", \"suburban\", \"bate\", \"cultured\", \"goebbel\", \"farfetched\", \"audition\", \"scottish\", \"filth\", \"grandfather\", \"purity\", \"joanna\", \"kilmer\", \"frontier\", \"scotland\", \"val\", \"reid\", \"palme\", \"bandit\", \"clockwork\", \"nathan\", \"pea\", \"liberation\", \"immaculate\", \"wes\", \"jerry\", \"samurai\", \"unexpectedly\", \"lewis\", \"annie\", \"humble\", \"rural\", \"ego\", \"stern\", \"dub\", \"painfully\", \"kurosawa\", \"western\", \"jasenovac\", \"serbian\", \"genocide\", \"regime\", \"curtis\", \"marion\", \"concentration\", \"heighten\", \"unrelated\", \"serbs\", \"penelope\", \"lunatic\", \"croatia\", \"billion\", \"croatian\", \"nest\", \"mail\", \"unforgiving\", \"myers\", \"devour\", \"delusional\", \"tuco\", \"cuckoos\", \"helm\", \"tesla\", \"cruz\", \"noisy\", \"sant\", \"gus\", \"schumacher\", \"ww2\", \"camp\", \"halloween\", \"bale\", \"clerk\", \"truth\", \"illusion\", \"true\", \"world\", \"michael\", \"johnny\", \"depp\", \"laura\", \"hank\", \"vader\", \"burton\", \"headache\", \"stevens\", \"zemeckis\", \"hanks\", \"serb\", \"vince\", \"donnie\", \"fred\", \"hawaii\", \"ted\", \"jo\", \"porno\", \"babe\", \"rediscover\", \"endlessly\", \"daddy\", \"unsatisfye\", \"superlative\", \"andie\", \"dale\", \"blanchett\", \"hawkin\", \"splash\", \"cate\", \"dern\", \"tom\", \"lynch\", \"tim\", \"sweden\", \"miserably\", \"relatable\", \"mcconaughey\", \"matthew\", \"francis\", \"implication\", \"octopus\", \"murphy\", \"coppola\", \"cutie\", \"dinklage\", \"naomi\", \"watt\", \"horrifying\", \"jarring\", \"kurtz\", \"willard\", \"simba\", \"decay\", \"jacket\", \"tomei\", \"coleman\", \"rail\", \"stimulate\", \"benjamin\", \"oscarwinne\", \"ironically\", \"rom\", \"marisa\", \"propel\", \"jovovich\", \"thano\", \"mccarthy\", \"com\", \"craig\", \"commander\", \"peter\", \"river\", \"heartbreake\", \"lifetime\", \"eve\", \"pan\", \"gyunwoo\", \"implement\", \"kaufman\", \"invoke\", \"fellinis\", \"mortensen\", \"signature\", \"february\", \"autistic\", \"veronica\", \"clinic\", \"stoner\", \"exbf\", \"timely\", \"uh\", \"brainless\", \"viggo\", \"log\", \"filmgoer\", \"cinematographic\", \"fellini\", \"carrier\", \"roger\", \"unwanted\", \"bullying\", \"profession\", \"resilience\", \"barbie\", \"guido\", \"aimless\", \"widow\", \"encourage\", \"abortion\", \"batman\", \"crop\", \"farmer\", \"terminator\", \"chemical\", \"warming\", \"pixar\", \"derive\", \"spartan\", \"ark\", \"panda\", \"remainder\", \"dupont\", \"t2\", \"ministry\", \"electronic\", \"continuation\", \"revisionist\", \"superman\", \"stain\", \"hispanic\", \"fearful\", \"disability\", \"drab\", \"basket\", \"graduation\", \"toy\", \"kristanna\", \"loken\", \"wartime\", \"andy\", \"joker\", \"apple\", \"global\", \"factory\", \"spoof\", \"tiger\", \"dark\", \"product\", \"villain\", \"kung\", \"justin\", \"diane\", \"sara\", \"barrel\", \"akira\", \"boom\", \"bieber\", \"panic\", \"misfortune\", \"popularity\", \"election\", \"timethe\", \"slater\", \"neighbour\", \"unimportant\", \"selfdiscovery\", \"muddle\", \"crawford\", \"handed\", \"unusually\", \"seth\", \"greta\", \"spotlight\", \"conceal\", \"maurice\", \"teaser\", \"jerome\", \"chew\", \"merchandise\", \"sync\", \"bette\", \"davis\", \"concert\", \"haneke\", \"climate\", \"bateman\", \"marcus\", \"siberia\", \"expedition\", \"juror\", \"karen\", \"testify\", \"donkey\", \"zack\", \"bradford\", \"condone\", \"bankrupt\", \"frost\", \"aircraft\", \"penns\", \"vladimir\", \"troy\", \"shia\", \"plantation\", \"aussie\", \"ryder\", \"dersu\", \"scandinavian\", \"derail\", \"elmer\", \"leftist\", \"bigot\", \"dee\", \"tramp\", \"nurture\", \"graham\", \"alexander\", \"neil\", \"feelgood\", \"identical\", \"dakota\", \"jury\", \"travis\", \"bible\", \"chronicle\", \"atheist\", \"muslim\", \"commend\", \"hannah\", \"dedication\", \"ireland\", \"islam\", \"mechanical\", \"osment\", \"christianity\", \"surrogate\", \"criticise\", \"breen\", \"bass\", \"collective\", \"grainy\", \"vignette\", \"miyazaki\", \"indulgent\", \"reproduction\", \"stoop\", \"lilly\", \"juan\", \"farhan\", \"viable\", \"scarlet\", \"akhtar\", \"christian\", \"nut\", \"inaccuracy\", \"god\", \"faith\", \"jordan\", \"holly\", \"wicked\", \"frankenstein\", \"bloke\", \"hiroshima\", \"license\", \"humility\", \"pope\", \"mockingbird\", \"chi\", \"excerpt\", \"adoption\", \"irene\", \"janitor\", \"omar\", \"kicking\", \"miscarriage\", \"overdramatic\", \"glossy\", \"kinetic\", \"counterculture\", \"testimony\", \"cocktail\", \"nana\", \"compulsive\", \"wasteland\", \"thati\", \"dunne\", \"loathe\", \"dancing\", \"doc\", \"package\", \"jake\", \"lad\", \"michael\", \"india\", \"singh\", \"bogart\", \"wallace\", \"robinson\", \"hackman\", \"gadot\", \"devotion\", \"40\", \"stuart\", \"bin\", \"doo\", \"valance\", \"morris\", \"scooby\", \"desolate\", \"cardinal\", \"drifter\", \"di\", \"stigma\", \"gruele\", \"manmohan\", \"voight\", \"lukewarm\", \"minister\", \"welle\", \"torturous\", \"overacting\", \"humphrey\", \"stoddard\", \"opera\", \"soap\", \"mrs\", \"ford\", \"jessica\", \"biel\", \"vulgar\", \"predominantly\", \"sofia\", \"hilton\", \"Action\", \"pad\", \"impossibly\", \"stinker\", \"vanilla\", \"unsympathetic\", \"nate\", \"goth\", \"p\", \"unemployed\", \"bonnie\", \"lampoon\", \"nauseating\", \"sack\", \"slop\", \"gareth\", \"clyde\", \"bestow\", \"goo\", \"domination\", \"migraine\", \"shelby\", \"reali\", \"dripping\", \"vile\", \"tall\", \"khan\", \"ali\", \"salman\", \"eli\", \"frog\", \"overrated\", \"showdown\", \"excruciatingly\", \"boogie\", \"tuck\", \"wallach\", \"dhawan\", \"overview\", \"beyonc\\u00e9\", \"cleef\", \"gen\", \"reckless\", \"eyeopene\", \"friar\", \"varun\", \"tiana\", \"misrepresentation\", \"monetary\", \"ancestor\", \"fantasticthe\", \"ohh\", \"misconception\", \"streetsmart\", \"appraisal\", \"tik\", \"van\", \"dracula\", \"dorian\", \"holland\", \"shack\", \"barne\", \"prescreening\", \"beyonce\", \"turd\", \"bomber\", \"lycan\", \"nolte\", \"attest\", \"sawyer\", \"daft\", \"underway\", \"permanently\", \"innate\", \"trainwreck\", \"ably\", \"novelist\", \"ideological\", \"isif\", \"stateoftheart\", \"blasphemy\", \"intercept\", \"raptor\", \"applicable\", \"bogie\", \"progressively\", \"sienna\", \"soviet\", \"platoon\", \"boycott\", \"addict\", \"   \", \"brenda\", \"zulu\", \"morricone\", \"ennio\", \"tearjerking\", \"spanglish\", \"fascist\", \"dissapointe\", \"zambian\", \"toe\", \"castthe\", \"zambia\", \"vagabond\", \"loveable\", \"balan\", \"vidya\", \"eyesore\", \"asthma\", \"addiction\", \"acne\", \"remakei\", \"unseenat\", \"hindi\", \"enthusiasm\", \"fascination\", \"cityscape\", \"fivethe\", \"fleeing\", \"gender\", \"switch\", \"reboot\"], \"Freq\": [17421.0, 13093.0, 870.0, 1336.0, 1022.0, 5191.0, 766.0, 815.0, 3094.0, 2626.0, 4342.0, 1282.0, 1873.0, 1357.0, 3376.0, 5720.0, 836.0, 853.0, 589.0, 4457.0, 969.0, 3506.0, 384.0, 558.0, 3488.0, 1053.0, 1960.0, 3189.0, 1500.0, 393.0, 652.0588825097162, 315.98036970115555, 256.2423181405536, 173.40839962702978, 164.28939037738982, 133.87846635252228, 95.30824592484801, 272.9042365711036, 70.64242658399242, 67.5080918261854, 63.92151461034927, 62.366868176661534, 60.067218123731855, 53.853346454228024, 51.53133199873076, 51.49258221373903, 50.16107804028495, 47.78548852841065, 46.578620360894604, 46.4447164520826, 45.88669846562011, 45.47833307642926, 44.26052304905972, 42.57957508548709, 40.251644857591195, 138.15967246711114, 38.67687720801875, 37.103555823544475, 36.84413044036699, 36.050708564208826, 70.59257471152699, 119.79579095564372, 53.27703300344789, 2331.442619723577, 146.51348520084696, 354.3652067722524, 392.4057737187568, 129.90317693006835, 81.65246750639193, 584.7354867479479, 207.92927173958475, 127.72573416967569, 159.2478367406662, 331.83595528257183, 92.90742899582403, 331.1630667419712, 181.66422052146117, 753.2234985512597, 301.66580277267684, 643.406591018636, 1108.87801481814, 2796.087606324128, 366.6865758375091, 143.93710939827395, 6334.514162235868, 488.53347429153, 1084.0691796767023, 1622.5628403049107, 1234.7832596331098, 1172.2787011159796, 738.8733779571608, 1520.4201948696345, 901.7962628591143, 495.3365366526589, 741.12466035137, 1148.2182470233572, 1293.405384850211, 842.8064574188054, 1108.1649175779005, 656.9129360986692, 515.3020112905341, 1170.657534920809, 608.2995388071093, 751.5818232207026, 521.3329451829269, 792.2965722823664, 648.4098755532285, 1162.9760541475193, 674.4659899546045, 763.067208894055, 672.4569717221455, 599.6705224861222, 194.52148211523655, 55.186451941253615, 57.86447945431737, 41.351381036414764, 38.570731866530814, 35.96843433411534, 33.226245833042164, 32.723632487447205, 31.512539311277035, 31.382344155524834, 28.55603668785471, 27.887069585940505, 27.365170920233705, 25.386196854801312, 24.565821111583254, 23.891188649046953, 24.02181269330953, 23.604892738373113, 22.90580996503164, 22.310243282668758, 22.241443333891123, 21.014188025735947, 20.595328908218427, 20.003233656789952, 19.751267155514974, 43.76879462943451, 19.38807006864529, 17.420599013003997, 17.334791985218782, 16.984714487910075, 137.38269668332964, 685.5079461195861, 61.254208960354795, 231.76029521422552, 763.5504056386751, 655.7478635902722, 1031.7618840297243, 153.8471180881615, 45.71481209527262, 227.89474139877967, 481.60906965923823, 579.8994363988589, 10674.97232487877, 2239.508996676211, 103.90516983748635, 262.7050191291892, 2703.296040506732, 43.94795272393104, 464.6910381704071, 3201.0640749742843, 630.48939486586, 503.58068346868293, 1833.8182960270012, 1897.745528801132, 432.77058922862534, 224.74976426932986, 1746.3077963928474, 199.97654807012682, 244.31809214660086, 297.3799147188707, 771.5330985366103, 579.7980280932945, 306.44484362274767, 262.69270233094255, 626.5392217138885, 1697.632513539003, 2017.1377674397208, 1522.6068221570774, 845.2226842524815, 2580.3373263072654, 1334.7444463396196, 642.2271862921075, 770.3817125062264, 851.7417896101896, 417.56303843344716, 888.4141766183817, 687.4139999739702, 796.5167267058608, 676.2245993268012, 626.3679595668394, 683.4833577449327, 603.6658716442362, 80.78372928693732, 80.02573810184538, 75.54389619568398, 66.14141962422143, 62.20709197189179, 61.599646191294696, 57.15960063387539, 53.6207914300567, 51.38188587954889, 51.25890153587967, 49.57258147439552, 48.897059873559094, 47.90331481929723, 45.83505013175876, 45.57374371407087, 50.7430618312018, 40.846002023223136, 40.241446708880126, 41.15280141693634, 37.78134986471614, 37.61502575289861, 37.4659204138319, 36.90692008790107, 36.70198292986682, 36.47972364510366, 35.15471875564583, 33.31353653851501, 33.003626190061105, 32.98662446066429, 32.750119552586874, 105.41708886531251, 125.17733593751512, 116.85410227612816, 64.72426432789307, 91.59360453919597, 69.13382046177401, 429.64188060235085, 81.16293503257208, 81.83496326894887, 105.71361831873557, 654.2214759543039, 605.2434864707628, 158.32922685428775, 312.3243666495004, 286.95730407088587, 141.82227362141322, 1488.1092079054633, 373.7705115125267, 259.40599986077854, 322.2075976952098, 162.86956711205156, 182.0629107241316, 631.7576128398797, 222.19453005075417, 134.55334700031986, 722.7935055327384, 189.0185232057591, 333.6876890440959, 1457.928082393495, 522.8571366189362, 337.9871869718349, 530.3778886527934, 563.1722988301271, 523.3753396845799, 284.90284747482104, 262.66942238996006, 330.6095595268452, 467.4353711143607, 747.4153997098101, 577.2675214354541, 255.24295674518734, 307.84348938046793, 268.95257254850276, 360.7009737858259, 690.4028648965003, 1615.6084391927648, 392.26108941514417, 482.2033592144415, 560.181813486302, 478.5217527093526, 404.22655860623064, 739.3788699836618, 713.3102957238523, 621.7378994209616, 546.2646353339086, 509.6983286162212, 503.45971713197275, 574.8648032747965, 568.0689622627752, 429.31304958564016, 462.5758048695839, 427.7242095511637, 492.20857551536517, 435.71884646485694, 76.49905865575514, 71.56134964179557, 70.54464333030322, 61.2230472357381, 60.925109072278886, 57.63200457419439, 54.7039765238287, 53.85272091330978, 47.93944462484704, 46.664212491526825, 43.265558538805536, 43.03925482869278, 42.88729263497996, 42.45973790011184, 42.2073569293842, 45.24490011273109, 39.820197996744, 38.29049933595371, 37.43247476757873, 36.86272250928188, 36.02494240379949, 35.85166436392988, 35.714397283872295, 35.40550604684807, 35.1990879382433, 35.41681018046324, 35.190436258340455, 114.32937807617914, 34.83239921473857, 34.782894546776305, 59.94847561540115, 83.36405613975829, 116.9249028505913, 74.75278714935358, 93.24022829667406, 104.57677385603026, 52.232383748457664, 59.6919001271853, 109.48178487687338, 55.70553264382302, 103.30786080361293, 115.53235424984187, 87.55632164276214, 2740.0989379742446, 136.58816632680578, 92.68137190215626, 282.7081279035439, 239.01098133996064, 109.31347368660234, 137.75982274310456, 112.5776352338847, 81.659360545075, 105.5708927038508, 116.4455680585469, 88.45321505980193, 283.60091261777444, 131.7565918675666, 172.08911649973564, 195.70521212296524, 145.7493877886783, 317.83988772505603, 635.5882397925888, 312.1292276948771, 224.22160676103132, 127.3836942667535, 679.5462421765551, 541.3118382545298, 142.02274233175925, 174.8154436483928, 254.3611574461301, 390.66382613747646, 341.7872800178445, 257.05763148829953, 215.78842663340583, 239.26784287710626, 205.38279871106204, 378.960867501766, 206.48913195358278, 407.9517080956224, 217.24975179864893, 357.98497229488754, 394.1271609541062, 266.28103478661234, 268.200308685733, 267.0493310066111, 255.70109384736213, 285.61001628261914, 266.57005862524227, 292.6032989102854, 235.60641657617256, 301.91867474085063, 268.8971654857465, 354.48876785578346, 243.9169597262753, 293.70726054713396, 264.8963499309865, 241.20835720948403, 93.77614767631088, 90.77292400616132, 58.52978555004689, 49.347677818617456, 48.12015032272304, 49.58027206520683, 47.64424217400349, 45.87137486969771, 45.611502234310684, 44.36826052127984, 42.273956392624434, 42.9372694038701, 37.84677751237513, 36.89882371044383, 36.85523787079326, 36.668506548731294, 36.53341505594687, 35.94499974431353, 35.023179312390845, 35.1036881221775, 34.36946166221893, 33.96909107655263, 33.87880556787496, 33.86277530482844, 33.28772977140827, 34.1693553808633, 32.60091278850439, 31.62855868798951, 31.17457946245819, 31.149743953113166, 79.82924372590168, 45.25657231511158, 39.70075463180479, 53.08475491093007, 143.55304692732204, 92.27273282522441, 112.06580739087, 177.79532601612968, 982.4389700736363, 102.3028280010056, 72.54142192014041, 67.26361065394748, 160.77253374165124, 5924.561730906498, 61.95528282364854, 52.32979376683699, 539.9148855456408, 171.47142351184007, 93.21593816688352, 149.72780684852918, 145.68106175620764, 159.7286081952008, 178.70346580500959, 140.0486469788867, 706.2241291457392, 264.3661864989389, 488.3191576252671, 103.55693962091155, 105.78857605966476, 584.7915395903337, 1136.7593177579818, 142.8092406495312, 406.9620184429477, 259.0564908633897, 381.61820365287883, 328.67672116083224, 318.4656612055782, 200.62973628003417, 947.2349788236471, 457.36381817442657, 202.81107478935766, 304.1476830891525, 197.79342629905938, 291.7996346735984, 393.6982036880274, 577.0106343540173, 243.40212556882315, 471.8945521802325, 326.90441859430024, 359.36471685821425, 477.347245637399, 322.051134701574, 301.2478945099728, 319.35525272629184, 249.1340999889049, 242.24407118126447, 353.6089509165069, 257.5679249889972, 257.5251292641219, 68.42632231319797, 51.4039885550044, 42.02234805571729, 36.3440694482112, 34.36002960446542, 33.660959484927524, 32.33047151348599, 31.057958138582205, 30.847927433146626, 27.642054283837428, 27.199946307387663, 26.519486176446282, 25.265358190255277, 24.742112015099245, 24.581692045044303, 24.570718452494763, 24.165353426936285, 23.75534534537423, 23.39576291912495, 23.317781044448513, 23.22844793140032, 23.213908567245298, 22.476600814404012, 22.426848958742443, 21.621596759657095, 21.351764844470022, 20.951975063813094, 20.88736185124283, 20.61524617263619, 20.45882924829666, 58.9896170778137, 132.4953035262487, 709.8464299099111, 141.10131118456917, 82.68591849269045, 66.70452767883206, 52.47585870456887, 64.01668720472283, 152.51034924321075, 31.664512982117405, 207.71574449587985, 225.02488236755437, 70.38641543659656, 164.56828618010508, 188.1677790822635, 45.67162709053889, 366.92875850304245, 66.82282225122445, 101.44289479364339, 150.5749104291736, 63.661791561888116, 242.7872111267557, 1500.2870259298347, 70.71034809722937, 107.3775233604837, 115.11062853969415, 170.3640444400714, 128.09107074064073, 73.13506483851971, 97.38305114657437, 88.9408702211471, 225.11348205266364, 90.43561623676297, 242.36129024281152, 101.4108306757283, 134.94234509846035, 221.98819653420412, 135.1517914466763, 177.79326796419872, 128.69874659598503, 102.12658479016945, 149.30937555154233, 106.0094437288557, 150.84144092483075, 119.8977146846378, 127.55975089532781, 164.49670017466042, 81.31266863026276, 69.29623173282964, 59.706116297359884, 56.256818559185305, 38.298090007855755, 36.79567386407015, 34.52598220465245, 32.16259736984744, 31.669264897103876, 30.388703000219444, 28.37809769423721, 25.9495449043902, 25.494160069485385, 22.640029972495558, 22.199793688496946, 22.00891629814742, 21.93959449558534, 20.829931601839146, 20.386592171551733, 20.194492729053277, 20.297467098650245, 19.74166497707334, 19.634323983542068, 19.421272654662832, 19.023808195876708, 18.23658065471296, 18.049678229057402, 18.037566920240906, 17.904415959367014, 35.888060063908604, 90.15186785529285, 655.4790367104141, 69.00455104549904, 49.149645707586885, 29.608329495761545, 54.348699931726735, 29.760195366201224, 70.76341737475632, 34.0425240639104, 225.29482268934046, 74.79492410559651, 102.45831335637669, 133.90883656775378, 42.05861373583532, 76.16969323062119, 295.8878908902767, 96.29790516515632, 79.5677212678543, 59.35497872458313, 119.37449020138678, 179.43768674730464, 131.4848725931409, 60.692449838874424, 66.95848551444959, 279.76053835102556, 55.67621386302337, 58.117467333524495, 114.17133485308612, 104.91464194167645, 85.72190057077165, 97.25499230608973, 104.66535083520358, 91.37455268453229, 67.84534514082766, 65.29493548685448, 91.05840082658925, 62.589044646377, 130.24025507558173, 108.77383942309778, 83.45323704774705, 77.43528138414614, 65.49095200384842, 61.9290736205113, 58.368273955182985, 53.756705985870546, 53.73935559452682, 52.813348906537584, 49.764961506134874, 45.98010362608061, 41.57811602136482, 40.405240474913086, 38.121573239543956, 32.050569496785755, 27.149141514210807, 25.099004607319014, 23.940975068968672, 22.11031182526543, 21.834809972914947, 21.426687922671228, 20.574358280704136, 19.724470904790667, 18.547895597977302, 18.16168424976056, 17.15278747612631, 16.253930544762966, 16.180335311623512, 14.482757387202133, 38.57456511078278, 187.63888880534103, 28.880835410713754, 33.06027379387322, 333.87357456716796, 248.9275372403716, 171.97642449598257, 75.76004443691255, 60.32700769146989, 60.40129378895205, 26.29393327081575, 114.59829852881747, 31.945106630785318, 70.28951392192164, 44.93261086751085, 40.29004793608612, 47.71861829291446, 33.40686649612083, 36.49022089618319, 35.18053302815294, 31.518673896093013, 30.866056791577826, 29.497358479940683, 28.602112749117996, 24.097349680455316, 23.921741195743365, 20.386190329896507, 20.255004349186724, 18.782724562888337, 18.27534538976017, 18.152515670160835, 16.91222545284424, 16.181646713888465, 15.847825706382164, 15.637674995487982, 15.627077913198761, 14.951035509473245, 14.324716303460475, 13.945681723956625, 12.721106314532056, 12.564798812624554, 12.424324509737643, 12.318006047440658, 12.27434770435921, 11.697792594812691, 11.468372113320681, 11.188567596361956, 10.995344664286062, 622.498384188673, 32.96878440542681, 22.412917296907075, 159.155217536935, 65.47609310711762, 48.78337879830481, 25.39429435792043, 14.98469948873815, 15.977755664846214, 18.425224253386794, 21.366830505467455, 18.17492172938255, 18.53566595580667, 15.952414581919166, 383.45598272921166, 62.42434480199252, 38.13678392240154, 37.516829198727166, 36.944229892872606, 31.46034935347062, 31.290461792042063, 26.442178757997794, 24.74069502023922, 24.038942949652025, 23.865893063342217, 19.303860022600837, 16.995609171021318, 16.64110500356967, 16.4773325282887, 14.939564653839291, 14.787119538882036, 14.474492339487316, 14.470330862963301, 14.462914694656664, 14.141598054185907, 13.882973709932049, 13.859197674965852, 13.62937889243657, 13.32204242942453, 13.17040477995988, 12.613306229262902, 12.528123089159058, 12.487697175326852, 12.430349247886975, 29.524809874029675, 79.88356033525378, 27.28184156467532, 47.80276363463482, 18.590386294272253, 34.27174357384567, 25.484987511465217, 35.628194227716556, 29.14675313992159, 23.04966562660756, 17.988122315191138, 17.934847470143854, 14.883337047525687, 75.57762209665921, 41.63890683987228, 40.327498521332764, 40.21612498101065, 38.99746160217644, 30.226854628563853, 26.96132830501022, 26.086830906155857, 23.266783433760715, 20.275207006703216, 18.519646077012137, 18.493282033238042, 18.255462129657932, 17.350223903629985, 16.885075363018085, 16.21542750289626, 15.974800104236552, 15.438284349745315, 15.277877247608597, 13.756824449012791, 13.044976129449719, 12.248689572562396, 11.849782222945914, 11.208693496479784, 11.162872191305334, 23.452344122522348, 10.620494570809447, 10.548015458742482, 10.275733409527787, 10.042197471684347, 58.157400278897306, 66.29710358999307, 94.47494611434576, 104.28307466428778, 35.7182652463436, 15.809894694543866, 24.931425156743035, 20.478862355299643, 26.060143435886566, 23.077563623941003, 55.74514003148971, 23.450175825368163, 17.469503212305085, 17.52765491338811, 148.25238197214847, 52.76842001453904, 30.05954050434597, 27.11968143894256, 25.50263665444464, 23.706259692146574, 19.304200968582062, 18.017245223207592, 15.025634890832672, 14.152257818107348, 40.73385005023222, 11.333922468211547, 10.81582945021135, 10.611833989531464, 10.541699203126274, 11.212939600843965, 10.238074501573582, 10.04396850990353, 9.930656266858962, 9.72654382498025, 9.497905591111325, 9.409101668838963, 9.304195360510029, 9.013942977295853, 8.958912438666344, 7.921837649402143, 7.748279329344048, 7.208633664887436, 7.130740898069201, 6.816635744567149, 259.92213606185584, 86.84815462857298, 41.09581418718121, 12.845919655400134, 16.586191429424805, 28.560607230517178, 22.757762458938824, 37.89675516990652, 18.402920081845885, 14.610773982705483, 17.500783865045562, 24.302696690160477, 24.18349284855686, 16.918443861108358, 18.847386348694123, 21.270028995470355, 12.017263583444153, 15.27173598690815, 13.48069914037467, 12.104468509459837, 60.09894333422737, 54.96640331498054, 47.107027673576106, 35.58345271975505, 34.35583110607476, 25.55271928847015, 25.054366443902776, 19.89904095028161, 18.877841551245933, 17.5905738102351, 16.39566021705785, 16.276325327566646, 15.513591059135655, 13.96508550240167, 13.878641963182996, 13.789492533413041, 13.102008740134597, 12.981738726702877, 12.927093664944794, 12.563884704875173, 12.25452353524049, 11.82646770914683, 11.579134532685103, 11.570660149619293, 11.565542548844519, 11.024966300272993, 10.771246426260076, 9.812880108106022, 9.774384409414905, 9.76530248001996, 20.221711292717114, 19.898546386252278, 42.85166233375429, 17.3043903554118, 27.53892253812825, 13.500915843459303, 24.717579709216274, 22.27822790420047, 30.24733422903226, 33.20587878357553, 15.693632262066258, 18.704853570377313, 13.856794455071192, 158.33775748175182, 40.32002336811323, 31.615084720006102, 24.508011740940326, 19.324575528663054, 16.618710952132965, 15.718441189662942, 14.935540338470462, 14.762019374993109, 14.70916998478161, 14.372650619015685, 14.136962380351163, 13.682613171889813, 13.0538714452642, 12.646737114342645, 12.388112438839567, 11.082534395283439, 10.66641802027181, 10.177371825417394, 10.174798631587471, 78.33532982090425, 9.62974790978056, 8.764037806054029, 8.629181374330754, 8.56582505924701, 8.52163342951743, 8.074080580030383, 7.988452031640933, 7.819569615455935, 7.69988177045214, 429.2549708182839, 23.198936601198962, 69.55204914791338, 26.317182512608163, 17.25967755893022, 26.83715394237126, 16.81875246813483, 25.16684997769822, 11.213248562804411, 14.975966335274297, 12.990204443369285, 43.938271249191544, 35.43806272495011, 31.11119668466927, 27.39480977071205, 21.415581405482147, 19.327436691537656, 19.30524769759954, 18.94203482136935, 18.25772728042723, 15.80940762277226, 15.109306092020462, 14.931552409631081, 14.805947412425843, 14.310403173424866, 12.033158363947415, 11.416329048771686, 10.611015508602868, 10.605599650360308, 10.53916575633627, 10.296793877633563, 10.172438955030199, 9.63940877443709, 9.43323949077706, 9.563359966582228, 8.877041849379296, 8.27993600892419, 8.134782044968281, 8.059170525567307, 7.920217366209074, 7.871480074180729, 28.152505721741438, 20.107458798411503, 25.113839861144495, 16.19232835627829, 64.52850600696597, 15.27729276024723, 23.644194849948033, 11.125503029104719, 12.78988874947769, 26.671512939362724, 16.126440393802273, 12.65062052514647, 13.056318283337614, 11.51352523489966, 40.42925174740412, 22.645812356685337, 21.040761408152605, 18.29181306788607, 16.977245805623937, 16.80812659058417, 15.52147607733318, 15.265558519116059, 14.225577321092553, 13.553805642232367, 12.965561450638704, 11.845750577479418, 11.094978384838068, 10.88082952047125, 10.861008851433105, 10.71196400405491, 10.709691854432405, 9.849625057264088, 9.44009793277846, 9.398258698335301, 9.332769183194495, 9.017417462833798, 8.668537303667158, 8.267251053003022, 7.6693347560854574, 7.619149455708652, 7.5963134857144965, 7.872290560942065, 7.399426646951986, 7.02417668159998, 78.59341783202309, 35.3270011639924, 14.664342283762542, 25.574982011263693, 21.773930628990843, 18.747953292375215, 15.0548587184736, 16.972096342345484, 10.796060766281322, 11.580926341248588, 12.252243837695126, 10.043347469782184, 15.075530824843055, 24.78804930213984, 21.408765892872292, 20.339549794176506, 17.60310258100496, 17.084531621035115, 17.046427813281166, 12.934384417694064, 12.315331660482103, 12.205479137310562, 11.997476285168977, 11.048718207906859, 10.682424402559477, 10.374052325012377, 10.234435104669977, 10.193739506271823, 10.077219740184189, 9.274773389672239, 9.243563727442531, 9.011040442674167, 8.98031536702232, 8.913445057679859, 8.820723681423315, 8.502846319423355, 8.11530282098289, 8.022725677391989, 7.10416059796029, 6.976011633471455, 6.859233303946631, 6.843441585993673, 6.731191634489554, 11.157193485015004, 32.99004211144647, 16.414127637516888, 17.574778451555588, 12.839676324415034, 30.750812387989825, 11.72376050287997, 17.28437477395819, 14.831218105458916, 9.16760324229467, 54.12086450507696, 28.358106444714775, 27.630080428507338, 22.146280723107314, 20.1610938974727, 18.416922705180962, 15.79374875144347, 13.781590806938258, 12.579229016086165, 9.262888296805246, 9.056513640858636, 8.910635634871127, 8.26384300583175, 19.110565632692815, 7.637292309476231, 7.573911500644757, 7.503414672575522, 7.247413530758145, 7.167056343753844, 6.966114439531816, 6.211158473433107, 5.989446430754743, 5.817183769658727, 5.702791876736717, 5.563324191539474, 5.479924772317654, 5.238615130320537, 5.23305848327138, 5.098996751123498, 5.045404838093167, 6.4904177582498335, 62.53699078866072, 16.42215360541503, 16.719471313464968, 7.126836581272368, 6.780270675897539, 5.809283209494196, 34.80897250715112, 14.97069958778035, 14.71750945005269, 14.591245682832534, 14.443477427104796, 14.255637890055455, 12.779234975723462, 11.824689297781232, 11.132086607984853, 9.258096236501979, 9.256469074349946, 9.121670235414435, 8.829085320420784, 8.341893511026939, 7.923067224495339, 7.922806198957899, 7.910747282487099, 7.740876805216341, 7.590448096895462, 7.209851239890213, 6.986830402889305, 6.863794702966487, 6.572271652215383, 6.0031185963301885, 15.39138008234566, 5.642795237576072, 5.540436158184101, 5.15997135847738, 5.064306271270417, 4.859248315928861, 13.93797598910985, 6.9599764353940055, 7.929548071566933, 10.269777161077293, 22.345114300523306, 12.6906623693918, 8.307209549673335, 9.98054850935499, 8.773900042669466, 7.562319120044204, 26.48477821206853, 14.488785730443553, 13.059072285743026, 12.247990021886622, 11.011992170904168, 10.280647856128772, 9.851940805332505, 8.848783915849522, 8.559353687077325, 8.505385831856634, 7.992754984381278, 7.640457061680424, 7.496720667548893, 7.446605639931249, 7.144646137142847, 7.077770482073083, 6.987361414020648, 6.109679288051034, 6.106393904456096, 6.0972258570978815, 15.596526066981017, 6.022514924132446, 6.855758531012668, 5.980406855525928, 5.907815391431602, 5.855315444222165, 5.70504172279674, 5.68249875126929, 5.619768609077714, 5.029302783921568, 10.624343796514736, 31.306882918849734, 15.80066387652723, 45.361505969541376, 41.99417480609062, 37.97695622230842, 30.65102859688352, 19.5719069753045, 10.17264656960483, 41.849952455354924, 8.272076873478733, 7.296473418098581, 6.4377347018662014, 5.93657708007893, 5.830713975729864, 5.792798150353639, 4.919917899194817, 4.677117931206694, 4.6421296166153825, 3.876741342931687, 3.6084213639393017, 27.042308622849376, 3.2170677007962754, 3.1343750087341737, 3.0940729657884396, 2.809446279007993, 2.717275795747133, 2.5521550660548162, 2.5106758934598377, 83.25919502696267, 2.460434661777491, 2.460434661777491, 2.2656759647004954, 38.16816255782232, 7.556015858216073, 9.000812672187472, 9.004200914712927, 6.171180750246339, 5.9038677896897465, 4.564087185885378, 11.211877610991591, 5.3872986246016366, 5.965113152910724, 4.023101901737829, 25.20393156489385, 18.772021929390686, 18.25674651126414, 13.835543807635654, 13.289735581233929, 13.210731418536215, 12.881657983046907, 10.548364305177419, 9.289355120941888, 8.517362615863261, 8.516157656106198, 8.42864611104399, 7.520938615729178, 7.383295816253978, 7.172345084920034, 6.44451084810678, 6.373456134126213, 6.155610530810588, 6.105790784085185, 5.898793609751733, 4.927797609357273, 4.884118923921272, 4.847549311493509, 4.749540968337856, 4.495454004610091, 4.4638827320660415, 4.307738081973944, 4.271966510538928, 4.231915657533538, 3.8106855476406247, 4.985641680379905, 12.563220039236086, 10.123153784173013, 18.479704683841383, 14.485605078338194, 14.141673228772909, 13.440720514867296, 10.281541645902388, 7.911266223150932, 7.746144578273561, 7.601163228888759, 7.131390031872012, 6.595812665058793, 6.093721466228396, 6.071966816840234, 5.857580871945769, 5.748892823510008, 5.473038997175837, 5.282843116713741, 5.145694034342712, 4.8022071103037725, 4.801080596737068, 4.708220221688514, 4.66990045851948, 4.640711135141088, 4.576028114561003, 4.320586903385527, 4.200208357857535, 3.8718810878162455, 3.863047580593186, 3.8512964978973674, 3.7597633262274006, 3.7586734164628637, 6.265366792192055, 7.374532049969215, 8.293847270104758, 14.961719568530429, 12.709206150552031, 7.118204458791253, 7.011265188559043, 5.91472950472972, 5.427031173351569, 30.556391118938034, 18.442093155192662, 11.790347853319865, 10.763845143501683, 9.613924109143818, 9.436443699230832, 7.725662348004383, 7.689828772319011, 7.48165204828001, 6.8501538502844115, 6.250093575676155, 5.809637024576266, 5.625550036413131, 5.4760754014747555, 5.44448669668028, 5.079039083456755, 4.863661210512526, 4.827752778809428, 4.74335154945183, 4.684574069647638, 4.1207523952492355, 4.065365358362184, 3.9659635177381642, 3.6760801136367776, 3.614581082902409, 3.229217758819997, 3.191664900813213, 2.9921212143613687, 2.918948120319731, 2.871447699585941, 48.56198600231098, 5.48602750903141, 4.582767212111254, 9.43776330388617, 6.403829122080655, 13.519882887025828, 12.039408286434632, 9.025352759843669, 8.288891431887379, 7.993464369468985, 7.032748481959799, 6.629864866350631, 5.786452305000291, 5.761120166017423, 5.565966257356634, 5.4987811056824185, 5.418152259022498, 5.319081726592137, 5.312276322282504, 4.355391614295142, 4.316490445101396, 4.219730656544642, 4.1375478023011905, 4.103210178181116, 3.9214900172659397, 3.6314322263687506, 3.6215585958524272, 3.4949075994971754, 3.4696888590513675, 3.2311256919866795, 3.2183151388328946, 3.1355130688936526, 3.0685599365946064, 3.020443395497692, 2.866546908527681, 12.32705660710029, 14.708585041208801, 7.7153231235222215, 5.976743007979607, 3.5572902236544848, 3.5744708480621328, 19.638680532677167, 9.45443617609709, 8.116324995026924, 6.518856770912711, 6.1412796073667, 6.1186871998589485, 5.973148812784514, 5.755045898918208, 5.22016088093296, 4.909247245589505, 4.846600697591957, 4.752680348020835, 4.746135530199472, 4.369567694488123, 4.288043914803967, 4.2528362647718385, 3.9630877913543525, 3.9110165954722724, 3.7788163356739877, 3.0048618093017163, 2.8430390877846885, 2.7791453376888478, 2.754747977316513, 2.5880699551839057, 2.5493825186779304, 2.3970795116372967, 2.2745349590093333, 2.12229384897557, 1.974625210898255, 1.93993148674434, 19.303749955061996, 11.0358974727922, 7.546880212468323, 5.644951454829139, 14.065412351119866, 5.53377063994996, 5.4281164034297715, 4.8814549644543215, 4.775817339327001, 4.672988122946186, 4.011100312868623, 3.9465462734777574, 3.8736139530880913, 3.7603809491302136, 3.7474489797819945, 3.382811933805731, 3.313330449725902, 3.2649573514580146, 3.192238812472223, 2.7695359204892336, 2.6467127260876193, 2.5718085250292773, 2.554423438481942, 2.550272034733733, 2.550038030764627, 2.5140940460934464, 2.4729340840615204, 2.1633128879144095, 2.1338993473440135, 2.112474803391714, 1.8492732418228646, 1.8467734077547557, 1.799143488759121, 1.7639984759731975, 4.4351440331499, 4.301443987591703, 22.75370351883236, 11.146792275801113, 9.108686581893865, 8.617783723922837, 7.584907574035271, 5.6073045655060305, 3.8271235894261992, 3.546258511740615, 3.3532364543985524, 2.9661862031496087, 2.7633966217535164, 2.6457940131459248, 2.5585265653827207, 2.201857367808489, 1.6393652002338104, 1.4834962573411954, 1.4029892377950401, 1.3901531087633858, 1.3484977164389944, 1.3008733081954913, 1.299691488139587, 1.0766468351846685, 1.0735430935088273, 1.0144927249904585, 0.8533329602268579, 0.8429518585923977, 0.6607412278441389, 0.638935958623498, 0.406150711782344, 0.19618627238286238, 0.5716106984873773, 7.09495949132461, 5.545381358411674, 3.8969260087339332, 3.4260882713756926, 2.4178205670610176, 2.10054061160625, 2.055817924633145, 2.050142469381638, 2.0479135623132563, 1.8487454865943165, 1.8004744470690701, 1.7931164533057227, 1.7073157034345279, 1.5429609872480277, 1.4811827487570017, 1.4748968822548576, 1.4529635735779802, 1.374577944106551, 1.318833412654739, 1.2318998125359304, 1.2279732899499705, 1.0883853912709174, 1.0457225137559736, 1.0373744863360577, 1.0267539115238131, 0.972624072058023, 0.841324861024011, 0.8065301056867248, 0.7342682994860504, 0.7159268453641074, 4.888959512540496, 1.8979633344156455, 2.1635695499677183, 8.614434941397077, 4.1713098727465185, 2.3062329861214486, 2.104018815592368, 2.095877241519032, 1.5973236843278107, 1.522142142784593, 1.465046973517233, 8.934523098181652, 0.8487805097180285, 0.7155822017749278, 2.0239846711550777, 0.6014616396773791, 0.5741977245069114, 0.5546757391513825, 0.4949477370349831, 0.4301727058636543, 0.4301727058636543, 0.42733108615494514, 0.3579972611382058, 5.722307370210907, 0.14938153075012434, 0.14879111474329124, 0.14879111474329124, 1.1804759368558952, 0.7638295246683215, 0.3272498344272287, 0.009125846698581018, 0.009125846698581018, 0.009125846698581018, 0.14879185704581432, 0.00912585115953368, 0.009125847590771551], \"Total\": [17421.0, 13093.0, 870.0, 1336.0, 1022.0, 5191.0, 766.0, 815.0, 3094.0, 2626.0, 4342.0, 1282.0, 1873.0, 1357.0, 3376.0, 5720.0, 836.0, 853.0, 589.0, 4457.0, 969.0, 3506.0, 384.0, 558.0, 3488.0, 1053.0, 1960.0, 3189.0, 1500.0, 393.0, 652.7297190774715, 316.6511835013917, 256.9131319591647, 174.079213427266, 164.9602042421357, 134.5492801527585, 95.97905987618182, 275.36062485865, 71.31424436366802, 68.17899760436525, 64.59232899515497, 63.03768197689767, 60.738031923967995, 54.52432908059578, 52.202145798966896, 52.16339601397517, 50.83189184052109, 48.45630232864679, 47.24943416113074, 47.11553043063127, 46.557512265856246, 46.154359643524366, 44.93133925450491, 43.25038888572323, 40.922458657827335, 140.54610831451453, 39.34769100825489, 37.774369623780615, 37.51514588935373, 36.721522364444965, 72.27804835467592, 123.58860597558001, 54.53307017985898, 2626.1183743176116, 153.71960886983658, 382.7266500149992, 429.00827091812073, 137.19429007740663, 84.86682374785383, 665.2013868275804, 226.61405963001064, 135.81980806319658, 171.7207387615834, 376.13502528773836, 97.95531338501311, 389.92085688889006, 205.25955811328384, 1018.8495236191982, 372.6587628710486, 903.903480949539, 1873.3983574019558, 5911.146291334244, 512.4255315517615, 169.1468676079259, 17421.055620238832, 755.4360267097105, 2236.3748245441448, 4062.072515999835, 2834.1305217369813, 2950.227543545194, 1554.4864751447385, 4483.758392345063, 2133.3198758302037, 944.4221643236395, 1746.4289931136643, 3488.840086267928, 4342.837121235458, 2203.4063439480915, 3506.375103774367, 1522.162277966899, 1010.4788441935285, 5720.490926989692, 1475.1552120362526, 2358.5358254447897, 1069.025148662083, 3094.6299578494595, 2049.654776354014, 13093.922825942574, 2504.3973917428784, 4457.491920683831, 5191.366925348812, 3392.1826896521366, 195.19264033878105, 55.857600230381706, 58.58558401324056, 42.022529325542855, 39.241880155658905, 36.639582742656486, 33.897408300513014, 33.394780776575296, 32.183687600405136, 32.05349285710187, 29.22718499887699, 28.55821787506861, 28.036349914392964, 26.05734514392942, 25.236969400711363, 24.56233694229275, 24.69852206546119, 24.27604428047768, 23.576958254159745, 22.981391571796866, 22.912591623019228, 21.685336339570203, 21.266477197346536, 20.67438194591806, 20.422415444643082, 45.28123354974004, 20.0592183577734, 18.091747302132102, 18.005940274346887, 17.65586373590453, 146.50158387658098, 766.3869617135234, 65.2525275970914, 256.0625488224021, 875.3954516412255, 765.7495237278245, 1267.2477721798145, 174.6356031692925, 48.89845208626575, 265.8251055016449, 598.0847153028029, 732.6682969128601, 17421.055620238832, 3376.870561629835, 120.33389828033286, 333.65941960051623, 4342.837121235458, 47.318012716003366, 652.5441948813703, 5720.490926989692, 937.9533017672999, 733.6585188654279, 3189.617221772076, 3392.1826896521366, 655.0953179286025, 309.71267534320936, 3488.840086267928, 272.035277704486, 345.2874020060978, 439.20832521364974, 1426.513300213197, 1006.0107049281836, 458.77177306291253, 382.37810709760686, 1181.0970278175175, 4457.491920683831, 5911.146291334244, 4483.758392345063, 2049.654776354014, 13093.922825942574, 5191.366925348812, 1589.3709815785048, 2358.5358254447897, 2834.1305217369813, 784.2001906803999, 3506.375103774367, 2133.3198758302037, 4062.072515999835, 2504.3973917428784, 1904.2455058912562, 2950.227543545194, 2203.4063439480915, 81.45352545983076, 80.69620240943668, 76.21365080479956, 66.81117438076402, 62.87684667672996, 62.26940080041031, 57.85327811405408, 54.29054603917231, 52.051641191013736, 51.92868197167867, 50.24233609112963, 49.56681448564196, 48.57306942841284, 46.50480474087438, 46.243498520785316, 51.5710756015037, 41.51575663233875, 40.91120131799574, 41.854381064138515, 38.451104473831755, 38.28479696279007, 38.13567502294752, 37.576674697016685, 37.37173753898244, 37.149478254219275, 35.824483263621964, 33.983291147630624, 33.6733808457141, 33.656379078015284, 33.41987416170249, 107.7167045398641, 129.32170387351553, 121.65678996404121, 67.22093602058614, 97.633843251715, 72.78531026784039, 504.1000216838535, 87.16659717831307, 88.18701882969876, 117.37502144224096, 872.1886349396351, 807.811354236562, 186.4596397389268, 397.5091334995543, 366.38620476013546, 166.93484763740136, 2430.700762128256, 509.1852526651878, 336.9203029243917, 435.4689961028759, 198.30489066666618, 231.6717656312358, 1055.0938042849664, 301.4294152748565, 166.0172622145967, 1357.856286910924, 254.24165373828345, 526.6162540866302, 3506.375103774367, 948.6501230731211, 548.5125456239152, 988.3693826636943, 1128.8523846682335, 1061.4667455692734, 463.50347308909465, 415.31690184565554, 579.3441910111727, 964.719381098815, 1960.8505708634693, 1405.8219365484738, 405.27097158201826, 558.9493739159341, 448.1259194650627, 753.8527273347278, 2504.3973917428784, 13093.922825942574, 932.5951179904022, 1435.614064285434, 2099.384422556563, 1501.2713647288485, 1045.1778522780978, 4457.491920683831, 4483.758392345063, 3189.617221772076, 2299.5491600767687, 1904.2455058912562, 1896.771616055793, 2950.227543545194, 3488.840086267928, 1293.0639233570723, 2358.5358254447897, 1554.4864751447385, 5911.146291334244, 3392.1826896521366, 77.1688165877861, 72.23110757382653, 71.21440126233418, 61.892805167769104, 61.59486700430989, 58.30176250875522, 55.373734455859704, 54.52248294244404, 48.609202556878046, 47.334035722951086, 43.93531647083654, 43.70901276072379, 43.55705056701096, 43.129496342736616, 42.8771148614152, 45.97738632950185, 40.489955928775004, 38.96025727172254, 38.10230000754222, 37.53250890279805, 36.69470037883751, 36.52142229596088, 36.3841552159033, 36.07526397887907, 35.868845870274306, 36.09081654812573, 35.86019419037146, 116.5106168075146, 35.502372061453514, 35.45268342326058, 61.74385605708864, 87.79541047339288, 126.04302981598153, 79.02045402445084, 99.787589786878, 113.78632985730955, 54.47674764066035, 62.894748809699294, 121.41748856739872, 58.70594551132694, 115.07207180375106, 131.47023694958904, 97.6662192459099, 5191.366925348812, 166.25055357014193, 106.15749659402375, 393.2541826193667, 324.13310524063013, 130.89341946407166, 173.5250757071916, 136.29861005521067, 93.75978205434164, 129.4262298187434, 147.9232551008917, 104.01586246538173, 486.8902340650247, 176.5156200759888, 255.1678113760588, 303.8408003969057, 204.35542651366478, 671.2990672017935, 1960.8505708634693, 680.1446043229046, 436.59030140520696, 179.78949354899942, 3094.6299578494595, 2299.5491600767687, 228.29107474804468, 339.1834809893512, 683.6180170081163, 1594.9845879962763, 1282.10036006428, 750.5053539553269, 539.6923570742247, 673.3702866065719, 496.81694712236083, 2099.384422556563, 540.7044257166351, 2834.1305217369813, 621.855617731532, 2203.4063439480915, 2950.227543545194, 1053.894791301827, 1131.5359734748572, 1196.1965309531015, 1055.0938042849664, 1501.2713647288485, 1280.3835831396168, 1904.2455058912562, 859.1743180149363, 2358.5358254447897, 1501.9750901632217, 5720.490926989692, 1105.7806237206835, 5911.146291334244, 4483.758392345063, 1746.4289931136643, 94.48379160938552, 91.54024186310674, 59.19991588338433, 50.017807939424216, 48.7902804435298, 50.276435400619064, 48.31437229481025, 46.5415085431527, 46.281637246307795, 45.038390642086604, 42.944086513431195, 43.618396341151936, 38.51690763318189, 37.56895383125059, 37.52536799160002, 37.338636669538054, 37.20354517675363, 36.615130220697424, 35.693309433197605, 35.77652602719512, 35.039637902245296, 34.63922121978634, 34.54893568868172, 34.5329054256352, 33.957859909036976, 34.869227388351966, 33.27104290931115, 32.29868880879628, 31.844709583264965, 31.81987407391994, 81.58821003297568, 46.78123316586916, 40.925866211795295, 56.03070536985807, 163.62300993110563, 102.11263156695507, 127.31733985751261, 218.72799267411764, 1500.0188728369867, 118.34078001391103, 81.29414065844196, 74.79137317096425, 202.98424417013015, 13093.922825942574, 68.41016615879624, 56.64578118805023, 892.8608325953967, 232.08358248301144, 114.65134349239844, 206.43535421402981, 201.2739267403584, 228.7702103347442, 266.16900485932365, 196.79333480275994, 1669.7617773886086, 491.5794228216274, 1135.0577151048583, 139.35376407776175, 144.06405144334232, 1589.3709815785048, 4062.072515999835, 219.72160760442827, 967.993438003933, 533.86664462481, 951.2680391620796, 768.8945822305782, 749.2154738745324, 378.7729670794409, 4457.491920683831, 1596.93235040314, 401.0013258246935, 820.5972463390633, 392.5164105139128, 820.3679215077844, 1475.1552120362526, 3376.870561629835, 639.4524733878212, 3094.6299578494595, 1510.505895439417, 2430.700762128256, 5720.490926989692, 1896.771616055793, 1594.9845879962763, 1960.8505708634693, 892.6551139163678, 836.487446547371, 4483.758392345063, 2049.654776354014, 5191.366925348812, 69.09737379776412, 52.075039943069704, 42.69339944378259, 37.0151208362765, 35.03108099253072, 34.332010872992825, 33.00152290155129, 31.72901294678896, 31.518978821211938, 28.31310567190274, 27.870997695452974, 27.190538110233998, 25.93640957832059, 25.413163403164557, 25.252743433109615, 25.241769840560075, 24.836404875949523, 24.426396733439542, 24.066814352901208, 23.988832436211677, 23.899499319465633, 23.884959962929102, 23.147652280603538, 23.097900346807755, 22.292648147722407, 22.022816232535334, 21.623026451878406, 21.55841323930814, 21.286297560701502, 21.12988063636197, 61.353041302771125, 141.2609911654772, 853.617245299313, 158.60206981823282, 91.65797724897855, 73.57213579390778, 59.077931071044716, 78.1402239967834, 216.81835755629015, 34.4749739827146, 348.9066288135821, 391.63682770416733, 94.92965400537875, 296.7453312414236, 376.90339707832277, 57.60974962678219, 969.7610522207178, 97.49295345684345, 179.0306740781387, 323.54017426433415, 94.38116021926976, 723.1754546852736, 13093.922825942574, 118.88107647482317, 234.36011850099783, 268.1791721815288, 534.0428406835763, 336.81050774763736, 136.58633700247526, 240.879542667055, 226.1205966980608, 3094.6299578494595, 253.55295157649022, 4457.491920683831, 351.9882812351842, 878.7123461246387, 5720.490926989692, 1135.0577151048583, 3392.1826896521366, 1282.10036006428, 480.48002982680714, 3376.870561629835, 605.6328458579752, 5911.146291334244, 1501.9750901632217, 4062.072515999835, 165.1711994317233, 81.98716784695289, 69.97073240299936, 60.380615514050035, 56.931317775875456, 38.9726061375952, 37.470173080760304, 35.2004814213426, 32.83709658653759, 32.34376411379404, 31.06320221690961, 29.052596910927374, 26.624044121080367, 26.16865928617555, 23.314529189185723, 22.87429290518711, 22.683415514837584, 22.614093712275505, 21.50443081852931, 21.0610913882419, 20.868991945743442, 20.98740732901584, 20.416164193763507, 20.308823200232233, 20.095771871352998, 19.698307412566873, 18.911079871403125, 18.724177445747568, 18.71206613693107, 18.57891517605718, 41.21374232445076, 121.3783405345657, 1336.4951296575846, 92.28547354859896, 62.400131415438366, 34.147231674199375, 76.15414592688217, 35.301231794127, 120.25432610097434, 43.57471838418959, 673.3702866065719, 143.75504281835933, 235.937607910513, 406.31615345899473, 64.27714464929682, 176.72716482862683, 3094.6299578494595, 389.43808516101507, 266.23130175711975, 151.57794011469653, 733.6585188654279, 1873.3983574019558, 969.7610522207178, 177.6575051203397, 237.30035273804307, 13093.922825942574, 148.72918121047164, 184.33010884583456, 1960.8505708634693, 1522.162277966899, 878.7123461246387, 3189.617221772076, 5720.490926989692, 3376.870561629835, 468.6390866422269, 349.1091450849482, 17421.055620238832, 388.03948219923905, 130.91861158344147, 109.45219593095742, 84.13159355560668, 78.11363790209893, 66.16930851170805, 62.60743013596044, 59.04663046304265, 54.435062493730214, 54.41771210238649, 53.49170541439725, 50.44331801399454, 46.65846013394028, 42.25647252922449, 41.083596982772754, 38.799929747403624, 32.72892600464542, 27.827498022070483, 25.77736111517869, 24.619331576828348, 22.788668333125106, 22.513166480774622, 22.10504447090355, 21.25271481547891, 20.402827412650343, 19.226252105836977, 18.840040757620237, 17.83114399071476, 16.932287055087876, 16.858691819483187, 15.161113895061808, 43.587050620669125, 252.02076796631906, 32.541247378095306, 40.67210312018754, 1022.8304035261114, 766.8999956094112, 589.1595608965401, 165.77154459163998, 121.39340326490907, 143.75504281835933, 32.77972709609544, 1282.10036006428, 53.67445376236531, 969.7610522207178, 249.84376000615808, 156.38219797918575, 459.08175148948294, 308.33560754200596, 37.166083176903236, 35.856395312550674, 32.19453617681307, 31.54191907229788, 30.173220760660737, 29.27797502983805, 24.77321196117537, 24.59760347646342, 21.06205261061656, 20.930866629906777, 19.45858684360839, 18.951207670480223, 18.82837795088089, 17.588087737681985, 16.85750899460852, 16.523687987102218, 16.313537276208034, 16.302940193918815, 15.626897790193299, 15.000578584180529, 14.621544004676679, 13.39696859525211, 13.240661093344608, 13.100186790457697, 12.993868328160712, 12.950209985079264, 12.373654875532745, 12.144234394040735, 11.86442987708201, 11.671206945006116, 870.4916972515653, 41.868040928404206, 28.085650721360945, 815.0164242949666, 221.27647516816876, 218.01080838756167, 92.83981405947401, 24.473457707362652, 38.25467778482059, 97.60826150227469, 294.72147604510633, 125.27297046501624, 417.55390608423744, 110.2992255393222, 384.13445893277645, 63.10282065841251, 38.815259778821535, 38.195305105612974, 37.6227057492926, 32.13882520989062, 31.968937648462067, 27.1206546144178, 25.419170876659223, 24.71741880607203, 24.54436891976222, 19.98233587902084, 17.674085027441322, 17.319580859989674, 17.155808384708703, 15.61804874152504, 15.465595395302039, 15.152968195907318, 15.148806719383304, 15.141390551076666, 14.82007391060591, 14.561449566352051, 14.537673531385854, 14.307854748856572, 14.000518285844532, 13.848880636379882, 13.291782085682904, 13.20659894557906, 13.166173031746855, 13.108825104306977, 36.996214065916185, 177.77515065017457, 40.2211911753207, 101.19698129308962, 24.37648064587568, 68.30838005503959, 54.31222323360787, 157.4651172778297, 105.57506296884422, 97.3585847018735, 44.95926526892131, 66.21343019900372, 46.04989497025139, 76.2556916777583, 42.31697642097142, 41.00556810243191, 40.894194562109796, 39.67553118327559, 30.904924209662997, 27.63939837832921, 26.764900487255, 23.94485301485986, 20.95327658780236, 19.19771565811128, 19.171351614337187, 18.933531710757077, 18.02829348472913, 17.56314494411723, 16.893497083995406, 16.6528696853357, 16.116353930844458, 15.955946828707742, 14.434894030111936, 13.723045710548863, 12.926759153661541, 12.527851804045058, 11.886763077578928, 11.840941772404479, 24.91707188715087, 11.298564151908591, 11.226085039841626, 10.953802990626931, 10.720267052783491, 76.22074140006822, 144.85260907954802, 266.8207436449846, 393.2541826193667, 75.9402493865192, 22.487084919594544, 56.664000573335045, 38.32243987105589, 66.13051883129549, 60.974574505398195, 1336.4951296575846, 177.6575051203397, 155.89405226709098, 173.64571826260087, 148.93182486749654, 53.447839901441675, 30.73896039124861, 27.79972944127385, 26.18205654134728, 24.385679595954226, 19.983620855484702, 18.69666511011023, 15.705054781853004, 14.831677705009987, 42.858206560496974, 12.013342355114187, 11.49524933711399, 11.291253876434103, 11.221119090028914, 11.953649417081436, 10.917494388476221, 10.72338839680617, 10.610076153761602, 10.40596371188289, 10.177325478013964, 10.088521555741602, 9.983615247412668, 9.693362864198493, 9.638332325568983, 8.601257536304782, 8.427699216246689, 7.8880535517900725, 7.8101607849718375, 7.496055631469786, 287.45993415773864, 118.00274814359767, 56.98472465753185, 16.18888587798721, 23.86554344144353, 58.92485560076725, 42.03207182092819, 101.75100435718743, 33.05482297426934, 22.48299539722758, 44.075477208723505, 121.43560576091899, 133.91709020479036, 78.96170529592705, 161.2011861906153, 397.5091334995543, 23.966721124756564, 179.66797024080307, 72.78924092933968, 33.10035644315972, 60.779615686087524, 55.71503307921062, 47.787700030515246, 36.264125071615204, 35.03650345793491, 26.233391640330307, 25.735038795762932, 20.579713302141766, 19.55851390310609, 18.271246162095256, 17.076332568918005, 16.956997679426802, 16.194263410995813, 14.645757886901674, 14.559314315043157, 14.470164885273201, 13.782681091994757, 13.662411078563037, 13.607768564364132, 13.244557056735333, 12.945229902115743, 12.50714006100699, 12.259806884545263, 12.251332501479453, 12.246214907433453, 11.705638652133153, 11.451918778120236, 10.493552459966182, 10.455056761275065, 10.44597483188012, 22.102539551279573, 23.60893562584943, 64.78731331235211, 23.467202896138406, 49.91948459500007, 16.260232293941637, 59.59039224210691, 54.92827974725645, 125.13170010708197, 558.9493739159341, 32.69493810576587, 81.49870970634156, 110.65283981584672, 159.018891834729, 41.00120396857189, 32.29647088404118, 25.189139669375216, 20.005703457097944, 17.299838880567854, 16.399569118097833, 15.616668266905355, 15.443147303428002, 15.390297913216504, 15.053778547450579, 14.818090308786056, 14.363741100324706, 13.734999373699093, 13.327865042777539, 13.06924036727446, 11.763662323718332, 11.347545948706703, 10.858499753852287, 10.855926560022365, 83.68982279239023, 10.310875838215454, 9.445165734488922, 9.310309302765647, 9.246952987681903, 9.202761357952323, 8.755208508465277, 8.669579960075826, 8.500697543890828, 8.381009698887032, 766.8999956094112, 29.48554735488098, 130.9841122565689, 44.95926526892131, 27.789234587070027, 73.38666190563725, 36.06216225114855, 96.16324649772335, 21.992773610134357, 107.78456174973022, 118.34078001391103, 44.61955768054362, 36.119349156302185, 31.79248311602135, 28.076096202064132, 22.096867836834228, 20.008723122889737, 19.98653412895162, 19.623321252721432, 18.9390137147384, 16.490694054124344, 15.790592523372545, 15.612838840983164, 15.487233843777926, 14.99168960477695, 12.714444795299498, 12.09761548012377, 11.292301939954951, 11.28688608171239, 11.220452187688354, 10.978080308985646, 10.853725386382282, 10.320695205789173, 10.114525922129143, 10.273757157384813, 9.558328280731379, 8.961222440276273, 8.816068476320364, 8.74045695691939, 8.601503797561158, 8.552766505532812, 32.045351475397354, 25.37784751218763, 34.74795087023597, 20.519923206980398, 237.72230364663085, 24.8158664462883, 74.41677377439578, 14.456598090532315, 20.938815735000553, 218.9618693781994, 143.9466837150608, 40.13615304146999, 237.30035273804307, 59.86408841742977, 41.11125663209532, 23.327817241376536, 21.722766292843804, 18.974169731098595, 17.659250690315137, 17.49013147527537, 16.20348096202438, 15.947563403807262, 14.907582205783756, 14.23581052692357, 13.647566335329907, 12.52775546217062, 11.776983269529271, 11.562834405162453, 11.543014502201208, 11.393968888746112, 11.391696739123608, 10.531629941955291, 10.122102817469663, 10.080263583026504, 10.014774067885698, 9.699422347525001, 9.35054218835836, 8.949255937694225, 8.35133964077666, 8.301191462999501, 8.2783183704057, 8.589568291385941, 8.081431531643188, 7.70618156629118, 89.28404115443729, 43.08675193966564, 17.622036959988343, 33.59890336475377, 33.537667161512566, 30.024087833574363, 24.608086885176597, 37.414294360163375, 15.63726329922421, 24.544603960437286, 31.270465262618934, 13.625075759956761, 132.84443171377714, 25.47279510311154, 22.093511693843993, 21.024295595148207, 18.287848381976662, 17.76931295464731, 17.731173614252867, 13.619130366183668, 13.00007746145381, 12.890224938282268, 12.682222086140683, 11.733464008878565, 11.367172735014087, 11.058798125984083, 10.919180905641683, 10.87848530724353, 10.761965541155895, 9.959519190643945, 9.928309528414237, 9.695786243645873, 9.665061167994025, 9.598190858651565, 9.505469482395021, 9.187592120395061, 8.800048621954597, 8.707471478363695, 7.788906398931993, 7.660757434443157, 7.543979104918334, 7.528187386965375, 7.415937435461257, 14.471977503735692, 63.61318956471098, 34.92343360178627, 41.861759048835594, 28.616757398382646, 397.5091334995543, 46.41609104950715, 836.487446547371, 1357.856286910924, 242.88007391408397, 54.80688764553548, 29.04412954216629, 28.316103529838507, 22.832303838335303, 20.847116994924214, 19.102945802632476, 16.479771848894984, 14.467613904389772, 13.265252113537679, 9.94891139425676, 9.74253673831015, 9.596658732322641, 8.949866103283265, 20.767723251230287, 8.323315406927746, 8.25993459809627, 8.189437770027036, 7.933436628209656, 7.853079441205355, 7.652137536983328, 6.8971815708846185, 6.675469528206254, 6.503206867110238, 6.388814974188229, 6.249347288990985, 6.166923044606588, 5.924638227772048, 5.919081580722891, 5.7850198485750095, 5.731427935544678, 7.496206247829843, 178.29842353466285, 39.672877535640886, 50.33565587309306, 11.133492582131483, 20.71290311011391, 36.91807619052747, 35.4950897869678, 15.65681689552183, 15.403626729869377, 15.277362968955567, 15.129594706921482, 14.941755169872142, 13.465352255540148, 12.510806577597918, 11.81820388780154, 9.944213516318666, 9.942586354166632, 9.807787515231121, 9.51520260023747, 9.028010790843625, 8.609184504312026, 8.608923478774585, 8.596864562303786, 8.426994085033028, 8.27656537671215, 7.895968519706897, 7.672947682705989, 7.549911982783171, 7.258388932032067, 6.689235876146873, 17.161254172192994, 6.328912517392756, 6.226553438000786, 5.846088638294064, 5.750423551087102, 5.545365595745545, 21.186379032542593, 9.116561661400176, 11.557214344850417, 21.928496370572294, 176.6312894616939, 48.008441708932565, 16.949203846568523, 36.07202110786402, 29.038791968079952, 60.5014188440906, 27.172052991603724, 15.176060509978749, 13.746347065278222, 12.935264801421818, 11.699266950439364, 10.967922635663967, 10.539215584867701, 9.536058695384718, 9.24662846661252, 9.19266061139183, 8.680029763916474, 8.32773184121562, 8.183995447084088, 8.133881941311786, 7.831920916678041, 7.765045261608277, 7.674636193555842, 6.7969540675862286, 6.7936686839912905, 6.784500636633076, 17.37374632828964, 6.70978970366764, 7.6393183817851735, 6.667681635061122, 6.595090170966796, 6.542590223757359, 6.392316502331934, 6.369773530804484, 6.307043388612908, 5.716577563456762, 12.16617308073918, 50.19835267071965, 27.93233483516286, 46.05031514848622, 42.68298407899685, 38.665766257063716, 31.33983777582837, 20.26071615424935, 10.861455748549679, 44.72989086290748, 8.960886052423582, 7.9852825970434305, 7.126543880811051, 6.625386259023779, 6.519523154674713, 6.481607329298488, 5.608727078139666, 5.365927110151543, 5.330938795560232, 4.565550521876536, 4.297230542884151, 32.62666942351033, 3.905876879741125, 3.823184198418419, 3.7828821447332888, 3.4982554579528427, 3.4060849746919826, 3.2409642449996654, 3.1994850724046873, 106.10670522369989, 3.1492438407223404, 3.1492438407223404, 2.954485143645345, 58.08179581677546, 13.539796183286228, 18.606564706075158, 36.81874668935606, 18.419014247664304, 25.201018408764764, 14.066082984712107, 605.6328458579752, 90.0506990128622, 235.937607910513, 20.246477662153275, 25.89155291839991, 19.45964328797574, 18.9443678647702, 14.523165161141714, 13.977356934739989, 13.898352772042275, 13.569279336552967, 11.235985658683479, 9.976976474447948, 9.204983969369321, 9.203779009612258, 9.11626746455005, 8.208559969235237, 8.070917169760039, 7.859966438426093, 7.132132201612839, 7.061077487632272, 6.843231884316647, 6.793412137591244, 6.586414963257792, 5.615418962863332, 5.571740277427331, 5.535170664999568, 5.437162321843915, 5.18307535811615, 5.151504085572101, 4.995359435480003, 4.9595878640449875, 4.919537011039597, 4.498306901146684, 7.089924673404172, 34.3031245844312, 26.876221565049445, 19.167373787138878, 15.173274181635689, 14.829342333788112, 14.12838961816479, 10.969210749199883, 8.598935326448427, 8.433813681571054, 8.288832332186253, 7.819059135169505, 7.283481768356285, 6.781390569525888, 6.759635920137727, 6.545249975243262, 6.436561926807501, 6.160708100473329, 5.9705122200112335, 5.833363137640205, 5.489876213601265, 5.48874970003456, 5.3958893249860065, 5.357569561816973, 5.32838023843858, 5.263697217858496, 5.00825600668302, 4.887877461155027, 4.559550191113738, 4.550716683890679, 4.53896560119486, 4.447432429524893, 4.446342519760356, 7.42380865362575, 10.350114007266962, 12.328381744341755, 29.809040800712403, 30.51656147760485, 23.006808438372587, 22.78400652248135, 20.22758920755058, 13.28964596765788, 31.245864461925237, 19.13156788982555, 12.479821142777078, 11.453318432958897, 10.303397418011281, 10.125916988688045, 8.415135637461598, 8.379302061776224, 8.171125337737223, 7.539627139741623, 6.939566865133367, 6.499110314033477, 6.315023325870342, 6.165548690931967, 6.133959986137492, 5.768512372913967, 5.553134499969738, 5.517226068266639, 5.432824838909042, 5.37404735910485, 4.811068975272754, 4.754838647819396, 4.655436807195375, 4.365553403093989, 4.304054372359621, 3.9186910482772093, 3.8811381902704256, 3.681594503818581, 3.6084214097769434, 3.5609209890431535, 100.97038855582386, 10.571484129985537, 17.94581882657284, 329.1933010185063, 99.53079401919217, 14.20994910029623, 12.729474499705034, 9.71541897311407, 8.97895764515778, 8.683530582739387, 7.722814695230201, 7.319931079621033, 6.4765185182706935, 6.451186379287825, 6.256032470627036, 6.18884731895282, 6.1082184722929, 6.009147939862539, 6.002342535552905, 5.045457827565543, 5.006556658371797, 4.909796869815044, 4.827614015571592, 4.793276391451518, 4.611556230536341, 4.321498439639153, 4.311624809122828, 4.184973812767577, 4.1597550723217696, 3.9211919052570816, 3.9083813521032966, 3.8255792821640546, 3.7586261498650084, 3.710509608768094, 3.556613121798083, 15.842003715987923, 23.406748413645303, 11.051873429579766, 34.204064350144016, 13.045736304843231, 242.88007391408397, 20.32929128249395, 10.145046927017722, 8.806935740726015, 7.209467516611802, 6.831890353065791, 6.80929794555804, 6.663759558483605, 6.4456566446173, 5.910771626632052, 5.599857991288596, 5.537211443291048, 5.443291093719926, 5.436746275898563, 5.060178440187214, 4.978654660503058, 4.94344701047093, 4.653698537053444, 4.601627341171364, 4.4694270813730785, 3.6954725550008076, 3.53364983348378, 3.469756083387939, 3.4453587230156044, 3.278680700882997, 3.2399932643770217, 3.087690257336388, 2.9651457047084246, 2.812904594674661, 2.6652359565973462, 2.6305422324434313, 26.79544788942461, 23.7394026508012, 45.183586100140474, 49.52974896260496, 14.758459249850238, 6.226814805012946, 6.1211605684927575, 5.574499129517307, 5.468861504389987, 5.366032288009172, 4.704144477931609, 4.639590438540743, 4.566658118151078, 4.4534251141932, 4.4404931448449805, 4.075856101644423, 4.006374614788888, 3.958001516521001, 3.8852829775352093, 3.46258008555222, 3.3397568911506057, 3.2648526900922636, 3.2474676035449286, 3.2433161997967193, 3.2430821958276135, 3.207138211156433, 3.1659782491245068, 2.856357052977396, 2.826943512407, 2.8055189684547, 2.542317406885851, 2.539817572817742, 2.492187653822107, 2.4570426410361836, 15.669731685591888, 14.929295092443096, 23.44761795523013, 11.840706727377826, 9.80260101829163, 9.311698160320603, 8.278822010433037, 6.301219001903797, 4.521038025823966, 4.240172948138382, 4.04715089079632, 3.6601006395473754, 3.457311058151283, 3.3397084495436915, 3.2524410017804875, 2.8957718042062557, 2.333279636631577, 2.1774106937389623, 2.0969036741928067, 2.084067545161153, 2.042412152836761, 1.994787744593258, 1.9936059245373536, 1.7705612715824353, 1.767457529906594, 1.7084071613882252, 1.5472473966246247, 1.5368662949901646, 1.3546556642419056, 1.3328503950212647, 1.1000651481801107, 0.890100708780629, 56.69355387657891, 7.790004573981767, 6.240426441068831, 4.591971091391089, 4.12113335403285, 3.1128656497181746, 2.795585694263407, 2.750863007290302, 2.745187552038795, 2.7429586584279626, 2.5437905692514735, 2.495519529726227, 2.4881615359628797, 2.402360786091685, 2.2380060699051847, 2.1762278314141588, 2.1699419649120144, 2.1480086562351373, 2.069623026763708, 2.0138784982791544, 1.9269448951930874, 1.9230183726071275, 1.7834304739280744, 1.7407675964131306, 1.7324195689932147, 1.7217989941809702, 1.66766915471518, 1.536369943681168, 1.5015751883438817, 1.4293133821432074, 1.4109719280212645, 17.400766943894528, 10.295125192314956, 21.266149332542142, 9.311156228440726, 4.868031159790169, 3.002954273165099, 2.800740102636018, 2.7925985285626824, 2.294044971371461, 2.218863429828243, 2.1617682605608834, 13.858429817355418, 1.5455017967616789, 1.4123034888185781, 4.133078464243228, 1.2981829267210294, 1.2709190115505618, 1.2513970261950327, 1.1916690240786334, 1.1268939929073047, 1.1268939929073047, 1.1240523731985954, 1.0547185481818562, 30.823552915693682, 0.8461028177937746, 0.8455124017869415, 0.8455124017869415, 18.348190222528075, 13.028685175477264, 8.766072910213836, 0.7058471337422313, 0.7058471337422313, 0.7058471337422313, 51.5710756015037, 71.03394500436447, 32.541247378095306], \"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic26\", \"Topic26\", \"Topic26\", \"Topic26\", \"Topic26\", \"Topic26\", \"Topic26\", \"Topic26\", \"Topic26\", \"Topic26\", \"Topic26\", \"Topic26\", \"Topic26\", \"Topic26\", \"Topic26\", \"Topic26\", \"Topic26\", \"Topic26\", \"Topic26\", \"Topic26\", \"Topic26\", \"Topic26\", \"Topic26\", \"Topic26\", \"Topic26\", \"Topic26\", \"Topic26\", \"Topic26\", \"Topic26\", \"Topic26\", \"Topic26\", \"Topic26\", \"Topic26\", \"Topic26\", \"Topic27\", \"Topic27\", \"Topic27\", \"Topic27\", \"Topic27\", \"Topic27\", \"Topic27\", \"Topic27\", \"Topic27\", \"Topic27\", \"Topic27\", \"Topic27\", \"Topic27\", \"Topic27\", \"Topic27\", \"Topic27\", \"Topic27\", \"Topic27\", \"Topic27\", \"Topic27\", \"Topic27\", \"Topic27\", \"Topic27\", \"Topic27\", \"Topic27\", \"Topic27\", \"Topic27\", \"Topic27\", \"Topic27\", \"Topic27\", \"Topic27\", \"Topic27\", \"Topic28\", \"Topic28\", \"Topic28\", \"Topic28\", \"Topic28\", \"Topic28\", \"Topic28\", \"Topic28\", \"Topic28\", \"Topic28\", \"Topic28\", \"Topic28\", \"Topic28\", \"Topic28\", \"Topic28\", \"Topic28\", \"Topic28\", \"Topic28\", \"Topic28\", \"Topic28\", \"Topic28\", \"Topic28\", \"Topic28\", \"Topic28\", \"Topic28\", \"Topic28\", \"Topic28\", \"Topic28\", \"Topic28\", \"Topic28\", \"Topic28\", \"Topic29\", \"Topic29\", \"Topic29\", \"Topic29\", \"Topic29\", \"Topic29\", \"Topic29\", \"Topic29\", \"Topic29\", \"Topic29\", \"Topic29\", \"Topic29\", \"Topic29\", \"Topic29\", \"Topic29\", \"Topic29\", \"Topic29\", \"Topic29\", \"Topic29\", \"Topic29\", \"Topic29\", \"Topic29\", \"Topic29\", \"Topic29\", \"Topic29\", \"Topic29\", \"Topic29\", \"Topic29\", \"Topic29\", \"Topic29\", \"Topic29\", \"Topic29\", \"Topic29\", \"Topic30\", \"Topic30\", \"Topic30\", \"Topic30\", \"Topic30\", \"Topic30\", \"Topic30\", \"Topic30\", \"Topic30\", \"Topic30\", \"Topic30\", \"Topic30\", \"Topic30\", \"Topic30\", \"Topic30\", \"Topic30\", \"Topic30\", \"Topic30\", \"Topic30\", \"Topic30\", \"Topic30\", \"Topic30\", \"Topic30\", \"Topic30\", \"Topic30\", \"Topic30\", \"Topic30\", \"Topic30\", \"Topic30\", \"Topic30\", \"Topic30\", \"Topic30\", \"Topic30\"], \"logprob\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -5.4035, -6.128, -6.3375, -6.728, -6.782, -6.9867, -7.3265, -6.2745, -7.626, -7.6714, -7.726, -7.7506, -7.7882, -7.8974, -7.9415, -7.9422, -7.9684, -8.0169, -8.0425, -8.0454, -8.0575, -8.0664, -8.0936, -8.1323, -8.1885, -6.9552, -8.2284, -8.2699, -8.277, -8.2987, -7.6267, -7.0979, -7.9081, -4.1294, -6.8965, -6.0133, -5.9114, -7.0169, -7.4812, -5.5125, -6.5465, -7.0338, -6.8132, -6.079, -7.352, -6.081, -6.6815, -5.2593, -6.1743, -5.4169, -4.8725, -3.9477, -5.9791, -6.9143, -3.1299, -5.6922, -4.8952, -4.4919, -4.765, -4.8169, -5.2785, -4.5569, -5.0793, -5.6784, -5.2755, -4.8377, -4.7186, -5.1469, -4.8732, -5.3961, -5.6389, -4.8183, -5.473, -5.2615, -5.6273, -5.2087, -5.4091, -4.8249, -5.3697, -5.2463, -5.3727, -5.4873, -6.4731, -7.7329, -7.6856, -8.0216, -8.0912, -8.161, -8.2403, -8.2556, -8.2933, -8.2974, -8.3918, -8.4155, -8.4344, -8.5095, -8.5423, -8.5702, -8.5647, -8.5822, -8.6123, -8.6386, -8.6417, -8.6985, -8.7186, -8.7478, -8.7604, -7.9647, -8.779, -8.886, -8.8909, -8.9114, -6.8209, -5.2135, -7.6286, -6.298, -5.1057, -5.2579, -4.8046, -6.7077, -7.9212, -6.3148, -5.5665, -5.3808, -2.468, -4.0297, -7.1002, -6.1726, -3.8414, -7.9607, -5.6023, -3.6724, -5.2972, -5.5219, -4.2295, -4.1952, -5.6735, -6.3287, -4.2784, -6.4455, -6.2452, -6.0487, -5.0953, -5.381, -6.0186, -6.1727, -5.3035, -4.3067, -4.1342, -4.4155, -5.0041, -3.888, -4.5472, -5.2787, -5.0968, -4.9964, -5.7092, -4.9542, -5.2107, -5.0634, -5.2271, -5.3037, -5.2165, -5.3406, -7.2898, -7.2993, -7.3569, -7.4898, -7.5511, -7.5609, -7.6358, -7.6997, -7.7423, -7.7447, -7.7782, -7.7919, -7.8124, -7.8566, -7.8623, -7.7548, -7.9718, -7.9867, -7.9643, -8.0498, -8.0542, -8.0582, -8.0732, -8.0788, -8.0848, -8.1218, -8.1756, -8.185, -8.1855, -8.1927, -7.0237, -6.8519, -6.9207, -7.5115, -7.1642, -7.4456, -5.6187, -7.2851, -7.2769, -7.0209, -5.1982, -5.276, -6.6169, -5.9376, -6.0223, -6.727, -4.3763, -5.758, -6.1232, -5.9064, -6.5887, -6.4773, -5.2331, -6.278, -6.7796, -5.0985, -6.4398, -5.8714, -4.3968, -5.4223, -5.8586, -5.408, -5.348, -5.4213, -6.0295, -6.1107, -5.8807, -5.5343, -5.065, -5.3233, -6.1394, -5.952, -6.0871, -5.7936, -5.1443, -4.2941, -5.7097, -5.5032, -5.3533, -5.5109, -5.6796, -5.0758, -5.1117, -5.2491, -5.3785, -5.4478, -5.4601, -5.3275, -5.3394, -5.6194, -5.5448, -5.6231, -5.4827, -5.6046, -7.063, -7.1298, -7.1441, -7.2858, -7.2907, -7.3462, -7.3984, -7.4141, -7.5304, -7.5573, -7.633, -7.6382, -7.6417, -7.6518, -7.6577, -7.5882, -7.7159, -7.7551, -7.7778, -7.7931, -7.8161, -7.8209, -7.8248, -7.8335, -7.8393, -7.8331, -7.8395, -6.6612, -7.8498, -7.8512, -7.3068, -6.9771, -6.6388, -7.0861, -6.8651, -6.7504, -7.4446, -7.3111, -6.7046, -7.3802, -6.7626, -6.6508, -6.928, -3.4846, -6.4834, -6.8712, -5.7559, -5.9238, -6.7061, -6.4748, -6.6767, -6.9978, -6.7409, -6.6429, -6.9178, -5.7528, -6.5194, -6.2523, -6.1237, -6.4184, -5.6388, -4.9458, -5.6569, -5.9877, -6.5531, -4.8789, -5.1063, -6.4443, -6.2366, -5.8616, -5.4325, -5.5661, -5.851, -6.026, -5.9227, -6.0754, -5.4629, -6.0701, -5.3892, -6.0193, -5.5198, -5.4236, -5.8158, -5.8086, -5.8129, -5.8563, -5.7457, -5.8147, -5.7215, -5.9382, -5.6902, -5.806, -5.5296, -5.9035, -5.7177, -5.821, -5.9147, -6.8556, -6.8881, -7.327, -7.4976, -7.5228, -7.4929, -7.5327, -7.5707, -7.5763, -7.604, -7.6523, -7.6368, -7.763, -7.7883, -7.7895, -7.7946, -7.7983, -7.8145, -7.8405, -7.8382, -7.8593, -7.8711, -7.8737, -7.8742, -7.8913, -7.8652, -7.9122, -7.9424, -7.9569, -7.9577, -7.0166, -7.5842, -7.7151, -7.4246, -6.4298, -6.8718, -6.6774, -6.2159, -4.5065, -6.7686, -7.1123, -7.1879, -6.3165, -2.7096, -7.2701, -7.4389, -5.1051, -6.2521, -6.8616, -6.3877, -6.4151, -6.323, -6.2108, -6.4545, -4.8366, -5.8192, -5.2055, -6.7564, -6.7351, -5.0253, -4.3606, -6.435, -5.3878, -5.8395, -5.4521, -5.6014, -5.633, -6.095, -4.543, -5.271, -6.0842, -5.679, -6.1093, -5.7204, -5.4209, -5.0386, -5.9018, -5.2398, -5.6068, -5.5122, -5.2283, -5.6218, -5.6886, -5.6302, -5.8785, -5.9066, -5.5283, -5.8452, -5.8454, -6.2127, -6.4987, -6.7002, -6.8454, -6.9015, -6.9221, -6.9624, -7.0026, -7.0094, -7.1191, -7.1352, -7.1605, -7.209, -7.2299, -7.2364, -7.2369, -7.2535, -7.2706, -7.2859, -7.2892, -7.293, -7.2937, -7.326, -7.3282, -7.3647, -7.3773, -7.3962, -7.3993, -7.4124, -7.42, -6.3611, -5.5519, -3.8734, -5.4889, -6.0234, -6.2382, -6.4781, -6.2793, -5.4112, -6.9832, -5.1023, -5.0222, -6.1844, -5.3351, -5.2011, -6.6169, -4.5333, -6.2364, -5.8189, -5.424, -6.2848, -4.9462, -3.125, -6.1798, -5.7621, -5.6925, -5.3005, -5.5857, -6.1461, -5.8598, -5.9505, -5.0218, -5.9338, -4.948, -5.8192, -5.5336, -5.0358, -5.532, -5.2578, -5.581, -5.8122, -5.4324, -5.7749, -5.4222, -5.6518, -5.5898, -4.5883, -5.2929, -5.4528, -5.6018, -5.6613, -6.0458, -6.0858, -6.1495, -6.2204, -6.2359, -6.2771, -6.3456, -6.435, -6.4528, -6.5715, -6.5911, -6.5998, -6.6029, -6.6548, -6.6763, -6.6858, -6.6807, -6.7085, -6.7139, -6.7248, -6.7455, -6.7878, -6.7981, -6.7987, -6.8062, -6.1108, -5.1897, -3.2058, -5.457, -5.7963, -6.3031, -5.6958, -6.298, -5.4319, -6.1636, -4.2738, -5.3765, -5.0617, -4.794, -5.9521, -5.3582, -4.0012, -5.1238, -5.3146, -5.6077, -4.9089, -4.5014, -4.8123, -5.5854, -5.4871, -4.0573, -5.6716, -5.6287, -4.9535, -5.0381, -5.2401, -5.1139, -5.0404, -5.1762, -5.474, -5.5123, -5.1797, -5.5546, -3.9113, -4.0914, -4.3564, -4.4312, -4.5987, -4.6547, -4.7139, -4.7962, -4.7965, -4.8139, -4.8733, -4.9524, -5.0531, -5.0817, -5.1399, -5.3133, -5.4793, -5.5578, -5.6051, -5.6846, -5.6971, -5.716, -5.7566, -5.7988, -5.8603, -5.8813, -5.9385, -5.9923, -5.9969, -6.1077, -5.1281, -3.5461, -5.4175, -5.2823, -2.9699, -3.2635, -3.6333, -4.4531, -4.6809, -4.6796, -5.5113, -4.0392, -5.3166, -4.528, -4.9755, -5.0845, -4.9153, -5.2719, -4.8417, -4.8782, -4.9881, -5.0091, -5.0544, -5.0852, -5.2566, -5.2639, -5.4239, -5.4303, -5.5058, -5.5332, -5.5399, -5.6107, -5.6548, -5.6757, -5.689, -5.6897, -5.7339, -5.7767, -5.8035, -5.8955, -5.9078, -5.9191, -5.9277, -5.9312, -5.9793, -5.9991, -6.0238, -6.0412, -2.005, -4.9432, -5.3291, -3.3688, -4.257, -4.5513, -5.2042, -5.7317, -5.6675, -5.525, -5.3769, -5.5387, -5.519, -5.6691, -2.4091, -4.2244, -4.7172, -4.7336, -4.749, -4.9096, -4.9151, -5.0834, -5.1499, -5.1787, -5.1859, -5.3981, -5.5254, -5.5465, -5.5564, -5.6544, -5.6646, -5.686, -5.6863, -5.6868, -5.7092, -5.7277, -5.7294, -5.7461, -5.7689, -5.7804, -5.8236, -5.8304, -5.8336, -5.8382, -4.9731, -3.9778, -5.0521, -4.4913, -5.4357, -4.824, -5.1203, -4.7852, -4.986, -5.2207, -5.4687, -5.4716, -5.6581, -3.996, -4.5921, -4.6241, -4.6269, -4.6576, -4.9124, -5.0267, -5.0597, -5.1741, -5.3117, -5.4023, -5.4037, -5.4167, -5.4675, -5.4947, -5.5352, -5.5501, -5.5843, -5.5947, -5.6996, -5.7527, -5.8157, -5.8488, -5.9044, -5.9085, -5.1662, -5.9583, -5.9652, -5.9913, -6.0143, -4.258, -4.127, -3.7728, -3.674, -4.7455, -5.5605, -5.105, -5.3017, -5.0607, -5.1823, -4.3003, -5.1662, -5.4607, -5.4573, -3.0913, -4.1243, -4.6871, -4.79, -4.8515, -4.9245, -5.1299, -5.1989, -5.3805, -5.4404, -4.3832, -5.6625, -5.7092, -5.7283, -5.7349, -5.6732, -5.7641, -5.7833, -5.7946, -5.8154, -5.8392, -5.8486, -5.8598, -5.8915, -5.8976, -6.0206, -6.0428, -6.115, -6.1258, -6.1709, -2.5299, -3.6261, -4.3744, -5.5372, -5.2817, -4.7382, -4.9654, -4.4554, -5.1778, -5.4085, -5.228, -4.8997, -4.9046, -5.2619, -5.1539, -5.033, -5.6039, -5.3643, -5.489, -5.5967, -3.9275, -4.0168, -4.1711, -4.4516, -4.4868, -4.7828, -4.8025, -5.0329, -5.0855, -5.1562, -5.2265, -5.2338, -5.2818, -5.387, -5.3932, -5.3996, -5.4508, -5.46, -5.4642, -5.4927, -5.5176, -5.5532, -5.5743, -5.5751, -5.5755, -5.6234, -5.6466, -5.7398, -5.7438, -5.7447, -5.0168, -5.0329, -4.2658, -5.1726, -4.7079, -5.4208, -4.816, -4.9199, -4.6141, -4.5208, -5.2703, -5.0947, -5.3948, -2.9146, -4.2825, -4.5257, -4.7803, -5.0179, -5.1688, -5.2245, -5.2756, -5.2872, -5.2908, -5.314, -5.3305, -5.3632, -5.4102, -5.4419, -5.4626, -5.5739, -5.6122, -5.6591, -5.6594, -3.6183, -5.7144, -5.8086, -5.8242, -5.8315, -5.8367, -5.8906, -5.9013, -5.9227, -5.9381, -1.9173, -4.8352, -3.7372, -4.7091, -5.1309, -4.6895, -5.1568, -4.7538, -5.5622, -5.2729, -5.4151, -4.1953, -4.4103, -4.5405, -4.6677, -4.914, -5.0166, -5.0177, -5.0367, -5.0735, -5.2175, -5.2628, -5.2746, -5.2831, -5.3171, -5.4904, -5.543, -5.6162, -5.6167, -5.623, -5.6463, -5.6584, -5.7122, -5.7339, -5.7202, -5.7946, -5.8643, -5.8819, -5.8913, -5.9087, -5.9148, -4.6405, -4.977, -4.7547, -5.1936, -3.811, -5.2517, -4.815, -5.5689, -5.4294, -4.6945, -5.1976, -5.4404, -5.4088, -5.5346, -4.1856, -4.7652, -4.8387, -4.9787, -5.0533, -5.0633, -5.143, -5.1596, -5.2301, -5.2785, -5.3229, -5.4132, -5.4787, -5.4982, -5.5, -5.5138, -5.514, -5.5978, -5.6402, -5.6447, -5.6517, -5.686, -5.7255, -5.7729, -5.848, -5.8545, -5.8575, -5.8218, -5.8838, -5.9358, -3.5209, -4.3205, -5.1998, -4.6436, -4.8045, -4.9541, -5.1735, -5.0536, -5.506, -5.4358, -5.3795, -5.5783, -5.1721, -4.3333, -4.4798, -4.531, -4.6755, -4.7054, -4.7077, -4.9837, -5.0328, -5.0417, -5.0589, -5.1413, -5.175, -5.2043, -5.2179, -5.2218, -5.2333, -5.3163, -5.3197, -5.3452, -5.3486, -5.3561, -5.3665, -5.4032, -5.4499, -5.4613, -5.5829, -5.6011, -5.618, -5.6203, -5.6369, -5.1315, -4.0474, -4.7455, -4.6771, -4.9911, -4.1177, -5.082, -4.6938, -4.8469, -5.3279, -3.4282, -4.0745, -4.1005, -4.3218, -4.4157, -4.5062, -4.6598, -4.7961, -4.8874, -5.1934, -5.216, -5.2322, -5.3076, -4.4692, -5.3864, -5.3947, -5.4041, -5.4388, -5.45, -5.4784, -5.5931, -5.6294, -5.6586, -5.6785, -5.7032, -5.7184, -5.7634, -5.7644, -5.7904, -5.801, -5.5491, -3.2837, -4.6208, -4.6029, -5.4556, -5.5054, -5.66, -3.8637, -4.7075, -4.7246, -4.7332, -4.7434, -4.7565, -4.8658, -4.9434, -5.0038, -5.1881, -5.1883, -5.203, -5.2356, -5.2923, -5.3438, -5.3439, -5.3454, -5.3671, -5.3867, -5.4382, -5.4696, -5.4874, -5.5308, -5.6213, -4.6798, -5.6832, -5.7016, -5.7727, -5.7914, -5.8327, -4.779, -5.4734, -5.343, -5.0844, -4.307, -4.8728, -5.2965, -5.113, -5.2418, -5.3904, -4.0237, -4.6269, -4.7308, -4.7949, -4.9013, -4.97, -5.0126, -5.12, -5.1532, -5.1595, -5.2217, -5.2668, -5.2858, -5.2925, -5.3339, -5.3433, -5.3561, -5.4904, -5.4909, -5.4924, -4.5532, -5.5047, -5.3751, -5.5117, -5.524, -5.5329, -5.5589, -5.5628, -5.5739, -5.685, -4.9371, -3.8564, -4.5402, -3.4506, -3.5278, -3.6283, -3.8426, -4.2912, -4.9456, -3.5312, -5.1524, -5.2779, -5.4031, -5.4841, -5.5021, -5.5087, -5.672, -5.7226, -5.7301, -5.9103, -5.982, -3.9679, -6.0968, -6.1229, -6.1358, -6.2323, -6.2657, -6.3283, -6.3447, -2.8433, -6.3649, -6.3649, -6.4474, -3.6233, -5.2429, -5.068, -5.0676, -5.4454, -5.4897, -5.7471, -4.8483, -5.5812, -5.4794, -5.8732, -3.986, -4.2806, -4.3084, -4.5857, -4.626, -4.6319, -4.6572, -4.857, -4.9841, -5.0709, -5.071, -5.0813, -5.1953, -5.2137, -5.2427, -5.3497, -5.3608, -5.3956, -5.4037, -5.4382, -5.6181, -5.627, -5.6345, -5.6549, -5.7099, -5.7169, -5.7526, -5.7609, -5.7703, -5.8752, -5.6064, -4.6822, -4.8981, -4.2951, -4.5386, -4.5626, -4.6135, -4.8814, -5.1435, -5.1646, -5.1835, -5.2473, -5.3253, -5.4045, -5.4081, -5.444, -5.4628, -5.5119, -5.5473, -5.5736, -5.6427, -5.6429, -5.6625, -5.6706, -5.6769, -5.6909, -5.7484, -5.7766, -5.858, -5.8603, -5.8634, -5.8874, -5.8877, -5.3767, -5.2137, -5.0962, -4.5063, -4.6694, -5.2491, -5.2642, -5.4343, -5.5204, -3.6264, -4.1313, -4.5787, -4.6698, -4.7828, -4.8014, -5.0014, -5.0061, -5.0335, -5.1217, -5.2134, -5.2865, -5.3187, -5.3456, -5.3514, -5.4209, -5.4642, -5.4716, -5.4892, -5.5017, -5.6299, -5.6435, -5.6682, -5.7441, -5.761, -5.8737, -5.8854, -5.95, -5.9748, -5.9912, -3.1631, -5.3438, -5.5237, -4.8013, -5.1891, -4.3695, -4.4854, -4.7736, -4.8587, -4.895, -5.023, -5.082, -5.2181, -5.2225, -5.2569, -5.2691, -5.2839, -5.3023, -5.3036, -5.5022, -5.5112, -5.5338, -5.5535, -5.5618, -5.6071, -5.684, -5.6867, -5.7223, -5.7295, -5.8008, -5.8048, -5.8308, -5.8524, -5.8682, -5.9205, -4.4618, -4.2852, -4.9304, -5.1857, -5.7046, -5.6998, -3.9, -4.6311, -4.7837, -5.0028, -5.0625, -5.0662, -5.0903, -5.1275, -5.225, -5.2864, -5.2993, -5.3188, -5.3202, -5.4029, -5.4217, -5.43, -5.5005, -5.5137, -5.5481, -5.7773, -5.8327, -5.8554, -5.8642, -5.9266, -5.9417, -6.0033, -6.0558, -6.125, -6.1972, -6.2149, -3.9172, -4.4764, -4.8564, -5.1468, -4.0098, -4.9426, -4.9619, -5.0681, -5.0899, -5.1117, -5.2644, -5.2807, -5.2993, -5.329, -5.3324, -5.4348, -5.4555, -5.4703, -5.4928, -5.6348, -5.6802, -5.7089, -5.7157, -5.7173, -5.7174, -5.7316, -5.7481, -5.8819, -5.8955, -5.9056, -6.0387, -6.0401, -6.0662, -6.0859, -5.1639, -5.1945, -3.4038, -4.1174, -4.3193, -4.3747, -4.5024, -4.8045, -5.1864, -5.2627, -5.3186, -5.4413, -5.5121, -5.5556, -5.5891, -5.7392, -6.0342, -6.1341, -6.1899, -6.1991, -6.2296, -6.2655, -6.2664, -6.4547, -6.4576, -6.5142, -6.6872, -6.6994, -6.9429, -6.9765, -7.4296, -8.1572, -7.0878, -4.4276, -4.6741, -5.0268, -5.1556, -5.5042, -5.6448, -5.6663, -5.6691, -5.6702, -5.7725, -5.799, -5.8031, -5.8521, -5.9533, -5.9942, -5.9984, -6.0134, -6.0689, -6.1103, -6.1785, -6.1817, -6.3023, -6.3423, -6.3503, -6.3606, -6.4148, -6.5598, -6.602, -6.6959, -6.7212, -4.8, -5.7462, -5.6153, -4.042, -4.7672, -5.3598, -5.4515, -5.4554, -5.7271, -5.7753, -5.8135, -4.0055, -6.3594, -6.5301, -5.4903, -6.7038, -6.7502, -6.7848, -6.8987, -7.039, -7.039, -7.0456, -7.2226, -4.451, -8.0967, -8.1006, -8.1006, -6.0295, -6.4648, -7.3124, -10.892, -10.892, -10.892, -8.1006, -10.892, -10.892], \"loglift\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 1.5285, 1.5274, 1.5269, 1.5257, 1.5255, 1.5245, 1.5225, 1.5206, 1.5201, 1.5197, 1.5191, 1.5188, 1.5184, 1.5172, 1.5166, 1.5166, 1.5163, 1.5156, 1.5152, 1.5152, 1.515, 1.5148, 1.5145, 1.5139, 1.513, 1.5124, 1.5124, 1.5116, 1.5115, 1.5111, 1.5059, 1.4984, 1.5062, 1.4105, 1.4815, 1.4526, 1.4404, 1.4749, 1.4909, 1.4006, 1.4435, 1.4681, 1.4541, 1.4042, 1.4766, 1.3662, 1.4074, 1.2275, 1.3182, 1.1896, 1.0051, 0.7809, 1.1949, 1.3682, 0.5179, 1.0937, 0.8054, 0.6119, 0.6987, 0.6066, 0.7858, 0.4481, 0.6685, 0.8842, 0.6724, 0.4182, 0.3183, 0.5685, 0.3777, 0.6892, 0.8561, -0.0569, 0.6437, 0.3859, 0.8114, 0.1671, 0.3786, -0.8916, 0.2177, -0.2355, -0.5143, -0.2033, 1.6661, 1.6574, 1.6571, 1.6534, 1.6523, 1.651, 1.6495, 1.6492, 1.6485, 1.6484, 1.6463, 1.6457, 1.6453, 1.6434, 1.6426, 1.6418, 1.6418, 1.6415, 1.6407, 1.6399, 1.6398, 1.6381, 1.6375, 1.6365, 1.6361, 1.6356, 1.6355, 1.6317, 1.6315, 1.6308, 1.6053, 1.558, 1.6063, 1.5698, 1.5328, 1.5145, 1.464, 1.5428, 1.6022, 1.5156, 1.4529, 1.4357, 1.1798, 1.2588, 1.5227, 1.4304, 1.1955, 1.5956, 1.33, 1.089, 1.2723, 1.2932, 1.116, 1.0887, 1.255, 1.3489, 0.9775, 1.3618, 1.3236, 1.2796, 1.0549, 1.1185, 1.266, 1.2941, 1.0355, 0.7042, 0.5944, 0.5895, 0.7837, 0.0453, 0.3113, 0.7634, 0.5506, 0.4673, 1.0393, 0.2966, 0.537, 0.0403, 0.3603, 0.5576, 0.2071, 0.3748, 1.7233, 1.7232, 1.7228, 1.7215, 1.7209, 1.7208, 1.7195, 1.7192, 1.7186, 1.7186, 1.7182, 1.718, 1.7177, 1.7171, 1.717, 1.7154, 1.7153, 1.7151, 1.7147, 1.714, 1.7139, 1.7139, 1.7136, 1.7135, 1.7134, 1.7127, 1.7117, 1.7115, 1.7115, 1.7113, 1.71, 1.699, 1.6913, 1.6937, 1.6677, 1.6801, 1.5718, 1.6602, 1.6568, 1.627, 1.444, 1.4429, 1.5681, 1.4904, 1.4872, 1.5686, 1.2409, 1.4224, 1.4701, 1.4304, 1.5347, 1.4906, 1.2187, 1.4266, 1.5215, 1.1011, 1.4352, 1.2753, 0.854, 1.1359, 1.2474, 1.1091, 1.0362, 1.0245, 1.2449, 1.2734, 1.1706, 1.007, 0.7671, 0.8415, 1.2693, 1.1351, 1.2211, 0.9944, 0.4431, -0.3608, 0.8655, 0.6406, 0.4105, 0.5882, 0.7816, -0.0649, -0.1067, 0.0965, 0.2942, 0.4136, 0.4052, 0.0961, -0.0835, 0.629, 0.1026, 0.4412, -0.7541, -0.3206, 2.0042, 2.0036, 2.0034, 2.002, 2.0019, 2.0013, 2.0007, 2.0005, 1.999, 1.9986, 1.9975, 1.9974, 1.9974, 1.9972, 1.9971, 1.9968, 1.9962, 1.9955, 1.9951, 1.9949, 1.9945, 1.9944, 1.9943, 1.9941, 1.994, 1.994, 1.994, 1.994, 1.9938, 1.9938, 1.9834, 1.9611, 1.9378, 1.9574, 1.945, 1.9285, 1.9708, 1.9606, 1.9094, 1.9604, 1.905, 1.8836, 1.9036, 1.3739, 1.8163, 1.8771, 1.6828, 1.7082, 1.8327, 1.7821, 1.8217, 1.8747, 1.8091, 1.7736, 1.8508, 1.4724, 1.7204, 1.619, 1.573, 1.6749, 1.2652, 0.8863, 1.234, 1.3465, 1.6683, 0.4969, 0.5664, 1.5382, 1.3501, 1.0242, 0.6061, 0.6908, 0.9414, 1.0962, 0.9782, 1.1295, 0.3009, 1.0502, 0.0745, 0.9612, 0.1956, -0.0001, 0.6372, 0.5733, 0.5134, 0.5955, 0.3534, 0.4436, 0.1398, 0.7191, -0.0428, 0.2927, -0.7683, 0.5014, -0.9891, -0.816, 0.0332, 2.0092, 2.0083, 2.0053, 2.0032, 2.0029, 2.0027, 2.0027, 2.0022, 2.0021, 2.0017, 2.001, 2.0009, 1.9991, 1.9987, 1.9987, 1.9986, 1.9985, 1.9982, 1.9977, 1.9977, 1.9974, 1.9972, 1.9971, 1.9971, 1.9968, 1.9964, 1.9963, 1.9957, 1.9954, 1.9954, 1.9949, 1.9836, 1.9863, 1.9627, 1.8858, 1.9154, 1.8891, 1.8095, 1.5935, 1.8711, 1.9028, 1.9106, 1.7836, 1.2236, 1.9176, 1.9374, 1.5137, 1.714, 1.8097, 1.6955, 1.6934, 1.6574, 1.6183, 1.6765, 1.1562, 1.3964, 1.1732, 1.7198, 1.7079, 1.0169, 0.7432, 1.5858, 1.1502, 1.2936, 1.1033, 1.1668, 1.1612, 1.3812, 0.4679, 0.7663, 1.335, 1.0242, 1.3313, 0.983, 0.6958, 0.2498, 1.0508, 0.136, 0.4862, 0.1051, -0.4669, 0.2435, 0.35, 0.2019, 0.7405, 0.7774, -0.5233, -0.0575, -0.9869, 2.965, 2.9618, 2.9589, 2.9565, 2.9554, 2.955, 2.9542, 2.9534, 2.9532, 2.9508, 2.9504, 2.9498, 2.9486, 2.948, 2.9478, 2.9478, 2.9474, 2.9469, 2.9465, 2.9464, 2.9463, 2.9463, 2.9454, 2.9453, 2.9442, 2.9438, 2.9432, 2.9431, 2.9427, 2.9425, 2.9355, 2.9107, 2.7903, 2.8578, 2.8718, 2.8768, 2.8563, 2.7754, 2.6229, 2.8897, 2.4561, 2.4206, 2.6756, 2.3852, 2.2801, 2.7426, 2.0029, 2.597, 2.4067, 2.2099, 2.581, 1.8833, 0.8083, 2.4552, 2.1943, 2.129, 1.8322, 2.008, 2.3501, 2.0691, 2.0417, 0.354, 1.9438, 0.0629, 1.7304, 1.1012, -0.2744, 0.8467, 0.0262, 0.676, 1.4262, -0.1439, 1.232, -0.6936, 0.4469, -0.4861, 3.7179, 3.7137, 3.7123, 3.7108, 3.7101, 3.7045, 3.7038, 3.7026, 3.7012, 3.7009, 3.7, 3.6985, 3.6963, 3.6959, 3.6926, 3.6921, 3.6918, 3.6917, 3.6901, 3.6894, 3.6891, 3.6886, 3.6884, 3.6882, 3.6879, 3.6872, 3.6857, 3.6853, 3.6853, 3.685, 3.5836, 3.4246, 3.0096, 3.4313, 3.4833, 3.5794, 3.3847, 3.5512, 3.1917, 3.4751, 2.6271, 3.0686, 2.8879, 2.612, 3.2979, 2.8804, 1.3746, 2.3247, 2.5142, 2.7844, 1.9062, 1.3763, 1.7238, 2.648, 2.4567, -0.124, 2.7394, 2.5677, 0.8786, 1.0473, 1.3946, 0.2317, -0.279, 0.1123, 1.7894, 2.0455, -1.5319, 1.8975, 4.6274, 4.6263, 4.6244, 4.6238, 4.6222, 4.6217, 4.621, 4.62, 4.62, 4.6198, 4.619, 4.6179, 4.6164, 4.6159, 4.6149, 4.6116, 4.6079, 4.6059, 4.6046, 4.6023, 4.602, 4.6014, 4.6001, 4.5987, 4.5966, 4.5959, 4.5938, 4.5917, 4.5915, 4.5868, 4.5104, 4.3376, 4.5132, 4.4253, 3.513, 3.5074, 3.4012, 3.8495, 3.9333, 3.7654, 4.4121, 2.2177, 4.1136, 2.0081, 2.9169, 3.2763, 2.3686, 2.4101, 4.9561, 4.9555, 4.9533, 4.9528, 4.9518, 4.9511, 4.9468, 4.9466, 4.9419, 4.9417, 4.9391, 4.9382, 4.9379, 4.9353, 4.9336, 4.9327, 4.9322, 4.9321, 4.9303, 4.9284, 4.9272, 4.9227, 4.9221, 4.9215, 4.9211, 4.9209, 4.9183, 4.9172, 4.9158, 4.9148, 4.6392, 4.7355, 4.7489, 3.3412, 3.7568, 3.4773, 3.6781, 4.4839, 4.1014, 3.3072, 2.3503, 3.044, 1.8598, 3.0409, 5.0531, 5.044, 5.0372, 5.0369, 5.0366, 5.0335, 5.0334, 5.0295, 5.0278, 5.027, 5.0268, 5.0203, 5.0157, 5.0149, 5.0145, 5.0104, 5.01, 5.009, 5.009, 5.009, 5.008, 5.0071, 5.007, 5.0062, 5.0052, 5.0046, 5.0024, 5.0021, 5.0019, 5.0017, 4.8292, 4.2549, 4.6667, 4.3048, 4.7839, 4.3651, 4.2982, 3.5688, 3.7677, 3.6141, 4.1388, 3.7487, 3.9253, 5.0831, 5.0759, 5.0754, 5.0754, 5.0748, 5.0699, 5.0672, 5.0664, 5.0633, 5.0592, 5.0561, 5.0561, 5.0556, 5.0537, 5.0527, 5.0511, 5.0505, 5.0491, 5.0486, 5.044, 5.0414, 5.0382, 5.0364, 5.0333, 5.0331, 5.0315, 5.0302, 5.0298, 5.0282, 5.0267, 4.8216, 4.3105, 4.0538, 3.7647, 4.3378, 4.7398, 4.2711, 4.4654, 4.1608, 4.1205, 1.9151, 3.0671, 2.9034, 2.7988, 5.3184, 5.3101, 5.3006, 5.2982, 5.2966, 5.2947, 5.2883, 5.2859, 5.2787, 5.276, 5.2721, 5.2647, 5.262, 5.2609, 5.2605, 5.259, 5.2587, 5.2575, 5.2568, 5.2554, 5.2538, 5.2532, 5.2525, 5.2503, 5.2498, 5.2407, 5.2389, 5.2329, 5.2319, 5.2279, 5.2222, 5.0164, 4.9961, 5.0916, 4.9591, 4.5987, 4.7094, 4.3353, 4.7373, 4.8919, 4.3993, 3.7141, 3.6114, 3.7824, 3.1767, 2.395, 4.6326, 2.8578, 3.6366, 4.317, 5.3784, 5.3761, 5.3753, 5.3707, 5.3701, 5.3634, 5.3629, 5.356, 5.3542, 5.3517, 5.349, 5.3487, 5.3467, 5.3421, 5.3418, 5.3415, 5.339, 5.3386, 5.3384, 5.3369, 5.3348, 5.3337, 5.3325, 5.3325, 5.3325, 5.3298, 5.3284, 5.3226, 5.3223, 5.3223, 5.3007, 5.2187, 4.9763, 5.085, 4.7949, 5.2037, 4.5097, 4.4873, 3.9697, 2.5663, 4.6557, 3.9179, 3.312, 5.4296, 5.4171, 5.4126, 5.4065, 5.3993, 5.3937, 5.3915, 5.3893, 5.3888, 5.3886, 5.3876, 5.3868, 5.3853, 5.383, 5.3814, 5.3804, 5.3742, 5.372, 5.3691, 5.3691, 5.3678, 5.3656, 5.359, 5.3579, 5.3574, 5.357, 5.3529, 5.3521, 5.3504, 5.3491, 4.8536, 5.1941, 4.8009, 4.8984, 4.9576, 4.4279, 4.6711, 4.0934, 4.7603, 3.4602, 3.2245, 5.4197, 5.4161, 5.4134, 5.4105, 5.4038, 5.4005, 5.4004, 5.3998, 5.3985, 5.3929, 5.391, 5.3905, 5.3901, 5.3886, 5.38, 5.3771, 5.3729, 5.3728, 5.3725, 5.371, 5.3703, 5.3668, 5.3654, 5.3634, 5.3612, 5.356, 5.3547, 5.354, 5.3526, 5.3521, 5.3056, 5.2023, 5.1104, 5.1982, 4.1311, 4.95, 4.2885, 5.1732, 4.9422, 3.3298, 3.2461, 4.2805, 2.535, 3.7865, 5.5113, 5.4983, 5.4961, 5.4914, 5.4886, 5.4882, 5.485, 5.4843, 5.4812, 5.4789, 5.4767, 5.472, 5.4684, 5.4672, 5.4671, 5.4663, 5.4663, 5.4611, 5.4583, 5.458, 5.4575, 5.4551, 5.4523, 5.4487, 5.4428, 5.4423, 5.442, 5.4408, 5.4398, 5.4353, 5.4005, 5.3294, 5.3443, 5.2551, 5.0961, 5.0571, 5.0366, 4.7375, 5.1575, 4.7769, 4.591, 5.223, 3.3519, 5.8423, 5.8381, 5.8365, 5.8314, 5.8303, 5.8302, 5.818, 5.8155, 5.815, 5.8141, 5.8095, 5.8075, 5.8057, 5.8048, 5.8046, 5.8038, 5.7984, 5.7981, 5.7963, 5.7961, 5.7956, 5.7948, 5.7921, 5.7886, 5.7877, 5.7776, 5.7759, 5.7744, 5.7742, 5.7727, 5.6095, 5.213, 5.1146, 5.0017, 5.0681, 3.3103, 4.4936, 1.9902, 1.3527, 2.5927, 5.9812, 5.9698, 5.9692, 5.9632, 5.9603, 5.9572, 5.9512, 5.9452, 5.9406, 5.9223, 5.9207, 5.9196, 5.914, 5.9106, 5.9077, 5.907, 5.9063, 5.9033, 5.9023, 5.8998, 5.889, 5.8853, 5.8823, 5.8802, 5.8775, 5.8756, 5.8707, 5.8706, 5.8675, 5.8663, 5.8497, 4.946, 5.1117, 4.8916, 5.5477, 4.877, 4.1445, 5.9801, 5.9548, 5.954, 5.9536, 5.9532, 5.9526, 5.9473, 5.9432, 5.9398, 5.9281, 5.9281, 5.927, 5.9247, 5.9205, 5.9165, 5.9165, 5.9164, 5.9146, 5.913, 5.9087, 5.9059, 5.9043, 5.9003, 5.8914, 5.8907, 5.8848, 5.8828, 5.8747, 5.8725, 5.8675, 5.5808, 5.7297, 5.6229, 5.241, 3.9321, 4.6691, 5.2865, 4.7147, 4.8027, 3.9201, 6.0873, 6.0666, 6.0617, 6.0584, 6.0524, 6.0482, 6.0455, 6.0382, 6.0357, 6.0353, 6.0305, 6.0268, 6.0252, 6.0247, 6.0211, 6.0203, 6.0191, 6.0064, 6.0063, 6.0062, 6.005, 6.0049, 6.0047, 6.0042, 6.0029, 6.002, 5.9992, 5.9988, 5.9976, 5.9849, 5.9774, 5.6408, 5.5432, 6.1328, 6.1316, 6.1299, 6.1257, 6.1133, 6.0824, 6.0814, 6.0679, 6.0577, 6.0463, 6.0381, 6.0363, 6.0356, 6.0169, 6.0105, 6.0096, 5.9844, 5.9732, 5.9602, 5.9539, 5.9493, 5.9469, 5.9286, 5.922, 5.909, 5.9055, 5.9054, 5.9011, 5.9011, 5.8825, 5.7281, 5.5646, 5.4217, 4.7396, 5.0544, 4.6966, 5.0224, 2.1586, 3.3316, 2.4703, 4.532, 6.1733, 6.1643, 6.1633, 6.1517, 6.1498, 6.1495, 6.1482, 6.1371, 6.1288, 6.1226, 6.1226, 6.1218, 6.1127, 6.1112, 6.1087, 6.0988, 6.0978, 6.0943, 6.0935, 6.09, 6.0696, 6.0685, 6.0676, 6.065, 6.0579, 6.057, 6.0521, 6.051, 6.0497, 6.0343, 5.8481, 5.1958, 5.2238, 6.1649, 6.1551, 6.154, 6.1515, 6.1367, 6.1181, 6.1164, 6.1148, 6.1094, 6.1023, 6.0945, 6.0941, 6.0904, 6.0884, 6.0831, 6.0791, 6.076, 6.0676, 6.0676, 6.0651, 6.0641, 6.0633, 6.0614, 6.0537, 6.0498, 6.038, 6.0376, 6.0371, 6.0335, 6.0334, 6.0318, 5.8625, 5.805, 5.5121, 5.3255, 5.0283, 5.0229, 4.9718, 5.3058, 6.3449, 6.3305, 6.3104, 6.3051, 6.298, 6.2967, 6.2817, 6.2814, 6.2791, 6.2713, 6.2626, 6.2551, 6.2516, 6.2486, 6.248, 6.2399, 6.2346, 6.2337, 6.2315, 6.2299, 6.2123, 6.2106, 6.2069, 6.1953, 6.1926, 6.1737, 6.1716, 6.1599, 6.1552, 6.152, 5.6352, 5.7113, 5.0022, 2.8153, 3.6236, 6.3898, 6.3838, 6.3659, 6.3596, 6.3568, 6.346, 6.3406, 6.3269, 6.3264, 6.3227, 6.3214, 6.3197, 6.3176, 6.3175, 6.2925, 6.2913, 6.2881, 6.2853, 6.2841, 6.2775, 6.2656, 6.2652, 6.2594, 6.2582, 6.246, 6.2453, 6.2407, 6.2367, 6.2338, 6.2239, 6.1887, 5.975, 6.0802, 4.6951, 5.1401, 2.2208, 6.5011, 6.4652, 6.454, 6.435, 6.4291, 6.4287, 6.4262, 6.4223, 6.4114, 6.404, 6.4024, 6.4, 6.3998, 6.3889, 6.3863, 6.3852, 6.375, 6.373, 6.3678, 6.3288, 6.3182, 6.3137, 6.312, 6.2991, 6.2959, 6.2825, 6.2705, 6.2539, 6.2357, 6.2311, 6.2077, 5.7697, 4.7461, 4.3638, 6.7116, 6.6417, 6.6395, 6.6269, 6.6242, 6.6214, 6.6003, 6.5979, 6.5951, 6.5905, 6.59, 6.5733, 6.5698, 6.5672, 6.5632, 6.5364, 6.5271, 6.5211, 6.5196, 6.5193, 6.5193, 6.5162, 6.5126, 6.4818, 6.4785, 6.476, 6.4414, 6.441, 6.4338, 6.4283, 5.4975, 5.5153, 6.8546, 6.8243, 6.8112, 6.8072, 6.7971, 6.768, 6.718, 6.7059, 6.6966, 6.6744, 6.6606, 6.6517, 6.6447, 6.6107, 6.5317, 6.5009, 6.4828, 6.4797, 6.4695, 6.4571, 6.4568, 6.3872, 6.3861, 6.3635, 6.2896, 6.2841, 6.1667, 6.1494, 5.8882, 5.3724, 2.2877, 6.9327, 6.9081, 6.8621, 6.8415, 6.7735, 6.7403, 6.7349, 6.7342, 6.734, 6.707, 6.6997, 6.6986, 6.6846, 6.6543, 6.6414, 6.6401, 6.6352, 6.617, 6.6029, 6.5788, 6.5776, 6.5323, 6.5166, 6.5133, 6.5092, 6.487, 6.424, 6.4046, 6.3601, 6.3477, 5.7566, 5.3353, 4.7408, 7.14, 7.0633, 6.9538, 6.9318, 6.9308, 6.8558, 6.8409, 6.8288, 6.7788, 6.6185, 6.5379, 6.5038, 6.4484, 6.4233, 6.4042, 6.3391, 6.2548, 6.2548, 6.2507, 6.1373, 5.5339, 5.4837, 5.4804, 5.4804, 4.4742, 4.3812, 3.9299, 2.8695, 2.8695, 2.8695, 1.3696, -1.742, -0.9614]}, \"token.table\": {\"Topic\": [1, 2, 3, 4, 5, 6, 4, 6, 30, 26, 27, 4, 29, 4, 20, 8, 1, 3, 3, 4, 6, 14, 1, 2, 3, 4, 5, 6, 1, 2, 5, 6, 1, 2, 3, 4, 5, 7, 11, 12, 1, 2, 5, 6, 7, 12, 1, 2, 3, 6, 7, 2, 3, 4, 30, 4, 30, 2, 11, 25, 2, 3, 2, 20, 23, 14, 24, 22, 7, 12, 15, 5, 23, 28, 10, 10, 10, 2, 7, 1, 2, 3, 5, 6, 7, 6, 5, 1, 2, 3, 4, 5, 6, 7, 1, 5, 9, 8, 28, 18, 5, 21, 13, 14, 15, 3, 4, 4, 13, 4, 16, 2, 3, 9, 6, 10, 21, 29, 21, 12, 3, 4, 1, 4, 7, 14, 7, 1, 2, 3, 5, 24, 1, 5, 6, 4, 7, 15, 29, 1, 2, 3, 4, 5, 6, 7, 16, 23, 20, 14, 15, 4, 8, 2, 5, 6, 2, 1, 10, 18, 5, 10, 1, 2, 7, 6, 12, 5, 17, 16, 23, 20, 29, 22, 21, 24, 16, 23, 21, 13, 2, 3, 4, 5, 6, 2, 5, 6, 3, 4, 5, 6, 3, 3, 7, 1, 2, 3, 4, 5, 3, 1, 19, 3, 12, 27, 9, 22, 29, 28, 24, 2, 22, 27, 23, 17, 26, 4, 1, 2, 3, 4, 5, 6, 9, 15, 11, 18, 1, 29, 25, 1, 3, 4, 6, 14, 26, 29, 2, 11, 15, 4, 29, 27, 28, 1, 2, 9, 22, 14, 1, 2, 14, 7, 1, 2, 3, 4, 7, 3, 6, 29, 16, 23, 9, 20, 14, 24, 30, 4, 11, 15, 16, 2, 13, 10, 10, 9, 20, 18, 6, 4, 13, 5, 3, 9, 17, 2, 3, 4, 5, 6, 1, 4, 11, 26, 1, 2, 3, 5, 7, 12, 12, 20, 6, 7, 15, 12, 1, 3, 4, 5, 6, 7, 1, 2, 5, 6, 7, 30, 18, 3, 13, 2, 1, 2, 3, 4, 5, 7, 7, 1, 2, 3, 4, 5, 6, 7, 8, 1, 4, 7, 11, 15, 21, 11, 22, 25, 1, 2, 3, 4, 8, 1, 2, 5, 6, 13, 3, 10, 3, 5, 24, 24, 3, 5, 6, 7, 12, 24, 4, 20, 1, 3, 4, 5, 2, 4, 28, 4, 17, 1, 23, 20, 16, 1, 4, 5, 27, 3, 15, 25, 5, 19, 24, 3, 19, 1, 2, 3, 4, 5, 6, 7, 2, 5, 6, 1, 2, 4, 6, 7, 1, 6, 19, 24, 1, 3, 5, 17, 3, 2, 3, 5, 3, 5, 25, 1, 4, 6, 9, 22, 17, 4, 22, 11, 3, 5, 23, 1, 14, 1, 3, 4, 4, 9, 7, 21, 1, 2, 4, 6, 7, 8, 19, 25, 6, 19, 1, 6, 1, 4, 22, 1, 3, 4, 10, 1, 4, 6, 24, 17, 17, 21, 17, 2, 3, 4, 17, 1, 16, 17, 1, 3, 4, 6, 7, 9, 2, 3, 4, 19, 18, 16, 29, 6, 13, 23, 18, 5, 10, 25, 14, 12, 1, 2, 3, 4, 5, 6, 7, 8, 21, 1, 2, 3, 4, 5, 3, 4, 5, 7, 9, 10, 5, 7, 22, 1, 2, 3, 4, 5, 6, 4, 1, 3, 4, 5, 7, 8, 10, 11, 5, 6, 19, 11, 24, 23, 6, 2, 3, 5, 6, 13, 17, 5, 12, 6, 3, 5, 18, 5, 12, 23, 21, 18, 23, 26, 1, 3, 4, 8, 9, 26, 17, 28, 26, 22, 1, 2, 3, 4, 10, 11, 11, 19, 1, 2, 3, 4, 5, 6, 7, 21, 1, 2, 15, 11, 2, 8, 30, 9, 15, 16, 25, 3, 4, 3, 14, 3, 12, 1, 2, 3, 4, 6, 10, 7, 11, 27, 11, 23, 14, 18, 26, 1, 3, 4, 29, 21, 29, 4, 1, 26, 11, 27, 1, 3, 4, 6, 3, 4, 8, 1, 4, 6, 16, 1, 4, 1, 25, 21, 11, 5, 7, 10, 16, 5, 3, 5, 5, 13, 1, 9, 1, 2, 4, 5, 6, 7, 8, 4, 10, 1, 16, 12, 13, 22, 21, 1, 2, 3, 5, 6, 7, 28, 5, 23, 6, 1, 2, 3, 4, 5, 11, 8, 16, 5, 8, 10, 5, 10, 20, 1, 2, 3, 4, 5, 6, 18, 1, 2, 3, 6, 7, 2, 6, 7, 13, 30, 13, 6, 16, 30, 1, 3, 8, 1, 3, 4, 8, 3, 4, 19, 15, 13, 20, 5, 25, 28, 1, 2, 3, 5, 6, 7, 1, 2, 6, 7, 23, 3, 1, 2, 3, 5, 28, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 5, 21, 4, 5, 7, 2, 3, 5, 10, 24, 1, 3, 6, 8, 2, 3, 4, 5, 6, 1, 2, 6, 7, 8, 28, 16, 6, 24, 3, 21, 6, 3, 30, 1, 2, 3, 4, 7, 11, 2, 3, 4, 5, 8, 21, 20, 1, 2, 3, 5, 6, 7, 8, 3, 23, 15, 20, 20, 7, 4, 9, 14, 2, 4, 5, 1, 3, 4, 9, 1, 3, 4, 7, 8, 11, 1, 2, 3, 4, 5, 6, 7, 20, 3, 5, 1, 3, 5, 6, 16, 13, 10, 1, 2, 3, 4, 5, 6, 7, 8, 3, 5, 13, 8, 2, 4, 14, 6, 1, 3, 4, 5, 12, 5, 15, 1, 3, 4, 5, 8, 5, 8, 19, 26, 1, 6, 7, 8, 11, 19, 5, 25, 14, 18, 28, 1, 2, 3, 4, 5, 28, 16, 23, 1, 2, 6, 7, 9, 16, 1, 2, 11, 26, 8, 1, 2, 4, 6, 15, 1, 3, 27, 5, 13, 28, 3, 17, 1, 2, 5, 6, 7, 3, 4, 5, 8, 4, 15, 14, 14, 1, 2, 3, 4, 6, 7, 1, 3, 4, 6, 8, 7, 12, 2, 1, 2, 3, 4, 1, 2, 3, 4, 5, 6, 7, 1, 2, 17, 4, 21, 25, 1, 2, 3, 4, 5, 6, 7, 17, 1, 2, 3, 5, 24, 16, 27, 1, 2, 3, 4, 5, 6, 7, 8, 6, 1, 6, 3, 6, 27, 6, 21, 5, 23, 24, 16, 2, 3, 4, 5, 12, 3, 12, 6, 1, 2, 3, 4, 5, 6, 7, 3, 14, 22, 5, 9, 26, 14, 20, 1, 4, 17, 1, 2, 4, 7, 20, 13, 26, 1, 2, 4, 5, 6, 6, 17, 8, 22, 23, 18, 18, 24, 1, 2, 3, 4, 5, 6, 17, 7, 4, 15, 2, 3, 13, 3, 18, 18, 1, 2, 3, 4, 5, 7, 18, 1, 4, 13, 1, 19, 2, 6, 2, 12, 17, 4, 17, 4, 1, 2, 3, 4, 5, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 5, 27, 2, 30, 25, 21, 9, 2, 2, 5, 14, 4, 6, 11, 3, 4, 12, 7, 11, 1, 7, 29, 25, 3, 3, 10, 2, 2, 1, 1, 19, 1, 2, 3, 6, 1, 4, 5, 15, 4, 5, 1, 2, 4, 5, 6, 5, 13, 1, 3, 4, 5, 15, 16, 20, 25, 1, 2, 5, 7, 2, 3, 6, 11, 26, 4, 23, 29, 1, 4, 17, 5, 16, 15, 15, 20, 19, 1, 2, 3, 4, 5, 9, 27, 4, 4, 5, 24, 12, 5, 26, 24, 8, 12, 1, 3, 4, 12, 12, 10, 29, 3, 4, 3, 1, 3, 4, 14, 6, 29, 5, 6, 1, 3, 10, 3, 12, 5, 4, 9, 10, 20, 24, 25, 16, 4, 8, 19, 29, 24, 12, 19, 2, 4, 6, 7, 9, 4, 5, 13, 25, 7, 5, 6, 7, 25, 3, 19, 17, 7, 11, 12, 9, 5, 8, 22, 4, 16, 27, 10, 6, 5, 10, 10, 14, 5, 12, 18, 16, 5, 1, 2, 4, 5, 6, 7, 18, 2, 8, 11, 12, 21, 14, 25, 19, 24, 6, 23, 4, 23, 22, 8, 23, 4, 10, 16, 20, 3, 15, 28, 25, 1, 3, 4, 7, 8, 17, 1, 3, 4, 18, 16, 25, 4, 12, 1, 2, 3, 4, 5, 6, 7, 4, 7, 21, 3, 5, 6, 21, 16, 23, 2, 16, 19, 8, 4, 25, 12, 1, 27, 14, 11, 13, 5, 15, 1, 2, 3, 4, 5, 6, 7, 1, 2, 7, 18, 7, 3, 1, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 7, 12, 23, 7, 8, 1, 12, 1, 2, 3, 4, 5, 6, 7, 10, 6, 3, 16, 12, 13, 9, 16, 25, 1, 2, 3, 4, 5, 3, 4, 19, 8, 1, 2, 3, 4, 5, 6, 7, 15, 24, 7, 9, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 14, 25, 3, 4, 20, 21, 15, 1, 2, 3, 4, 5, 6, 7, 1, 1, 2, 3, 4, 6, 7, 1, 2, 3, 4, 5, 6, 7, 6, 8, 8, 26, 17, 29, 3, 18, 13, 14, 7, 2, 3, 17, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 7, 10, 6, 2, 26, 23, 4, 1, 10, 14, 17, 19, 7, 19, 22, 7, 19, 19, 15, 24, 1, 2, 1, 2, 3, 4, 5, 2, 22, 15, 9, 6, 1, 7, 1, 2, 3, 1, 3, 4, 5, 6, 7, 17, 25, 27, 4, 7, 17, 4, 5, 14, 4, 7, 4, 13, 26, 21, 1, 2, 4, 5, 6, 7, 19, 25, 28, 3, 18, 16, 22, 28, 3, 4, 5, 9, 24, 25, 6, 1, 3, 4, 5, 6, 12, 1, 2, 3, 4, 5, 6, 28, 1, 3, 4, 6, 6, 5, 13, 15, 2, 4, 1, 30, 26, 20, 1, 3, 4, 1, 4, 13, 1, 2, 3, 4, 6, 7, 9, 4, 26, 22, 13, 25, 1, 3, 4, 5, 19, 24, 7, 17, 1, 14, 13, 3, 25, 19, 10, 27, 16, 4, 13, 2, 3, 5, 27, 15, 3, 14, 17, 13, 22, 6, 23, 11, 17, 1, 2, 3, 4, 5, 6, 7, 8, 6, 10, 3, 5, 13, 13, 16, 11, 13, 15, 17, 5, 12, 29, 9, 9, 1, 3, 5, 6, 9, 29, 11, 23, 5, 24, 19, 4, 9, 28, 1, 2, 3, 4, 5, 7, 8, 7, 13, 5, 25, 4, 26, 1, 3, 4, 8, 1, 2, 4, 5, 6, 7, 8, 3, 6, 7, 19, 9, 24, 26, 1, 2, 5, 6, 7, 9, 25, 28, 7, 28, 11, 27, 12, 3, 25, 27, 1, 16, 16, 8, 7, 1, 2, 4, 5, 19, 21, 12, 22, 6, 2, 3, 4, 1, 4, 5, 10, 4, 6, 7, 1, 2, 5, 9, 1, 9, 4, 5, 6, 11, 16, 16, 13, 13, 17, 10, 23, 1, 2, 3, 4, 17, 9, 2, 5, 6, 29, 15, 1, 2, 3, 4, 13, 2, 3, 4, 7, 19, 2, 10, 14, 11, 2, 14, 6, 21, 1, 4, 8, 23, 7, 29, 1, 2, 3, 4, 5, 6, 7, 2, 6, 1, 2, 3, 4, 5, 6, 7, 8, 4, 1, 2, 3, 4, 5, 6, 8, 1, 4, 3, 4, 9, 9, 11, 22, 1, 2, 3, 15, 1, 7, 25, 22, 4, 4, 6, 18, 1, 2, 3, 5, 1, 3, 4, 5, 8, 2, 3, 4, 5, 8, 7, 10, 27, 8, 29, 1, 2, 3, 4, 5, 6, 3, 15, 1, 3, 4, 5, 10, 6, 1, 3, 6, 8, 21, 20, 29, 1, 3, 5, 6, 9, 19, 6, 15, 15, 10, 5, 14, 16, 1, 2, 3, 4, 5, 1, 6, 9, 1, 3, 4, 5, 6, 13, 9, 19, 7, 3, 4, 29, 1, 2, 3, 9, 1, 2, 3, 4, 5, 6, 27, 2, 3, 4, 7, 8, 28, 1, 2, 5, 6, 1, 1, 5, 13, 18, 5, 15, 1, 6, 7, 17, 8, 16, 1, 5, 18, 1, 2, 3, 5, 1, 2, 3, 4, 5, 6, 7, 8, 14, 3, 5, 3, 9, 21, 1, 2, 6, 7, 8, 8, 3, 4, 24, 20, 4, 8, 8, 10, 3, 4, 5, 6, 7, 14, 1, 2, 3, 21, 10, 8, 2, 1, 2, 15, 1, 3, 4, 6, 7, 10, 1, 3, 4, 5, 8, 12, 1, 2, 3, 6, 19, 5, 9, 5, 10, 26, 1, 2, 3, 4, 6, 11, 4, 20, 1, 2, 3, 4, 5, 6, 7, 10, 19, 10, 9, 12, 3, 15, 5, 1, 3, 4, 6, 7, 11, 1, 2, 1, 3, 4, 5, 10, 1, 16, 5, 11, 16, 1, 5, 9, 9, 23, 1, 2, 14, 8, 27, 4, 4, 12, 28, 5, 16, 1, 2, 17, 13, 22, 2, 29, 23, 1, 6, 24, 1, 2, 6, 1, 2, 3, 4, 5, 6, 7, 17, 7, 2, 3, 4, 10, 15, 1, 2, 6, 7, 10, 26, 2, 4, 5, 6, 7, 8, 16, 4, 5, 6, 10, 16, 1, 2, 3, 4, 5, 6, 7, 14, 1, 4, 12, 4, 1, 2, 3, 4, 5, 6, 7, 1, 22, 11, 1, 3, 4, 5, 6, 7, 11, 18, 17, 17, 3, 1, 2, 3, 4, 5, 6, 22, 3, 29, 7, 11, 5, 27, 23, 1, 2, 4, 5, 6, 7, 1, 2, 3, 4, 5, 8, 28, 3, 23, 29, 13, 4, 20, 10, 19, 26, 8, 15, 8, 22, 12, 27, 15, 1, 2, 3, 4, 7, 7, 1, 4, 9, 10, 4, 11, 3, 26, 27, 4, 14, 8, 1, 3, 4, 2, 4, 5, 6, 5, 29, 30, 21, 14, 1, 2, 5, 6, 7, 8, 7, 3, 18, 2, 7, 21, 1, 4, 15, 24, 22, 13, 12, 21, 3, 7, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 29, 11, 2, 12, 16, 18, 26, 19, 13, 27, 26, 20, 24, 1, 2, 3, 4, 5, 6, 7, 17, 12, 9, 28, 26, 7, 11, 1, 2, 4, 1, 2, 3, 4, 5, 6, 7, 4, 10, 16, 2, 4, 5, 18, 7, 21, 3, 6, 1, 2, 3, 4, 7, 1, 2, 3, 4, 6, 24, 5, 1, 2, 6, 7, 3, 6, 7, 4, 18, 1, 4, 6, 7, 9, 22, 21, 5, 15, 1, 2, 3, 4, 5, 6, 7, 8, 4, 27, 7, 4, 4, 5, 7, 15, 30, 22, 18, 1, 2, 3, 4, 5, 6, 1, 4, 5, 6, 21, 1, 3, 7, 10, 17, 23, 25, 19, 25, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 6, 1, 2, 3, 4, 6, 7, 6, 3, 4, 2, 5, 6, 7, 11, 2, 3, 5, 6, 7, 28, 1, 6, 21, 5, 18, 1, 2, 3, 4, 5, 6, 7, 20, 22, 6, 2, 1, 2, 12, 15, 16, 20, 30, 3, 7, 1, 2, 5, 18, 19, 1, 2, 3, 12, 13, 5, 26, 15, 3, 8, 21, 29, 23, 1, 13, 24, 3, 7, 8, 8, 23, 1, 2, 3, 5, 17, 1, 2, 3, 4, 5, 6, 3, 5, 12, 17, 1, 2, 3, 4, 7, 28, 17, 4, 29, 6, 1, 20, 4, 6, 7, 11, 10, 3, 15, 5, 29, 11, 7, 9, 27, 15, 16, 17, 9, 22, 5, 7, 3, 17, 18, 3, 9, 27, 22, 20, 4, 1, 3, 4, 5, 6, 8, 1, 4, 7, 18, 30, 16, 26, 1, 1, 2, 4, 11, 15, 17, 28, 27, 11, 28, 4, 7, 20, 1, 2, 3, 5, 6, 7, 8, 24, 3, 4, 17, 1, 2, 4, 6, 12, 15, 5, 16, 1, 2, 3, 5, 6, 20, 24, 4, 27, 1, 3, 4, 7, 8, 21, 11, 18, 5, 1, 2, 3, 5, 6, 8, 23, 26, 6, 12, 27, 15, 12, 11, 26, 28, 14, 4, 11, 1, 2, 3, 4, 6, 7, 3, 4, 5, 7, 8, 14, 21, 5, 7, 16, 21, 1, 25, 1, 2, 3, 4, 5, 6, 7, 19, 13, 1, 2, 3, 4, 5, 6, 1, 3, 4, 10, 1, 2, 3, 4, 5, 6, 7, 2, 26, 16, 5, 3, 5, 14, 16, 12, 25, 20, 22, 15, 19, 1, 4, 11, 5, 6, 1, 10, 1, 2, 3, 4, 5, 6, 2, 3, 5, 6, 1, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 11, 17, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 8, 12, 17, 14, 1, 2, 3, 4, 5, 6, 8, 2, 8, 1, 2, 3, 4, 5, 6, 10, 23, 30, 30, 18, 1, 4, 30], \"Freq\": [0.12944567580432542, 0.2571577041648429, 0.030435144013516987, 0.5277993328926364, 0.04969789338916065, 0.005393569825180225, 0.9539747138754312, 0.04769873569377156, 0.8216874273607598, 0.8459132437923325, 0.8503140196405663, 0.9858120621731645, 0.49655428609744495, 0.3938088263983058, 0.5728128383975357, 0.9858876582714031, 0.030930616286282794, 0.9665817589463372, 0.07030721537492393, 0.9227822017958766, 0.008788401921865491, 0.9410992402322721, 0.41216001885031345, 0.27861474958466914, 0.02576000117814459, 0.002711579071383641, 0.26709053853128867, 0.012880000589072295, 0.35486699917918396, 0.5765346205152008, 0.056659436843735256, 0.011928302493417949, 0.16535787904937677, 0.24990738281670516, 0.02992902788224014, 0.014216288244064066, 0.008978708364672043, 0.4900878315716823, 0.041900639035136196, 0.9802943816695252, 0.22461716247357225, 0.40393338461634, 0.36807014018778644, 0.002516718907266916, 0.000629179726816729, 0.9325410616459978, 0.48735990977578697, 0.2787586432114866, 0.18521547434857163, 0.032740109102020236, 0.014031475329437245, 0.17864853176787998, 0.02305142345392, 0.79527410916024, 0.9665824285613095, 0.7786253604716834, 0.19465634011792085, 0.9673807929212989, 0.9714215082690394, 0.8320647203294476, 0.9298784432068681, 0.06340080294592283, 0.9630354907982891, 0.8746492012917858, 0.8374490857319764, 0.9756353892458743, 0.8424786759467311, 0.9300756974796273, 0.023332754220325085, 0.9566429230333284, 0.889809376068658, 0.46965617221958433, 0.5032030416638403, 0.9289986023018404, 0.9970467139659164, 0.961860258882159, 0.9114265755937638, 0.9396549551627089, 0.9613239381091192, 0.004597389760773807, 0.7125954129199401, 0.015324632535912692, 0.1838955904309523, 0.08122055244033727, 0.003064926507182538, 0.9825436089451618, 0.9703296619347006, 0.1586722234065171, 0.11188426009433898, 0.0447537040377356, 0.13222685283876426, 0.5370444484528272, 0.004068518548885054, 0.010171296372212636, 0.18316401151803627, 0.8111549081513034, 0.9665632929256986, 0.9736245834748323, 0.5853405573337772, 0.9601002668822324, 0.32712487161962595, 0.6542497432392519, 0.9432127111720409, 0.9497291630232015, 0.8927353442365126, 0.09059650409334494, 0.8977289951067816, 0.32413753443911086, 0.6637101895657985, 0.32798941998635706, 0.6559788399727141, 0.9572962512417549, 0.9891451056791409, 0.969802384235591, 0.9929924847584284, 0.4837002500016268, 0.4837002500016268, 0.650884901851167, 0.841922831087256, 0.9337714965611231, 0.14420877397418358, 0.8460248073152103, 0.23917661723852596, 0.46795425111885514, 0.020797966716393564, 0.2599745839549195, 0.9553412624851999, 0.11833155729867323, 0.1228827710409299, 0.10467791607190324, 0.6508235651427028, 0.9604203414397006, 0.15639778012442315, 0.2736961152177405, 0.5641491354488121, 0.04029679971740699, 0.3223743977392559, 0.6044519957611049, 0.803806332946157, 0.2649095624215159, 0.0683298474499942, 0.1461207507007568, 0.019973340023844457, 0.40156925732150434, 0.09355932958537666, 0.004204913689230412, 0.9578731031457107, 0.9383714705512818, 0.9733277412947822, 0.9528684041124441, 0.9504190804821236, 0.17210813955980456, 0.8113669436390786, 0.26829108406902363, 0.7253795976681009, 0.004968353408685623, 0.9990130758083569, 0.9914271517219945, 0.0072632025767179085, 0.8913700736644506, 0.9750717909900599, 0.9246178514961094, 0.8876218310630049, 0.09672069716430855, 0.015231605852647016, 0.6042231005682435, 0.3625338603409461, 0.5733156117974353, 0.42998670884807644, 0.8939290658013295, 0.9321746715448703, 0.9419487162273126, 0.6424948022350632, 0.9639771939975248, 0.9256504463536005, 0.9003923820010569, 0.940582559240301, 0.9440742336968992, 0.9771920095421812, 0.9397180561348899, 0.4898457290291628, 0.022690401617154677, 0.022690401617154677, 0.42444398319148163, 0.0387071556998521, 0.18293302482059018, 0.7114062076356286, 0.10162945823366122, 0.0678178408601705, 0.18892112811047496, 0.7266197235018267, 0.014532394470036535, 0.9956736246543658, 0.9298420684608157, 0.056697687101269253, 0.2444790837971623, 0.23268404028063255, 0.4203324598617878, 0.08042075124906654, 0.021445533666417747, 0.9860545379885522, 0.9938004463254486, 0.9644013383063881, 0.9951766555860364, 0.8921029657589322, 0.7001925749847169, 0.14104522206719833, 0.7052261103359916, 0.7270445655416593, 0.6906621568367018, 0.9408533635956029, 0.9671332899101367, 0.9580464575580341, 0.9635745060491686, 0.8993953395324117, 0.9158196101351554, 0.9029815912227913, 0.9787420206425544, 0.2622377459588023, 0.02289377147259385, 0.19563768349307473, 0.22061270691772256, 0.060356306609565605, 0.21228769910950662, 0.024975023424647837, 0.975073255110986, 0.9925094737908936, 0.8439333859343919, 0.9776970806566452, 0.5772273748795992, 0.9212842545751991, 0.2484688094076261, 0.17353377164977063, 0.19719746778383024, 0.35495544201089446, 0.019719746778383025, 0.9083749712179127, 0.6659673173628559, 0.751702751843758, 0.05011351678958387, 0.20045406715833547, 0.9848537707396813, 0.7291396805616585, 0.8982689751907201, 0.7412622066605771, 0.27800380033959576, 0.004595104137844558, 0.7145386934348288, 0.9353626442804511, 0.9545389263852505, 0.8488902149040982, 0.14874813433364864, 0.9299990667373648, 0.9750438517208381, 0.1536219770231262, 0.14499152887575953, 0.5713356673556715, 0.09148275036208638, 0.03797397184841322, 0.7053463118989068, 0.1880923498397085, 0.09404617491985424, 0.9667277047913595, 0.8876217700017415, 0.9271410522007644, 0.9014757498722136, 0.9754000328090648, 0.8667745991977907, 0.6660108073813624, 0.44360432313809256, 0.5218874389859912, 0.9074339680422976, 0.9626682523581428, 0.9471954321010255, 0.955908195950774, 0.9709751729458179, 0.9154138455975993, 0.9598834139309013, 0.9097676975537758, 0.942263051257755, 0.9504203594087506, 0.9932506907917215, 0.9871730731218497, 0.9838025025405526, 0.39300025939696936, 0.07860005187939388, 0.5187603424039996, 0.10490876397273229, 0.021855992494319226, 0.16173434445796228, 0.6993917598182152, 0.013113595496591536, 0.015257307525721702, 0.7196363382965403, 0.2644599971125095, 0.8595313959749222, 0.16750068528226794, 0.03448543520517281, 0.004926490743596116, 0.7931650097189746, 0.2933098932850698, 0.7123240265494553, 0.9759601371730517, 0.8942158048143205, 0.19932154413837663, 0.4733886673286445, 0.32389750922486205, 0.9930465148503606, 0.21143315139984384, 0.6148821239689336, 0.04962206614486131, 0.09924413228972262, 0.012944886820398602, 0.012944886820398602, 0.11467019882788607, 0.3584735044439321, 0.42045739570224894, 0.07644679921859071, 0.029958880774853116, 0.770307465470845, 0.872382948233795, 0.9882019103351615, 0.9704164698061453, 0.9601707376435589, 0.2029574138982514, 0.15652924732022003, 0.4788733752762664, 0.04244860944277153, 0.10744804265201545, 0.011938671405779495, 0.9765608816510989, 0.39954973566012675, 0.1962052614424652, 0.057359881952439636, 0.02043291932211369, 0.27990637673787067, 0.03151100811121147, 0.015016964802999218, 0.0002461797508688396, 0.056288080783450774, 0.4671910705026414, 0.3433572927790497, 0.12946258580193679, 0.9092700969106566, 0.9871319378710771, 0.9400883671166531, 0.8065186280897224, 0.8079048879890645, 0.0607666711956963, 0.11924026045947954, 0.7498377917355732, 0.06879245795739204, 0.9902977947722607, 0.12652183191411454, 0.17170820045486973, 0.5693482436135154, 0.00903727370815104, 0.12652183191411454, 0.6731828600266354, 0.32573364194837195, 0.4456752186817691, 0.059423362490902555, 0.4852907936757042, 0.9501152553815903, 0.09616805877664346, 0.425887117439421, 0.12364464699854158, 0.15112123522043971, 0.17859782344233785, 0.961552242032348, 0.9903422633534014, 0.8843686987960255, 0.07837979207543772, 0.003918989603771886, 0.6740662118487644, 0.24297735543385693, 0.9441328662086005, 0.9858496198343051, 0.8571625829158164, 0.5241684021421577, 0.45427928185653665, 0.9973745931935821, 0.9226749502058225, 0.9216558257964266, 0.9579301458341856, 0.08070515169262706, 0.8290620128424415, 0.09537881563674105, 0.6317162793373781, 0.38206554282950655, 0.6208565070979482, 0.7211963079176074, 0.9815463166527193, 0.8865283571647071, 0.9062525149655255, 0.21938095460573323, 0.7678333411200663, 0.22221747152513177, 0.23308916778369984, 0.23743784628712708, 0.23526350703541346, 0.050879538490098665, 0.011306564108910814, 0.009132224857197196, 0.8951091736558376, 0.0965569662544198, 0.00782894320981782, 0.04706541072188158, 0.41686506639380827, 0.12774897195939286, 0.02689452041250376, 0.3765232857750526, 0.31921933367916155, 0.18241104781666373, 0.4560276195416594, 0.8888083923711957, 0.042598501270225096, 0.9030882269287721, 0.05111820152427012, 0.00851970025404502, 0.9925610951243422, 0.1578715877938082, 0.09328775642361394, 0.7463020513889115, 0.9874363930973851, 0.994528876864111, 0.7675811876406454, 0.3535836249919662, 0.3898486121706294, 0.1087949615359896, 0.1450599487146528, 0.9195973384705463, 0.9545396549165158, 0.5953217777013279, 0.37207611106332994, 0.9698720316751391, 0.9617219066406598, 0.032879381423612304, 0.9166953168625164, 0.9903835757461457, 0.9211558262401692, 0.05121175028529026, 0.9422962052493407, 0.010242350057058052, 0.19107653051357898, 0.7881906883685134, 0.9688402056540291, 0.8761265439585842, 0.4544920120263963, 0.02253679398477998, 0.01126839699238999, 0.1314646315778832, 0.30049058646373306, 0.07887877894672993, 0.9654407662934562, 0.9277245069043412, 0.25957812241638656, 0.6922083264436975, 0.9562865862121308, 0.04553745648629194, 0.06012771741270171, 0.9319796198968765, 0.8767787065276613, 0.08524740357157967, 0.09471933730175519, 0.5399002226200046, 0.27468607817509005, 0.43196121438508733, 0.02928550606000592, 0.5344604855951081, 0.8151341077052676, 0.9042573963352944, 0.919245622673353, 0.983998680182885, 0.8987140994478805, 0.7882289081329837, 0.09290911084457223, 0.11688565557865539, 0.9795820147502374, 0.9774583764494587, 0.9391194230388589, 0.9567055318001978, 0.3466533067616733, 0.17461055451699098, 0.18231396133391706, 0.028245824995395602, 0.24650901814163434, 0.017974615906160837, 0.8642618703976414, 0.07479189263056513, 0.05817147204599509, 0.9591707717300515, 0.8988131808029153, 0.9729695289531051, 0.8936526253857443, 0.5932491448620395, 0.04943742873850329, 0.29662457243101975, 0.8107771029140464, 0.9769443217823093, 0.18936998461705745, 0.7574799384682298, 0.9605121747887678, 0.9712324739360052, 0.14695372057292788, 0.09246526238296586, 0.10567458558053242, 0.07925593918539932, 0.3087679297431182, 0.1750235323677568, 0.023116315595741465, 0.04788379659117875, 0.01816281939665401, 0.03793576595679459, 0.06471395369100254, 0.6002777083751615, 0.2923285494317701, 0.002231515644517329, 0.037570114541643294, 0.2028786185248738, 0.672505050295415, 0.026299080179150305, 0.015028045816657317, 0.04508413744997195, 0.3789742234122944, 0.2332149067152581, 0.3789742234122944, 0.14228447329480987, 0.24303248544144543, 0.18912346139807026, 0.23684620399384498, 0.15288952720498203, 0.03535017970057388, 0.971069987708773, 0.08821653457021983, 0.3264011779098134, 0.4587259797651431, 0.044108267285109914, 0.008821653457021983, 0.047048818437450575, 0.011762204609362644, 0.013232480185532974, 0.8958252423958831, 0.09359368204136093, 0.9305718313952549, 0.9307316074321307, 0.9547334540538307, 0.8996158038260146, 0.9865501475827313, 0.8059058987253492, 0.046816110299397876, 0.1003202363558526, 0.046816110299397876, 0.9835166783500303, 0.9376767072606844, 0.9734740999582274, 0.9338244463678664, 0.9837586265601633, 0.8506312612956507, 0.14376866388095505, 0.9640502380816605, 0.4235387982836251, 0.544549883507518, 0.8772795193253351, 0.8927688571417891, 0.8004048716958667, 0.7986812165077819, 0.8091519928356522, 0.27617259681930545, 0.14008754911124188, 0.40025014031783396, 0.1801125631430253, 0.9687883847121984, 0.9308593880827545, 0.9311891403029725, 0.898282004349779, 0.8949692940892856, 0.9763796652809275, 0.38135015004131684, 0.04468947070796682, 0.09533753751032921, 0.47370838950444827, 0.004468947070796682, 0.9545046530642708, 0.928306921893951, 0.9307674926266868, 0.24051640402689115, 0.07752909726874146, 0.05109872319985233, 0.030835436413703992, 0.42993408485393, 0.11893668331000111, 0.04933669826192639, 0.8575703049872697, 0.9494125105236888, 0.01020873667229773, 0.04083494668919092, 0.9289801614965874, 0.25394732551784455, 0.7459702687086684, 0.6470390407150093, 0.9683068339519005, 0.9415872457679747, 0.3417817741543411, 0.6408408265393895, 0.1744355093997612, 0.824057406474734, 0.9795868188128444, 0.9732935824361955, 0.09392613297261879, 0.9044738730696623, 0.12937691187931735, 0.028125415625938552, 0.12937691187931735, 0.253128740633447, 0.01125016625037542, 0.45000665001501683, 0.9645498774112249, 0.9473115716583921, 0.7128805837665088, 0.9607953645424484, 0.961078811292163, 0.9746764371632622, 0.8938681213415243, 0.9185619350338688, 0.1554860321611551, 0.054082098143010474, 0.7841904230736518, 0.9614727545722578, 0.8807766166407209, 0.8985874056325533, 0.9795435195616552, 0.9961276342979201, 0.869257700251273, 0.9429622395708087, 0.8139866873277218, 0.17772465030851706, 0.13493908634535556, 0.6450746566753582, 0.042785563963161515, 0.1777074093266152, 0.8189993647226613, 0.9500093984267663, 0.036712911225765615, 0.9545356918699061, 0.4889058311693455, 0.4889058311693455, 0.9718873304803367, 0.018337496801515786, 0.9941791764471493, 0.8085142787154818, 0.9256963119130185, 0.960540454590659, 0.18383766017731734, 0.7090881178267955, 0.013131261441236953, 0.07878756864742173, 0.9805759274158878, 0.07061426139964373, 0.9179853981953684, 0.2556759758099384, 0.7244152647948255, 0.7004811509321132, 0.2937501600683055, 0.2447538821364319, 0.23230876948542686, 0.0013827902945561124, 0.1009436915025962, 0.33601804157713533, 0.07467067590603006, 0.009679532061892787, 0.2983501892744309, 0.6712879258674695, 0.5345550502028141, 0.454371792672392, 0.8962683602454468, 0.9718308368231041, 0.9778592022473122, 0.9379211039084059, 0.03708967526918599, 0.08963338190053281, 0.19162998889079427, 0.21635643907025162, 0.46671174713725705, 0.9637692659917961, 0.9665261743933213, 0.9782950969972644, 0.8789824280117922, 0.9638959442133693, 0.062343933523375174, 0.25685700611630574, 0.14214416843329541, 0.01745630138654505, 0.5062327402098065, 0.012468786704675036, 0.7931731683970301, 0.18303996193777616, 0.09177037544502235, 0.8947611605889679, 0.963015159872379, 0.3187355590123317, 0.03984194487654146, 0.6175501455863927, 0.3188418814279331, 0.3264737349727507, 0.19630823284725138, 0.12804554280749442, 0.028407454861265318, 0.0021199593180048744, 0.8699205520887066, 0.1049518515003774, 0.8143632387096954, 0.011836674981245573, 0.04813581159039866, 0.02051690330082566, 0.8577067977448397, 0.06395182263886963, 0.07899931031860366, 0.9615837461201552, 0.8718224903866333, 0.9573065377757711, 0.7675371586092288, 0.07675371586092287, 0.07675371586092287, 0.4365970355435335, 0.05766375941141008, 0.49426079495494357, 0.07712880787490214, 0.1758536819547769, 0.7373514032840646, 0.009255456944988258, 0.3099302481278487, 0.378803636600704, 0.3099302481278487, 0.9495858323045228, 0.9880041835762713, 0.8553279440660077, 0.9865797213497852, 0.8185692804997368, 0.9433577471777823, 0.24414861546786623, 0.6716752303264443, 0.026653778981208104, 0.008529209273986593, 0.04157989521068464, 0.006396906955489945, 0.11763180227956259, 0.7351987642472662, 0.04411192585483597, 0.10292782699461726, 0.9303477344914743, 0.9690580242782084, 0.9836344505309571, 0.1021070658132582, 0.007854389677942938, 0.8796916439296091, 0.4798308971903662, 0.22380919781724168, 0.05558659815068748, 0.14481771623468578, 0.37155252448091103, 0.1930902883129144, 0.01023963650144243, 0.31183266115650377, 0.0715990400238498, 0.4927144464799138, 0.05087300212220907, 0.06500439160060048, 0.007536741055142085, 0.6515006633170778, 0.3257503316585389, 0.031115258944874113, 0.31115258944874113, 0.6534204378423564, 0.13061284327233696, 0.42197995518755016, 0.301414253705393, 0.07032999253125836, 0.0602828507410786, 0.8513311658468597, 0.10050437374580982, 0.0472961758803811, 0.9882321263347895, 0.3457544483707594, 0.551309684444839, 0.06746428260892866, 0.03373214130446433, 0.0010541294157645103, 0.2503663468144724, 0.46203971275761724, 0.1536338946361535, 0.09787048102747557, 0.035278894323857474, 0.6463090532138142, 0.952550783090643, 0.9535565196687116, 0.772969127335033, 0.9769854806402976, 0.9827815061872183, 0.9126093385190486, 0.28863298748251076, 0.6494242218356492, 0.3185659362118255, 0.09744369813538191, 0.00749566908733707, 0.07120885632970217, 0.14616554720307287, 0.3522964471048423, 0.013747452353265256, 0.7345067400173151, 0.20621178529897882, 0.019639217647521794, 0.02356706117702615, 0.7930461180707797, 0.9437861371759215, 0.31615080133283785, 0.41226454803433327, 0.12929006535987966, 0.12587485608622248, 0.0039030963127510846, 0.00439098335184497, 0.007806192625502169, 0.6519809142663093, 0.30425775999094434, 0.057558109869009756, 0.9209297579041561, 0.9402298491519502, 0.9496184044463009, 0.25189610667639656, 0.10795547428988424, 0.6117476876426773, 0.11000897739003213, 0.012223219710003569, 0.880071819120257, 0.010771240874732807, 0.20465357661992334, 0.49547708023770914, 0.2692810218683202, 0.21384964983483815, 0.0608877475224192, 0.3549310160453216, 0.3341400778669346, 0.03267147428032249, 0.0014850670127419315, 0.08881983004327663, 0.1970379720650505, 0.12341603211516342, 0.0022911392100587267, 0.4524999939865985, 0.11455696050293633, 0.021383965960548115, 0.8831752443475049, 0.08813796943523795, 0.9009659097824324, 0.04308792501827225, 0.14219015256029843, 0.7368035178124555, 0.07324947253106283, 0.95132383761276, 0.9788082400488073, 0.9835096558147725, 0.16957351696764264, 0.24912064430920533, 0.26674485815134796, 0.18052910935600158, 0.09764767128754703, 0.03239044706123511, 0.0038106408307335424, 0.9920076789885136, 0.48937238994737053, 0.03058577437171066, 0.48937238994737053, 0.988109057234662, 0.9717180621735252, 0.9929430119817753, 0.9746400027200687, 0.9696522216705274, 0.2882185976731371, 0.04117408538187673, 0.4117408538187673, 0.05764371953462742, 0.1976356098330083, 0.09361732238459715, 0.8737616755895734, 0.2579946250362083, 0.20028530101495118, 0.22235063078778478, 0.02715732895117982, 0.29194128622518306, 0.5451269300877144, 0.2826584081936297, 0.04037977259908995, 0.12113931779726987, 0.47468135521435345, 0.044902290358114515, 0.25016990342378087, 0.11546303234943733, 0.10904841944113526, 0.9737966430277943, 0.996623037712112, 0.8909720165919572, 0.04815164319664938, 0.9148812207363383, 0.489617141481984, 0.16993059142796896, 0.2199786423279872, 0.3317138257326791, 0.2746823258698676, 0.0034917244813966225, 0.9663210526713023, 0.8891433096754329, 0.8115950177246425, 0.1253989391989534, 0.6869681016986142, 0.024534575060664796, 0.16220080178995058, 0.7833181512603368, 0.17802685255916745, 0.12679983633898587, 0.8727484230899567, 0.9966469168119435, 0.9003926308177528, 0.9698432623996973, 0.19770967754823923, 0.07151201102808653, 0.3449402884884174, 0.11357789986813743, 0.2734282774603308, 0.942425128008147, 0.051538874187945546, 0.9354133818006731, 0.1270705315793779, 0.8471368771958527, 0.45926108605760546, 0.9889264360914929, 0.9512803846144275, 0.10213544072064387, 0.22469796958541652, 0.09192189664857948, 0.5745118540536218, 0.005106772036032194, 0.26217817967654505, 0.338913256655046, 0.13428638471237672, 0.2557835899283366, 0.9701938708055302, 0.9438084157977841, 0.993592636554228, 0.9755811080733304, 0.4242943760793216, 0.3263802892917858, 0.09791408678753574, 0.13799587670056207, 0.008016357982605266, 0.004008178991302633, 0.1128286672007459, 0.07180006094592921, 0.07180006094592921, 0.6872291547681796, 0.06154290938222504, 0.9765429356030819, 0.915648590944984, 0.9756671161409286, 0.3013841130611575, 0.11098589560347387, 0.3865370846879607, 0.19996596707867276, 0.2191318822385064, 0.28202471853052485, 0.1727897923391244, 0.08473982153029856, 0.2164837628156846, 0.02118495538257464, 0.00264811942282183, 0.09161934742762352, 0.8818362189908764, 0.02290483685690588, 0.7333220825739143, 0.24444069419130476, 0.8673861490646477, 0.4357597473115294, 0.3006212993598567, 0.10232415119056157, 0.1439594954681004, 0.002469893304599762, 0.0010585257019713266, 0.013055150324313028, 0.0007056838013142177, 0.49211207973789484, 0.05467911997087721, 0.416168857556121, 0.006075457774541912, 0.027339559985438605, 0.9834353986042739, 0.7074778789255328, 0.20470271082419464, 0.5595673589651982, 0.032165071555637754, 0.061882800710303075, 0.08338445180456093, 0.03880785807256295, 0.018355068007293286, 0.0010488610289881878, 0.9524675260381625, 0.05663276134476905, 0.9344405621886893, 0.1015607671295165, 0.8801933151224763, 0.7579582745175237, 0.966323432069693, 0.9376508819730929, 0.24334093980962934, 0.6489091728256782, 0.9203315306966612, 0.9529573057282691, 0.19414997712569737, 0.2314865111883315, 0.03733653406263411, 0.3584307270012875, 0.17921536350064374, 0.980002576848478, 0.943925581343434, 0.9903294079038638, 0.046492750354110254, 0.6633360560076877, 0.042939164339783356, 0.005330379021490347, 0.1708682608555517, 0.04412369301122566, 0.026948027275312314, 0.5268680193849127, 0.47140822787071135, 0.8973856911917432, 0.9832001083434708, 0.9703687746649794, 0.8489805559036784, 0.9447911106128846, 0.9513173812681768, 0.37671205540958885, 0.6220129286995537, 0.9298387035530091, 0.7390689032519377, 0.10992791124066005, 0.1325023930132956, 0.01766698573510608, 0.956865497356203, 0.9675079801093556, 0.8811481077743154, 0.7162016281441439, 0.16782926436075307, 0.04293306762716939, 0.03512705533132041, 0.0390300614792449, 0.5154132381496226, 0.45814510057744234, 0.955411945843033, 0.8832085965753631, 0.9390957884943959, 0.9635470934414481, 0.9046215855531148, 0.9506679802505451, 0.40456626187741096, 0.14058287092265245, 0.23977189651807948, 0.2085312585352678, 0.0007810159495702915, 0.001562031899140583, 0.003905079747851457, 0.9745075943504463, 0.9969893552633493, 0.9702441842342214, 0.9407250748724653, 0.06135163531776947, 0.9798946115763492, 0.9797961953369629, 0.9611554541524835, 0.8447256439721778, 0.3365979476842308, 0.07397757091961116, 0.13870794547427093, 0.3809844902359975, 0.03698878545980558, 0.031440467640834746, 0.9708872274874877, 0.12743890819463785, 0.45513895783799235, 0.40052228289743325, 0.47199857128508443, 0.47199857128508443, 0.9755287239371537, 0.9793559458764702, 0.9906102275218243, 0.9841841768470454, 0.923071422888124, 0.9753529021888802, 0.9090858861894678, 0.9857228370862613, 0.07195151868725283, 0.129246246530806, 0.37441438427996376, 0.3424359315300736, 0.06129203443728944, 0.01998653296868134, 0.071610842488577, 0.19478149156892943, 0.20623922636710176, 0.17759488937167095, 0.12603508277989553, 0.00286443369954308, 0.1861881904703002, 0.03437320439451696, 0.01171588744155131, 0.9060286288133013, 0.08201121209085917, 0.9317871625880634, 0.926521896373586, 0.054501288021975645, 0.9064052778999567, 0.7846862312417604, 0.9560037538527246, 0.9804533364963247, 0.01690034491715281, 0.8619175907747932, 0.10985224196149326, 0.08728100095716604, 0.9055403849305976, 0.9471099986253356, 0.22033385161810445, 0.03389751563355453, 0.737270965029811, 0.9657729357879645, 0.9129249456610556, 0.8866822167645574, 0.11205324717354297, 0.8710856232303157, 0.9426940601733451, 0.9878587019569355, 0.9885646376204759, 0.969894764255667, 0.9683963241870666, 0.9793160879628524, 0.9964457559946379, 0.9942107136565913, 0.9176381509105231, 0.11011961217704754, 0.011714852359260376, 0.046859409437041505, 0.8317545175074866, 0.6321785097886133, 0.2292515475057609, 0.02084104977325099, 0.11115226545733861, 0.9282544236743274, 0.06347038794354376, 0.6473082864869864, 0.2541578548169762, 0.014561127098889265, 0.08471928493899208, 0.0013237388271717513, 0.9940983129155957, 0.9515157994621907, 0.04704068567581926, 0.7489372324702803, 0.082940156323155, 0.12007753975143337, 0.2331461338243311, 0.6328252203803273, 0.13322636218533207, 0.9264236615820054, 0.11294938585483422, 0.7066576961174243, 0.0868841429652571, 0.089780281064099, 0.9351434733662299, 0.013651729538193137, 0.020477594307289706, 0.020477594307289706, 0.7504026032101714, 0.6583565531022499, 0.3072330581143833, 0.5200158325290738, 0.9880252994903873, 0.7325045955234745, 0.25853103371416747, 0.9945016905352112, 0.8661831722005195, 0.9741149376354599, 0.9499326879468047, 0.9225055468640593, 0.9818448400081098, 0.14848382231991886, 0.06705721007996336, 0.47658517163973957, 0.1125603169199385, 0.14848382231991886, 0.04550310683997513, 0.875914048415671, 0.9837787971875472, 0.0557232862798812, 0.6686794353585743, 0.278616431399406, 0.8874179103933008, 0.991853489232365, 0.9838021267973315, 0.8412483148790821, 0.9449402758142031, 0.9425003039638589, 0.18610284892398962, 0.5272914052846372, 0.16128913573412434, 0.1178651376518601, 0.9159610844916333, 0.9241652005558052, 0.4655474721190008, 0.947993485857092, 0.04121710808074313, 0.991372550520974, 0.09035205014169398, 0.8131684512752458, 0.09637552015114025, 0.9209375352660742, 0.9623632567593996, 0.580787887192188, 0.9805338291852, 0.012256672864815, 0.16798920069329054, 0.34586011907442166, 0.47432244901634973, 0.6191585075548185, 0.37346068709655716, 0.9841113574777485, 0.41824950376000236, 0.41824950376000236, 0.13070296992500074, 0.927696509056466, 0.8566751470169548, 0.8330081081484674, 0.9486581102148605, 0.9738115109492385, 0.058270799439608355, 0.8740619915941253, 0.5607171205264097, 0.9284278745168669, 0.9937432790585519, 0.9493302023563299, 0.9942925247508672, 0.09062572409174979, 0.032954808760636285, 0.7414831971143164, 0.12358053285238607, 0.3800717910865837, 0.32159920784249385, 0.08770887486613468, 0.17541774973226937, 0.9583596587701647, 0.1790799743043072, 0.7373881294883238, 0.07373881294883237, 0.7927922770746889, 0.990052976835913, 0.9458547944922777, 0.9814392138280189, 0.2230364030542582, 0.7609477280674691, 0.9551065060487605, 0.9818240878119314, 0.9858225796217889, 0.9865497192221997, 0.8007431800782201, 0.1120020988152037, 0.8848165806401093, 0.9486085073644842, 0.9948866724569174, 0.9587794679528041, 0.5155377248978804, 0.4603015400873932, 0.4003624145620267, 0.5783012654784829, 0.5091230125918098, 0.49215224550541614, 0.9768680371782823, 0.9656155928222382, 0.9717926891858675, 0.07473466317303747, 0.04638703231429912, 0.26801396448261716, 0.4097521187763089, 0.03607880291112154, 0.16235461310004692, 0.9852776233024936, 0.15121611287385844, 0.43852672733418946, 0.39316189347203195, 0.36928177738540047, 0.5908508438166408, 0.9924912215400048, 0.9852252039177358, 0.8695011690147179, 0.7655617559641255, 0.9841184441976631, 0.9485625722893319, 0.5267258448445828, 0.37623274631755915, 0.9655658769788843, 0.9772059394127056, 0.9651540385170186, 0.9760125618448987, 0.9645654375213395, 0.9719766843394954, 0.9457057892010152, 0.2590080788824047, 0.7194668857844575, 0.9809098751060857, 0.8146976557404264, 0.4573511549522016, 0.25049938777464986, 0.2523971104093063, 0.007590890538625753, 0.023721532933205478, 0.007590890538625753, 0.114064708019295, 0.7433872350223019, 0.12979777119437017, 0.011799797381306378, 0.9495206397409185, 0.9256048696696977, 0.5898971865210805, 0.4083903598992095, 0.3972574937700043, 0.23150756676187112, 0.19490022091958403, 0.13354902094315846, 0.03186194915902765, 0.007457051930836259, 0.0027116552475768213, 0.24922665632622934, 0.7476799689786879, 0.6350730845729815, 0.03569471393940201, 0.9459099193941534, 0.7902609168363536, 0.1975652292090884, 0.7339408731501788, 0.2201822619450536, 0.9717049768899816, 0.022084204020226855, 0.8861309745125413, 0.9823285366281083, 0.6898805701490953, 0.30661358673293126, 0.9609873988490554, 0.9897992345679895, 0.9188775987057538, 0.9666703551219862, 0.9897010841481161, 0.9553366474826924, 0.20751770099804545, 0.7608982369928333, 0.10307146048632612, 0.048100014893618856, 0.2427905513677904, 0.5130668255319345, 0.032066676595745906, 0.05268096869301113, 0.006871430699088408, 0.1410382855665832, 0.8566769938118388, 0.0013059100515424372, 0.988836616255997, 0.9796159459821238, 0.9702201410552156, 0.979271945373595, 0.17548194396511602, 0.09505271964777116, 0.1657329470781651, 0.16085844863468968, 0.3704618817041338, 0.012186246108688611, 0.018279369163032917, 0.29774763610490546, 0.14720856281696668, 0.3190629031191269, 0.19050519893960394, 0.04263053402844284, 0.0006661020941944194, 0.003330510470972097, 0.9518229589426652, 0.9627385362037902, 0.8812580555682158, 0.9847936437681578, 0.9908078194443786, 0.18531231998362785, 0.8030200532623873, 0.5241300116611524, 0.2085931561560546, 0.15882727626095527, 0.09106097172294769, 0.002117697016812737, 0.010588485084063685, 0.005294242542031842, 0.9604272754072164, 0.9889413165927092, 0.2083401331289641, 0.7738347801932953, 0.930096554629721, 0.9911032609305653, 0.9686250721833423, 0.9313622907012463, 0.9562931568424554, 0.05183690315284955, 0.15551070945854867, 0.6121691419955566, 0.03250091546885012, 0.14769403358629357, 0.5267240209021122, 0.1663339013375091, 0.2772231688958485, 0.9939305740902269, 0.47300470368986525, 0.3412197737276317, 0.08323258734456856, 0.04973654609614463, 0.021654006463627595, 0.02554496075006068, 0.005582673541403989, 0.9607477636050092, 0.9293562891974042, 0.9529523912345897, 0.9424903569811764, 0.26565021125392974, 0.3495397516499075, 0.11052114052168505, 0.1790975108453812, 0.014647380069138983, 0.07989480037712172, 0.0718354911082456, 0.14974158709887814, 0.5362367646108473, 0.2195535432463281, 0.022258884568752156, 0.9464871199698393, 0.8434991091983933, 0.11732109182617959, 0.8745754117951569, 0.8827483517379061, 0.6350730845729815, 0.961672157185976, 0.38258944035238723, 0.27412102250633674, 0.09439929251873849, 0.16247570539282874, 0.05128423103181466, 0.03449204918953906, 0.0009076855049878701, 0.9749891526512311, 0.3448968894481874, 0.5411796720609771, 0.0771110931693102, 0.01892726832337614, 0.014721208695959219, 0.0035050496895141, 0.08903889722611176, 0.5749906250446795, 0.19500772561493493, 0.03918965546923933, 0.06709269016333774, 0.004075724168800891, 0.030411172644129722, 0.9556728058288859, 0.9857433614409987, 0.9929833384854071, 0.9150021834062877, 0.9676988514581913, 0.786228247000897, 0.579741158914866, 0.4032981975059937, 0.9564749602354556, 0.9227667357404511, 0.961946151124076, 0.9594223763745268, 0.9821162036774751, 0.9036580810502052, 0.24462618275828923, 0.2156295447158196, 0.26518743518840404, 0.06115654568957231, 0.16976213544864036, 0.0337415424494192, 0.00948980881389915, 0.06323786312049064, 0.006629775972309502, 0.380957127024246, 0.3243490398760649, 0.16268450270513318, 0.05813803544948333, 0.0035698793697051165, 0.9899914465222723, 0.9922269284953129, 0.8646140904148917, 0.9201331752124088, 0.9697757864061854, 0.2728168855080278, 0.18187792367201852, 0.500164290098051, 0.9587633830585739, 0.9636149532391185, 0.9936963955930282, 0.9580491424339456, 0.7717425897997839, 0.3304009613557794, 0.6608019227115588, 0.9860518795715351, 0.95064005982295, 0.864607275440481, 0.9662197355662633, 0.03534950252071695, 0.06983093441027874, 0.08010018947061384, 0.24646212144804258, 0.5832936874270341, 0.018484659108603193, 0.9846466689072825, 0.8130846441492102, 0.9503609359962858, 0.9574912194992623, 0.9705666806927664, 0.9259219424903312, 0.06988090132002499, 0.10254146084734561, 0.26394931588483406, 0.6342379245002489, 0.06587613278502137, 0.041172582990638355, 0.3376151805232345, 0.45289841289702193, 0.041172582990638355, 0.008234516598127671, 0.03705532469157452, 0.016469033196255343, 0.7866838320750243, 0.9453797135005575, 0.011390117030127198, 0.03417035109038159, 0.807165688552032, 0.04638883267540414, 0.13916649802621242, 0.9784516048725213, 0.017165817629342477, 0.9894416892841479, 0.9605377241250033, 0.9259278508336136, 0.9318054266038645, 0.7113591368456029, 0.18586055208407667, 0.04314619959094637, 0.049784076451091966, 0.002212625620048532, 0.0033189384300727977, 0.004425251240097064, 0.8285666557222467, 0.7381949719005688, 0.6276280988178922, 0.3379535916711727, 0.9859473675576004, 0.9020768990535275, 0.5647926541995647, 0.08403783815136208, 0.8327385780453151, 0.07639803468305643, 0.9757048089243635, 0.8314160575453459, 0.9590743059871979, 0.9687489588650401, 0.0044224188977145746, 0.25650029606744534, 0.030956932284002023, 0.30514690394230565, 0.39359528189659715, 0.008844837795429149, 0.12546839003545238, 0.25877855444812053, 0.15683548754431545, 0.12322788307053358, 0.2789431171323896, 0.056012674122969806, 0.5657844576626674, 0.8794329229978461, 0.05562225324943642, 0.05411894910755976, 0.00901982485125996, 0.9629490706996112, 0.4407096783648186, 0.5609032270097691, 0.9213426398779708, 0.11409426458820184, 0.8823289794820941, 0.9763235090333181, 0.7161788490339771, 0.7904859576161527, 0.91174968425501, 0.01586986640722107, 0.8530053193881325, 0.1289426645586712, 0.1840519936333768, 0.576696246717914, 0.23313252526894393, 0.36364042099953703, 0.6127642453307116, 0.017163139049544054, 0.0004018126198889912, 0.0008036252397779824, 0.005223564058556886, 0.949807543296468, 0.8188814388923629, 0.1770554462469974, 0.8497286725020669, 0.9048733949145743, 0.045243669745728715, 0.005936121933408151, 0.7687277903763555, 0.13059468253497933, 0.09200988996782634, 0.9369715833805755, 0.9705536527707924, 0.9836413803112406, 0.9282382855643238, 0.2034895241314478, 0.7800431758372166, 0.9594519563598652, 0.985250998009622, 0.7650734961423199, 0.9050489498471457, 0.9285370537420659, 0.7488066614954033, 0.9637170803321442, 0.9717566059451119, 0.016195943432418532, 0.017265807031345424, 0.7855942199262168, 0.19424032910263603, 0.923796744492603, 0.9682356903454868, 0.011948884184887152, 0.9320129664211979, 0.035846652554661455, 0.9714382091437161, 0.8673115895957239, 0.5570745581043188, 0.42599819149153795, 0.9328125830040329, 0.9291982920552954, 0.15053423742141656, 0.1567739985580556, 0.07487713363966834, 0.2667497885913185, 0.12557519287486046, 0.10061614832830434, 0.03587862653567442, 0.08969656633918603, 0.9770237748015815, 0.9778210260144878, 0.13424982952784048, 0.43631194596548156, 0.4195307172745015, 0.8609963097031663, 0.12299947281473804, 0.979860740495084, 0.19702222568713781, 0.7880889027485513, 0.9137477670977601, 0.26322843692142683, 0.7194910609185667, 0.8014363246515692, 0.9939574785067669, 0.9235125135132569, 0.013760785633466608, 0.027521571266933217, 0.5917137822390641, 0.13760785633466607, 0.2247594986799546, 0.5189561997826596, 0.19323458645921887, 0.676321052607266, 0.37837638980643984, 0.4729704872580498, 0.9253387332044846, 0.9491213499835521, 0.050619805332456115, 0.6506746899582436, 0.19478404590786685, 0.31182167005851646, 0.16218070775161447, 0.22320746891588175, 0.06437069328285729, 0.038455219363784876, 0.004179915148237486, 0.9861265936533523, 0.9851544793557666, 0.9934931930214891, 0.7989523085315201, 0.2612384024662151, 0.7090756638368695, 0.08108048304668845, 0.5805362586142894, 0.22702535253072767, 0.10702623762162876, 0.13096009548865112, 0.2650137365400263, 0.009280636688172127, 0.00824945483393078, 0.37844374050657453, 0.13508482290561652, 0.07218272979689433, 0.13886538392940492, 0.7984759575940782, 0.05207451897352684, 0.8969634366453398, 0.9160174730287752, 0.9232032863089349, 0.7110088283073528, 0.16214317412312967, 0.6878008837803727, 0.12291498683527573, 0.007845637457570791, 0.018306487400998512, 0.9807805461869203, 0.8345022638656364, 0.9521967095870197, 0.9728446463480357, 0.9223841411289879, 0.9768664147613816, 0.7721445303588091, 0.9916209915635974, 0.2714471912038601, 0.723859176543627, 0.862145064954934, 0.575623031151935, 0.3837486874346233, 0.9625110307726547, 0.9822745099113194, 0.9454725163896269, 0.18181391792391677, 0.09917122795850006, 0.5123846777855836, 0.08264268996541671, 0.13222830394466675, 0.9056075774945193, 0.9507786470430989, 0.9789973335805121, 0.9465268803072374, 0.19241652294323386, 0.7364908291965159, 0.06966805141048123, 0.05119477377751972, 0.9010280184843471, 0.040955819022015776, 0.010238954755503944, 0.3825226209440069, 0.016631418301913344, 0.5904153497179236, 0.3766267782365875, 0.4818108334197786, 0.0678606807633491, 0.07125371480151656, 0.991163623599109, 0.9905056606696755, 0.08284282952342278, 0.25543205769722027, 0.19329993555465316, 0.4556355623788253, 0.006903569126951899, 0.9663798421427394, 0.9435632593977479, 0.9269823781220559, 0.9374895590659704, 0.9780479333920774, 0.8571384777568761, 0.31599585532287056, 0.2532529959627338, 0.41581404066854266, 0.013689351133120746, 0.0008555844458200466, 0.9828190836754183, 0.27932981883590907, 0.654658429825448, 0.06533251132677587, 0.460841817970255, 0.9353698589602293, 0.2164775660312277, 0.09482075206326503, 0.551033804443125, 0.07871911492044643, 0.05903933619033483, 0.04529208853301704, 0.23778346479833945, 0.28873706439798363, 0.29439857546461073, 0.12455324346579685, 0.20511574548584244, 0.7794398328462012, 0.9693725894323157, 0.9707190930634912, 0.9825439403295422, 0.9137418020672977, 0.04471282986425776, 0.938969427149413, 0.09651837436524009, 0.44639748143923547, 0.45846227823489044, 0.9332589978177145, 0.7770668010887126, 0.19426670027217816, 0.11917867727887638, 0.17307858157585565, 0.11618423815126641, 0.13654642421901414, 0.42281480481852624, 0.017966634765659757, 0.013774419987005812, 0.9348296877731835, 0.04597523054622214, 0.5919723349912457, 0.257820232462373, 0.033628725973353, 0.0037365251081503334, 0.006405471613972, 0.010675786023286667, 0.09554828490841567, 0.9748436883878984, 0.9757771445053841, 0.385905128885267, 0.1461644676539388, 0.33177014086528966, 0.09898969237938712, 0.030934278868558478, 0.0038667848585698097, 0.0007733569717139619, 0.9709632943324153, 0.024274082358310384, 0.28381923098147666, 0.7144415124706136, 0.9495750661034232, 0.9611171518623441, 0.9230619112938605, 0.040133126577993936, 0.8826617509125196, 0.10900340899823285, 0.002658619731664216, 0.005317239463328432, 0.947561301032663, 0.05102253159406647, 0.9300614874906724, 0.9777311975717253, 0.994824127165797, 0.03259822100961874, 0.9616475197837528, 0.8823414527708523, 0.00914377705180057, 0.09600965904390597, 0.07772210494030483, 0.8137961576102507, 0.2287174335710999, 0.35505658735323126, 0.2635696139247913, 0.04792174798632569, 0.10455654106107423, 0.16632654776228228, 0.2666504972061986, 0.013200519663673197, 0.5306608904796625, 0.023760935394611753, 0.18920854948909352, 0.8108937835246864, 0.89694156978601, 0.9912115611849412, 0.7154135908278672, 0.03207730361060186, 0.029609818717478645, 0.6292086477464212, 0.0986993957249288, 0.1085693352974217, 0.10363436551117526, 0.9710654526261451, 0.9152839536229045, 0.1778171602958712, 0.13336287022190338, 0.31753064338548426, 0.13971348308961307, 0.22862206323754866, 0.9868724367879863, 0.4775087863988506, 0.38866994241766906, 0.011104855497647687, 0.06662913298588613, 0.05552427748823844, 0.9170679799283298, 0.6996366314716327, 0.35921555809652256, 0.055877975703903515, 0.35921555809652256, 0.07982567957700502, 0.14368622323860902, 0.8552726975858923, 0.9904218348361776, 0.910906070874242, 0.9803526467560509, 0.9834486718355097, 0.9848557446181181, 0.9713045990742033, 0.965423032782261, 0.13308720207477637, 0.009115561785943587, 0.6162119767297864, 0.14949521328947482, 0.08933250550224715, 0.9908297315738622, 0.9282095474834723, 0.05801309671771702, 0.22376424180314727, 0.22376424180314727, 0.16782318135236046, 0.03196632025759247, 0.10389054083717553, 0.23974740193194352, 0.9761159674561343, 0.9122960678823939, 0.9698715780085874, 0.9747791714249976, 0.018567222312857096, 0.599639321248218, 0.3595019575874939, 0.3521401427563507, 0.09202268538929025, 0.1950880930252953, 0.14209947163030856, 0.3106684526819491, 0.3357448300284741, 0.039007698094594506, 0.15742392445318495, 0.01462788678547294, 0.8025077874584321, 0.12400423562471702, 0.73943266428072, 0.13318973455988123, 0.09219068848661925, 0.8911766553706528, 0.476893627641215, 0.006824370620467388, 0.791626991974217, 0.09690606281063692, 0.10509530755519779, 0.9955935259993006, 0.09840807634109904, 0.8979736966125288, 0.9369693366784667, 0.9147770758390711, 0.9845681844876104, 0.9690097085786807, 0.031786004675406515, 0.678101433075339, 0.28607404207865866, 0.9842601285856926, 0.9490653350403607, 0.9278903090859352, 0.29795702092468207, 0.5146530361426326, 0.16252201141346292, 0.03130139886488703, 0.10594319615807918, 0.6332513770357915, 0.22874099170494366, 0.06828282341807686, 0.25606058781778823, 0.10242423512711528, 0.18351008793608153, 0.11309342628618979, 0.11309342628618979, 0.1451009997634133, 0.019204544086334114, 0.9779673350132988, 0.9292550428956368, 0.05736142240096523, 0.994432095390955, 0.9814180638390727, 0.9203127065662466, 0.24078424991103503, 0.08302905169346035, 0.40269090071328273, 0.17020955597159373, 0.0954834094474794, 0.9777283860600264, 0.8473683646564221, 0.14480345471976833, 0.8592104598686977, 0.9386268652078138, 0.39124757734795024, 0.5961867845302099, 0.9702632977850119, 0.9239113960379904, 0.27160510409673483, 0.22067914707859704, 0.05092595701813778, 0.022633758674727902, 0.4300414148198301, 0.9698497156698127, 0.4157152108893976, 0.5308623976123265, 0.053340240908415576, 0.9308320696509198, 0.9387040253527825, 0.9958685531422078, 0.9886289431141106, 0.9178600848490974, 0.05736625530306859, 0.022063944347334073, 0.0989589909234137, 0.09236172486185279, 0.17812618366214467, 0.2375015782161929, 0.3892386976320939, 0.9789964106007, 0.16140884744861453, 0.005565822325814295, 0.4007392074586292, 0.2560278269874575, 0.08348733488721442, 0.08348733488721442, 0.9795001311340715, 0.16663736033139148, 0.49991208099417445, 0.04165934008284787, 0.2707857105385112, 0.9883650410117106, 0.9764326748240187, 0.4831032440442914, 0.4977427362880578, 0.8782342353178307, 0.011517704093201082, 0.40887849530863846, 0.017276556139801624, 0.38584308712223625, 0.0691062245592065, 0.10365933683880975, 0.9904171102589808, 0.9163121171504602, 0.07055970841152207, 0.2620789169570819, 0.010079958344503152, 0.03807984263478968, 0.6047975006701891, 0.006719972229668768, 0.006719972229668768, 0.9446646544711881, 0.9480301684548716, 0.9614427420983263, 0.9332973339284573, 0.8843187750497579, 0.9669606501774093, 0.029752635390074132, 0.9780159134800569, 0.18574017760604888, 0.10318898755891605, 0.5159449377945803, 0.15330935294467526, 0.038327338236168816, 0.9679359849327067, 0.9823176139399282, 0.013835459351266594, 0.38540475378900624, 0.13155643037990117, 0.4002280135501219, 0.08338083615627538, 0.9825234332331626, 0.36573342909567724, 0.609555715159462, 0.47405691041081394, 0.47405691041081394, 0.03950474253423449, 0.29710599854628267, 0.5122517216315218, 0.18441061978734785, 0.9555266083165079, 0.9499026621508865, 0.40879363117203626, 0.21802326995841934, 0.3679142680548326, 0.9234150008305129, 0.9249791926510373, 0.9872313354152602, 0.31134641431555793, 0.6671708878190528, 0.9181236677088072, 0.16246293082853164, 0.8123146541426581, 0.9803515127372942, 0.9900046398255888, 0.9278922837201811, 0.9714439498893936, 0.9501504683866285, 0.9735257547551377, 0.8325144214719424, 0.8183511210722502, 0.10718649522974692, 0.8890174016114303, 0.8313884824736828, 0.34035918805350723, 0.10109678853074472, 0.5560323369190959, 0.25592720641481215, 0.14185864092942238, 0.06171981871872363, 0.21973548025514172, 0.15252227453003955, 0.07270659273148072, 0.09564956199341464, 0.9439130333715677, 0.986509305565063, 0.9771057231397031, 0.034760823693361274, 0.8950912101040529, 0.060831441463382235, 0.9861146610869749, 0.10440780979459126, 0.32068113008338744, 0.4288177902277855, 0.10067895944478443, 0.044746204197681966, 0.8034298967817599, 0.18597948530208652, 0.03821496273330545, 0.504437508079632, 0.22674211221761234, 0.020381313457762906, 0.020381313457762906, 0.8928337960482017, 0.0924417710832521, 0.4622088554162605, 0.20542615796278246, 0.2362400816571998, 0.9340252718589175, 0.2835239121695691, 0.1924841238582396, 0.009103978831132955, 0.03511534692008425, 0.42788700506324884, 0.02991307330229399, 0.019508526066713473, 0.918186494606691, 0.9905832202062799, 0.4589697498769444, 0.5006942725930302, 0.9810681005249969, 0.17687726602411533, 0.5595217515229515, 0.12853081331085714, 0.018277317489158583, 0.05954867956145216, 0.05247358892048755, 0.004716727093976409, 0.9878489325289324, 0.8412631496992131, 0.9253991122905815, 0.012305688458198594, 0.014766826149838312, 0.40854885681219333, 0.009844550766558874, 0.2116578414810158, 0.3297924506797223, 0.012305688458198594, 0.9237840453410552, 0.950505301782845, 0.946206423329692, 0.997196685862133, 0.18040685906877654, 0.06704308951880208, 0.07557584636664963, 0.26085856649133904, 0.3559378570816402, 0.058510332670954554, 0.8904055125836012, 0.9777273390015334, 0.7279550896027829, 0.9929091788656083, 0.975477279087562, 0.9907522930554713, 0.7874581314047474, 0.9266313111440563, 0.29387081267185866, 0.06038441356271068, 0.4126268260118563, 0.0946022479149134, 0.09258943412948971, 0.04428190327932117, 0.15148127631431316, 0.15059542089727038, 0.4987365997950778, 0.08769968628723393, 0.10098751754287544, 0.010630265004513204, 0.884752567253843, 0.9946483124527302, 0.9116426175629291, 0.7087313221053178, 0.9794852926039594, 0.9878993217568112, 0.9488372184319, 0.9326287424765775, 0.9292683364794804, 0.8871324169069837, 0.9802562946545057, 0.9745823533935353, 0.9793832166034488, 0.9745923803910922, 0.9492507735180935, 0.9250459343459284, 0.9733537445755144, 0.18987673917814712, 0.32007793175744803, 0.07595069567125885, 0.09222584474367146, 0.3146528820666438, 0.9879594835036178, 0.12245955695164583, 0.8760568305002355, 0.9057796187957127, 0.984356385286598, 0.9872110126888881, 0.9735750359165631, 0.505488709068044, 0.4633646499790403, 0.9142670729522734, 0.4657053359304715, 0.5344159592644755, 0.9923239679463083, 0.07369273091948501, 0.7833264360700815, 0.1419267410301193, 0.7264797921191483, 0.1678975519564254, 0.006457598152170207, 0.09686397228255311, 0.6896247756602755, 0.28734365652511473, 0.46258427336727764, 0.8766126827611294, 0.9350829441798403, 0.19661343997346675, 0.34641415614372717, 0.08800792075002797, 0.3183265218618033, 0.04119519694682161, 0.005617526856384765, 0.9943045829702475, 0.9875768461380516, 0.8643012696372375, 0.19840468027518404, 0.5158521687154785, 0.23808561633022085, 0.18812962844160427, 0.456886240501039, 0.32250793447132164, 0.02687566120594347, 0.9033145141515505, 0.9529661225929801, 0.9742053557894064, 0.7680733654356343, 0.988268101007638, 0.9874520707511297, 0.23659836387902414, 0.23659836387902414, 0.0009776791895827444, 0.0029330375687482333, 0.09874559814785719, 0.06745986408120937, 0.02835269649789959, 0.32654484932063665, 0.37891783506765264, 0.24145838177342066, 0.1293204067175998, 0.22065859607758292, 0.007234708068117473, 0.0226084627128671, 0.5744592225065024, 0.9781339485546854, 0.9874696126267785, 0.25579923567562146, 0.703447898107959, 0.9676785745403471, 0.8118041618088392, 0.9271631266646299, 0.9815352785534668, 0.898185081691815, 0.7602995212672409, 0.9606457259354091, 0.9162641320949342, 0.17117249197009132, 0.380931705590059, 0.1657882982515039, 0.008749314792704537, 0.21245131047926144, 0.05429061999575635, 0.004711169503763981, 0.0015703898345879937, 0.9014770478392004, 0.949131927209247, 0.7502717512298488, 0.8928797851263793, 0.7852547564968261, 0.20833289458079057, 0.913735297366367, 0.03729531825985172, 0.04661914782481465, 0.019887025714139858, 0.07102509183621378, 0.017046022040691305, 0.14773219101932464, 0.3721714812217602, 0.28694137101830364, 0.08523011020345653, 0.9855749765203075, 0.9784835145267899, 0.9874421451476216, 0.07635492608873416, 0.1874166367632566, 0.7357838332187111, 0.9391412999501316, 0.15324886322589423, 0.8275438614198289, 0.08155261411476974, 0.9106708576149288, 0.810392858263476, 0.05903524133045189, 0.05903524133045189, 0.002683420060475086, 0.06708550151187714, 0.24993617998223572, 0.53302715934987, 0.15812288937651647, 0.025503691834922012, 0.033154799385398614, 0.8109578320829406, 0.9859996578389951, 0.018448622363360187, 0.11530388977100117, 0.7056598053985272, 0.16142544567940165, 0.012797506186329391, 0.819040395925081, 0.1535700742359527, 0.2694572235863166, 0.6287335217014053, 0.6334999414327208, 0.2533999765730883, 0.056311105905130734, 0.014077776476282684, 0.028155552952565367, 0.889223454046753, 0.8914678732519871, 0.9773769916804241, 0.024434424792010603, 0.14670988156316048, 0.14670988156316048, 0.2595636366117455, 0.24514343457775958, 0.1887165570534671, 0.0037617918349528326, 0.004388757140778305, 0.005015722446603777, 0.6698239895507052, 0.2679295958202821, 0.9617783636499272, 0.9968004426127577, 0.38348025592888735, 0.2781285372671051, 0.2823426060135764, 0.05478289370412676, 0.9013623700827839, 0.7764722561713313, 0.9685306711561397, 0.2069963431602535, 0.16076004657806628, 0.41043604812187723, 0.15222411490135482, 0.06544214285478805, 0.0035566381986297853, 0.08533875186581648, 0.17921137891821462, 0.2730840059706127, 0.4565623224821182, 0.9891563645523883, 0.924942122494283, 0.013064154272518122, 0.06009510965358336, 0.9815480026581969, 0.9187512149627342, 0.895248376945323, 0.7168503637579661, 0.9016538068898552, 0.7981639781087952, 0.1591205132622432, 0.6669983158663892, 0.04141492810935097, 0.019617597525482037, 0.07629065704354127, 0.02833652975902961, 0.006539199175160679, 0.4847130222103797, 0.2454865767467698, 0.15069030303034867, 0.09971494829604674, 0.018780393849479656, 0.32904918873138583, 0.5004528602134144, 0.16280482508660904, 0.0014331410659032485, 0.0017197692790838983, 0.0040127949845290955, 0.9740976651152614, 0.03406809962057613, 0.9539067893761315, 0.19347065724855694, 0.06729414165167198, 0.5972355071585889, 0.11776474789042596, 0.016823535412917996, 0.15190310422080705, 0.02579486675447667, 0.1175099485481715, 0.5961480316590164, 0.10604556332395965, 0.5016036457817334, 0.3554649866230926, 0.2843719892984741, 0.3554649866230926, 0.6555988876592778, 0.33773276030932486, 0.33900131697440117, 0.3396703985210612, 0.15901838092286055, 0.0591022032883002, 0.07895162250588027, 0.02074152794646007, 0.0033454077333000113, 0.8605976888411881, 0.8775521375507223, 0.9711868986857627, 0.9938361731216891, 0.16653949464643403, 0.261704920158682, 0.5472011966954261, 0.7797300135390942, 0.19493250338477355, 0.24195040298686932, 0.48390080597373863, 0.984653386664295, 0.9893715489457385, 0.028042872734811108, 0.016825723640886664, 0.6057260510719199, 0.35334019645861997, 0.966584523395378, 0.367266637559509, 0.025328733624793723, 0.37993100437190586, 0.21529423581074664, 0.9927166291453715, 0.9796510886551273, 0.6745031101925794, 0.9338507112326446, 0.15079160140038214, 0.056546850525143307, 0.7822314322644823, 0.48317978060174116, 0.8082104860110626, 0.981884177761672, 0.014230205474806842, 0.9921312958959789, 0.055650221676802974, 0.5217208282200279, 0.4173766625760223, 0.9533880725124779, 0.9109542743347391, 0.00597737601519026, 0.28332762312001836, 0.39928871781470937, 0.2893049991352086, 0.020323078451646884, 0.037532109107105215, 0.3534273607585741, 0.15481995006680901, 0.015638378794627174, 0.3800126047094403, 0.056298163660657825, 0.7848876257338873, 0.08301696041416115, 0.05282897480901164, 0.07798562947996956, 0.4753981535485483, 0.11386396911785257, 0.27533208351661526, 0.13316294693443775, 0.0019298977816585182, 0.8196495931245742, 0.9468233017494616, 0.9874673410623181, 0.7285476719121214, 0.9837421498217295, 0.9959176284545344, 0.8937781770872495, 0.2952050120236005, 0.2296038982405782, 0.08200139222877792, 0.37720640425237845, 0.9696912778548749, 0.99473442692326, 0.9685396469961823, 0.9742338994800761, 0.459510711867966, 0.9506942642810432, 0.8785485244084106, 0.11713980325445474, 0.8664059533287454, 0.11349425747665229, 0.8512069310748922, 0.906498732160045, 0.9266259013425991, 0.8905890444745594, 0.9062980472241987, 0.08770626263459988, 0.980497632365805, 0.9309379826539398, 0.9226217345698795, 0.36774533895520695, 0.6129088982586782, 0.7360416867488615, 0.9109659858163965, 0.8998630001243301, 0.9787115116956677, 0.2106598320649978, 0.28463198683591306, 0.3489555996801872, 0.09970159990862491, 0.04985079995431246, 0.006432361284427414, 0.9862683223764319, 0.1416381170254756, 0.8498287021528537, 0.9593652688220407, 0.7991069013809116, 0.8986722954500026, 0.9196677104770022, 0.997943530498793, 0.08819344807497748, 0.08819344807497748, 0.45860592998988287, 0.15874820653495944, 0.10583213768997297, 0.05291606884498649, 0.017638689614995495, 0.9008008501586464, 0.9578657368955608, 0.5013064686758953, 0.12131875723970992, 0.8734950521259114, 0.9790419096781319, 0.08224919234028027, 0.1273535881397888, 0.02653199752912267, 0.18041758319803416, 0.49880155354750616, 0.06632999382280667, 0.013265998764561335, 0.8148643194921045, 0.8219666164158769, 0.13615397940631088, 0.04034191982409212, 0.5754426574718722, 0.0867730991425839, 0.08220609392455316, 0.10504112001470682, 0.027402031308184388, 0.12330914088682975, 0.9619241938417151, 0.021376093196482558, 0.17607393334352475, 0.04870130071203876, 0.27909591561899133, 0.48513988016992454, 0.009365634752315145, 0.9120953519435471, 0.9303974576124401, 0.7019903225346452, 0.25526920819441645, 0.08052976449268048, 0.03390726926007599, 0.30092701468317445, 0.4323176830659689, 0.12291385106777547, 0.025430451945056994, 0.9389009372995278, 0.9378264092779472, 0.9939147086606089, 0.10985405487326143, 0.27018159441802136, 0.005938057020176294, 0.20189393868599398, 0.3800356492912828, 0.02969028510088147, 0.9107673480164109, 0.8707366173395735, 0.9835344024260497, 0.9202210652322882, 0.8168385625654603, 0.9689269763911558, 0.9284703488446345, 0.9829736070790063, 0.9709454940841117, 0.8677263773899998, 0.9826681113831268, 0.26681982219810924, 0.7115195258616247, 0.42281516720457935, 0.3220332814518248, 0.16734480564527143, 0.08296927338715138, 0.0018750118279582232, 0.002812517741937335, 0.0013039509789087398, 0.06389359796652824, 0.03390272545162723, 0.016951362725813615, 0.3246837937482762, 0.5593949699518493, 0.920686897917463, 0.06884726077973928, 0.7802689555037119, 0.13769452155947856, 0.6769368951817885, 0.998882050171543, 0.7841949620510699, 0.2977316357727377, 0.6224041852232869, 0.053881827355623066, 0.0023026421946847463, 0.010592154095549833, 0.011513210973423732, 0.0011513210973423732, 0.905197066377842, 0.9871662450923425, 0.269126617933005, 0.2699252132384442, 0.2755153803765185, 0.07426936340584413, 0.07866163758575963, 0.032343109870286955, 0.1699566303938722, 0.07931309418380704, 0.7478091737330378, 0.9586789245926962, 0.4316228364807022, 0.36198505768777306, 0.03810369028292348, 0.02036576549604531, 0.033504969041880994, 0.045330252233133105, 0.06898081861563733, 0.9628529226485374, 0.6477333648503041, 0.9083616755955765, 0.9948796338383031, 0.14302443659013772, 0.7000669790990952, 0.037638009628983615, 0.11291402888695083, 0.9569170426329937, 0.926362519712852, 0.9041462690856009, 0.08219511537141826, 0.9300699259434488, 0.9292401615963849, 0.983538703449185, 0.5470845631500997, 0.44119722834685454, 0.9909306632554604, 0.9725755093231618, 0.7249284602192718, 0.27184817258222693, 0.10141278393015828, 0.00853004724646191, 0.5989988733071031, 0.24263245501047215, 0.04454580228707887, 0.004738915136923284, 0.6762166902358385, 0.020491414855631468, 0.2595579215046653, 0.040982829711262936, 0.9947209069154115, 0.1058279018252317, 0.1991317916001401, 0.21040340244543107, 0.13338072833594292, 0.2861736753498869, 0.04132923976606682, 0.0225432216905819, 0.10384014962347018, 0.11783279389897326, 0.5324569374309854, 0.1075224244328131, 0.1251973435176591, 0.0007364549618685828, 0.0014729099237371656, 0.011046824428028743, 0.21218286285347765, 0.660972515219826, 0.009158972497272416, 0.009158972497272416, 0.07021878914575519, 0.03663588998908966, 0.0015264954162120693, 0.5096593589854181, 0.2780859803396165, 0.08708742444799379, 0.018802966642180476, 0.08114964550835785, 0.019792596465453133, 0.003958519293090627, 0.20729717132476205, 0.7600896281907942, 0.990820331883764, 0.11553132162810698, 0.32873912426906804, 0.2678226092287934, 0.153866714713797, 0.12393359956469657, 0.0031508542262210994, 0.006826850823479049, 0.9881783689727883, 0.9653920833988042, 0.017621704646004942, 0.16170505439863359, 0.48407859233437106, 0.11713250735285638, 0.19798503455217317, 0.019694846369064347, 0.9508397874518674, 0.8847742861121598, 0.7868321985206344, 0.7080631096057981, 0.9800039900284307, 0.28922713431988195, 0.7063816549735578, 0.7140969624841761], \"Term\": [\" \", \" \", \" \", \" \", \" \", \" \", \"  \", \"  \", \"   \", \"40\", \"Action\", \"aaron\", \"ably\", \"abortion\", \"abortion\", \"abrams\", \"abuse\", \"abuse\", \"accident\", \"accident\", \"accident\", \"ace\", \"act\", \"act\", \"act\", \"act\", \"act\", \"act\", \"acting\", \"acting\", \"acting\", \"acting\", \"action\", \"action\", \"action\", \"action\", \"action\", \"action\", \"action\", \"activist\", \"actor\", \"actor\", \"actor\", \"actor\", \"actor\", \"actorsactresse\", \"actually\", \"actually\", \"actually\", \"actually\", \"actually\", \"adam\", \"adam\", \"adam\", \"addict\", \"addiction\", \"addiction\", \"adjust\", \"administration\", \"adoption\", \"adore\", \"adore\", \"afternoon\", \"aimless\", \"aircraft\", \"ajla\", \"akhtar\", \"akira\", \"al\", \"al\", \"alain\", \"alexander\", \"alexander\", \"ali\", \"alien\", \"alien3\", \"alliance\", \"almodovar\", \"alright\", \"amazing\", \"amazing\", \"amazing\", \"amazing\", \"amazing\", \"amazing\", \"amazon\", \"ambitious\", \"american\", \"american\", \"american\", \"american\", \"american\", \"american\", \"american\", \"americans\", \"americans\", \"americas\", \"anakin\", \"ancestor\", \"andie\", \"andy\", \"andy\", \"ang\", \"angelina\", \"animator\", \"anna\", \"anna\", \"anne\", \"anne\", \"annie\", \"annie\", \"ant\", \"anton\", \"apollo\", \"applause\", \"apple\", \"apple\", \"applicable\", \"ark\", \"arkin\", \"arm\", \"arm\", \"army\", \"army\", \"army\", \"army\", \"arnold\", \"aspect\", \"aspect\", \"aspect\", \"aspect\", \"atheist\", \"atmosphere\", \"atmosphere\", \"atmosphere\", \"atrocious\", \"atrocious\", \"atrocious\", \"attest\", \"audience\", \"audience\", \"audience\", \"audience\", \"audience\", \"audience\", \"audience\", \"audition\", \"aussie\", \"autistic\", \"ava\", \"avenger\", \"awaken\", \"awaken\", \"award\", \"award\", \"award\", \"awesome\", \"awful\", \"awful\", \"babe\", \"backdrop\", \"bacon\", \"bad\", \"bad\", \"bad\", \"baldwin\", \"baldwin\", \"bale\", \"bale\", \"bandit\", \"bankrupt\", \"barbie\", \"barne\", \"barrel\", \"basket\", \"bass\", \"bate\", \"bateman\", \"batman\", \"beatle\", \"beautiful\", \"beautiful\", \"beautiful\", \"beautiful\", \"beautiful\", \"beautifully\", \"beautifully\", \"beautifully\", \"beauty\", \"beauty\", \"beauty\", \"beauty\", \"being\", \"belief\", \"belief\", \"believe\", \"believe\", \"believe\", \"believe\", \"believe\", \"bell\", \"bella\", \"benjamin\", \"bergman\", \"bernard\", \"bestow\", \"bette\", \"bette\", \"beyonce\", \"beyonc\\u00e9\", \"bible\", \"biblical\", \"bieber\", \"biel\", \"bigot\", \"billion\", \"bin\", \"birthday\", \"black\", \"black\", \"black\", \"black\", \"black\", \"black\", \"black\", \"blah\", \"blake\", \"blanchett\", \"bland\", \"blasphemy\", \"bloke\", \"blood\", \"blood\", \"blood\", \"blood\", \"blood\", \"bogart\", \"bogie\", \"bollywood\", \"bollywood\", \"bollywood\", \"bomb\", \"bomber\", \"bonnie\", \"boogie\", \"book\", \"book\", \"book\", \"boom\", \"borat\", \"boring\", \"boring\", \"bosnian\", \"bourne\", \"boy\", \"boy\", \"boy\", \"boy\", \"boy\", \"boycott\", \"boycott\", \"boycott\", \"boyle\", \"bradford\", \"bradley\", \"brainless\", \"brazilian\", \"breen\", \"brenda\", \"brian\", \"brian\", \"bribe\", \"bride\", \"bridesmaid\", \"brokeback\", \"brosnan\", \"brothers\", \"bubble\", \"bullying\", \"burton\", \"butcher\", \"butler\", \"cage\", \"caine\", \"camp\", \"camp\", \"camp\", \"capture\", \"capture\", \"capture\", \"capture\", \"capture\", \"car\", \"car\", \"car\", \"cardinal\", \"career\", \"career\", \"career\", \"career\", \"carl\", \"carl\", \"carrey\", \"carrier\", \"carter\", \"carter\", \"carter\", \"cary\", \"case\", \"case\", \"case\", \"case\", \"case\", \"case\", \"cast\", \"cast\", \"cast\", \"cast\", \"cast\", \"castthe\", \"cate\", \"catholic\", \"celebrity\", \"chan\", \"change\", \"change\", \"change\", \"change\", \"change\", \"change\", \"chanwook\", \"character\", \"character\", \"character\", \"character\", \"character\", \"character\", \"character\", \"character\", \"chase\", \"chase\", \"chase\", \"chase\", \"cheese\", \"chemical\", \"chernobyl\", \"chew\", \"chi\", \"child\", \"child\", \"child\", \"child\", \"chinese\", \"chris\", \"chris\", \"chris\", \"chris\", \"chris\", \"christ\", \"christ\", \"christian\", \"christian\", \"christian\", \"christianity\", \"christopher\", \"christopher\", \"christopher\", \"christopher\", \"christopher\", \"chronicle\", \"cigarette\", \"cinematographic\", \"city\", \"city\", \"city\", \"city\", \"clap\", \"claustrophobic\", \"cleef\", \"clerk\", \"clerk\", \"cliche\", \"climate\", \"clinic\", \"clockwork\", \"club\", \"club\", \"club\", \"clyde\", \"coach\", \"coach\", \"cocktail\", \"coen\", \"coleman\", \"collective\", \"com\", \"com\", \"come\", \"come\", \"come\", \"come\", \"come\", \"come\", \"come\", \"comedy\", \"comedy\", \"comedy\", \"comic\", \"comic\", \"comic\", \"comic\", \"comic\", \"commander\", \"commander\", \"commander\", \"commend\", \"commit\", \"commit\", \"commit\", \"commit\", \"compassion\", \"compelling\", \"compelling\", \"compelling\", \"complement\", \"comprehend\", \"compulsive\", \"computer\", \"computer\", \"computer\", \"computer\", \"conceal\", \"concentration\", \"concert\", \"concert\", \"condense\", \"condition\", \"condition\", \"condone\", \"confusing\", \"congratulation\", \"consequence\", \"consequence\", \"consequence\", \"conservative\", \"conservative\", \"contest\", \"continuation\", \"cool\", \"cool\", \"cool\", \"cool\", \"cool\", \"cool\", \"coppola\", \"counterculture\", \"craig\", \"craig\", \"crap\", \"crap\", \"crash\", \"crash\", \"crawford\", \"creature\", \"creature\", \"creature\", \"creature\", \"creepy\", \"creepy\", \"creepy\", \"criticise\", \"croatia\", \"croatian\", \"crop\", \"cruz\", \"cry\", \"cry\", \"cry\", \"cuckoos\", \"cullen\", \"cultured\", \"curtis\", \"cut\", \"cut\", \"cut\", \"cut\", \"cut\", \"cut\", \"cute\", \"cute\", \"cute\", \"cutie\", \"daddy\", \"daesu\", \"daft\", \"dakota\", \"dakota\", \"dakota\", \"dale\", \"damon\", \"dancing\", \"dancing\", \"danijel\", \"danish\", \"dark\", \"dark\", \"dark\", \"dark\", \"dark\", \"dark\", \"dark\", \"dark\", \"dark\", \"daughter\", \"daughter\", \"daughter\", \"daughter\", \"daughter\", \"david\", \"david\", \"david\", \"david\", \"david\", \"david\", \"davis\", \"davis\", \"davis\", \"day\", \"day\", \"day\", \"day\", \"day\", \"day\", \"deaf\", \"death\", \"death\", \"death\", \"death\", \"death\", \"death\", \"death\", \"death\", \"debut\", \"debut\", \"decay\", \"deckard\", \"dedication\", \"dee\", \"deer\", \"definitely\", \"definitely\", \"definitely\", \"definitely\", \"del\", \"delusional\", \"democracy\", \"denmark\", \"denzel\", \"depict\", \"depict\", \"depp\", \"depression\", \"depression\", \"derail\", \"derive\", \"dern\", \"dersu\", \"desolate\", \"destroy\", \"destroy\", \"destroy\", \"destroy\", \"determined\", \"devotion\", \"devour\", \"dhawan\", \"di\", \"diane\", \"die\", \"die\", \"die\", \"die\", \"die\", \"diesel\", \"dimitri\", \"dinklage\", \"director\", \"director\", \"director\", \"director\", \"director\", \"director\", \"director\", \"disability\", \"disappointing\", \"disappointing\", \"disappointing\", \"discomfort\", \"disney\", \"disney\", \"dissapointe\", \"distinctive\", \"distortion\", \"doc\", \"doc\", \"doctor\", \"doctor\", \"document\", \"documentarian\", \"documentary\", \"documentary\", \"dog\", \"dog\", \"dog\", \"dog\", \"dog\", \"dog\", \"dolphin\", \"dom\", \"domination\", \"dominic\", \"donkey\", \"donna\", \"donnie\", \"doo\", \"door\", \"door\", \"door\", \"dorian\", \"drab\", \"dracula\", \"drain\", \"dreadful\", \"drifter\", \"drinking\", \"dripping\", \"drive\", \"drive\", \"drive\", \"drive\", \"driver\", \"driver\", \"droid\", \"drown\", \"drown\", \"dub\", \"dub\", \"dude\", \"dude\", \"dumb\", \"dunne\", \"dupont\", \"dwayne\", \"eastwood\", \"eastwood\", \"eastwood\", \"eastwood\", \"economic\", \"ed\", \"ed\", \"educational\", \"educational\", \"edward\", \"edward\", \"effect\", \"effect\", \"effect\", \"effect\", \"effect\", \"effect\", \"effect\", \"egg\", \"egg\", \"ego\", \"ego\", \"eisenberg\", \"el\", \"election\", \"electronic\", \"element\", \"element\", \"element\", \"element\", \"element\", \"elf\", \"eli\", \"elizabeth\", \"elmer\", \"emmerich\", \"emotional\", \"emotional\", \"emotional\", \"emotional\", \"emotional\", \"emotional\", \"emperor\", \"emperor\", \"empire\", \"empire\", \"encompass\", \"encourage\", \"encourage\", \"encourage\", \"end\", \"end\", \"end\", \"end\", \"end\", \"end\", \"endlessly\", \"enjoy\", \"enjoy\", \"enjoy\", \"enjoy\", \"enjoy\", \"enjoyable\", \"enjoyable\", \"enjoyable\", \"enni\", \"ennio\", \"ennis\", \"enthusiasm\", \"enthusiasm\", \"enthusiasm\", \"episode\", \"episode\", \"episode\", \"escape\", \"escape\", \"escape\", \"escape\", \"eve\", \"eve\", \"eve\", \"ewan\", \"exaggeration\", \"exbf\", \"excel\", \"excerpt\", \"excruciatingly\", \"expect\", \"expect\", \"expect\", \"expect\", \"expect\", \"expect\", \"expectation\", \"expectation\", \"expectation\", \"expectation\", \"expedition\", \"explicit\", \"exposition\", \"extraordinary\", \"extraordinary\", \"extraordinary\", \"eyeopene\", \"face\", \"face\", \"face\", \"face\", \"face\", \"face\", \"fact\", \"fact\", \"fact\", \"fact\", \"fact\", \"fact\", \"factory\", \"factory\", \"fairy\", \"fairy\", \"fairy\", \"faith\", \"faith\", \"faith\", \"faith\", \"faith\", \"fake\", \"fake\", \"fake\", \"falcon\", \"family\", \"family\", \"family\", \"family\", \"family\", \"fan\", \"fan\", \"fan\", \"fan\", \"fan\", \"fantasticthe\", \"farfetched\", \"farhadi\", \"farhan\", \"farm\", \"farmer\", \"fascination\", \"fascist\", \"fascist\", \"fast\", \"fast\", \"fast\", \"fast\", \"fast\", \"fast\", \"father\", \"father\", \"father\", \"father\", \"father\", \"fearful\", \"february\", \"feel\", \"feel\", \"feel\", \"feel\", \"feel\", \"feel\", \"feel\", \"feelgood\", \"feelgood\", \"fellini\", \"fellini\", \"fellinis\", \"felt\", \"feminist\", \"feminist\", \"feminist\", \"festival\", \"festival\", \"festival\", \"field\", \"field\", \"field\", \"field\", \"fight\", \"fight\", \"fight\", \"fight\", \"fight\", \"fight\", \"film\", \"film\", \"film\", \"film\", \"film\", \"film\", \"film\", \"filmgoer\", \"filmmake\", \"filmmake\", \"filmmaker\", \"filmmaker\", \"filmmaker\", \"filmmaker\", \"filth\", \"finch\", \"fincher\", \"find\", \"find\", \"find\", \"find\", \"find\", \"find\", \"find\", \"finn\", \"firm\", \"firm\", \"firm\", \"fleet\", \"flip\", \"float\", \"fluff\", \"fonda\", \"footage\", \"footage\", \"footage\", \"footage\", \"footage\", \"football\", \"football\", \"force\", \"force\", \"force\", \"force\", \"force\", \"ford\", \"ford\", \"ford\", \"ford\", \"franchise\", \"franchise\", \"franchise\", \"franchise\", \"franchise\", \"francis\", \"franco\", \"frankenstein\", \"fred\", \"fred\", \"friar\", \"friend\", \"friend\", \"friend\", \"friend\", \"friend\", \"frog\", \"frontier\", \"frost\", \"fun\", \"fun\", \"fun\", \"fun\", \"function\", \"function\", \"funny\", \"funny\", \"furious\", \"gadot\", \"galaxy\", \"game\", \"game\", \"game\", \"game\", \"game\", \"garbage\", \"garbage\", \"gareth\", \"gary\", \"gary\", \"gen\", \"gender\", \"genocide\", \"genre\", \"genre\", \"genre\", \"genre\", \"genre\", \"george\", \"george\", \"george\", \"george\", \"gerard\", \"gere\", \"german\", \"germany\", \"get\", \"get\", \"get\", \"get\", \"get\", \"get\", \"ghost\", \"ghost\", \"ghost\", \"ghost\", \"ghost\", \"ghostbuster\", \"gibney\", \"gibson\", \"girl\", \"girl\", \"girl\", \"girl\", \"give\", \"give\", \"give\", \"give\", \"give\", \"give\", \"give\", \"glad\", \"glad\", \"glad\", \"global\", \"global\", \"glossy\", \"go\", \"go\", \"go\", \"go\", \"go\", \"go\", \"go\", \"go\", \"god\", \"god\", \"god\", \"god\", \"god\", \"goebbel\", \"goo\", \"good\", \"good\", \"good\", \"good\", \"good\", \"good\", \"good\", \"good\", \"goosebump\", \"gore\", \"gore\", \"gory\", \"gory\", \"goth\", \"gothic\", \"graduation\", \"graham\", \"graham\", \"grainy\", \"grandfather\", \"grant\", \"grant\", \"grant\", \"grant\", \"grant\", \"grasp\", \"grass\", \"gratuitous\", \"great\", \"great\", \"great\", \"great\", \"great\", \"great\", \"great\", \"greek\", \"greek\", \"greta\", \"grim\", \"grown\", \"gruele\", \"grumpy\", \"guido\", \"gun\", \"gun\", \"gus\", \"guy\", \"guy\", \"guy\", \"guy\", \"gyunwoo\", \"h\", \"hackman\", \"half\", \"half\", \"half\", \"half\", \"half\", \"halloween\", \"halloween\", \"han\", \"handed\", \"haneke\", \"hank\", \"hanks\", \"hannah\", \"happen\", \"happen\", \"happen\", \"happen\", \"happen\", \"happen\", \"happen\", \"hardy\", \"harvey\", \"hateful\", \"hater\", \"hater\", \"hathaway\", \"hatred\", \"hawaii\", \"hawkin\", \"head\", \"head\", \"head\", \"head\", \"head\", \"head\", \"headache\", \"health\", \"health\", \"health\", \"heartbreake\", \"heartbreake\", \"hearted\", \"heartfelt\", \"heartwarme\", \"heath\", \"heighten\", \"helicopter\", \"helm\", \"helmet\", \"help\", \"help\", \"help\", \"help\", \"help\", \"help\", \"hero\", \"hero\", \"hero\", \"hero\", \"hero\", \"hero\", \"hero\", \"hero\", \"hilarious\", \"hilarious\", \"hilarious\", \"hilton\", \"hindi\", \"hindi\", \"hiroshima\", \"hispanic\", \"historian\", \"historic\", \"historical\", \"historical\", \"historical\", \"hitchcock\", \"hitchcock\", \"hitchcockian\", \"hitler\", \"hitler\", \"hitler\", \"hobbit\", \"hobbs\", \"hole\", \"hole\", \"holland\", \"holly\", \"holocaust\", \"homosexual\", \"hood\", \"hooked\", \"hopkin\", \"horrible\", \"horribly\", \"horrifying\", \"horror\", \"horror\", \"horror\", \"horror\", \"hot\", \"hot\", \"hot\", \"hot\", \"hotel\", \"hotel\", \"hour\", \"hour\", \"hour\", \"hour\", \"hour\", \"howard\", \"hulk\", \"human\", \"human\", \"human\", \"human\", \"humble\", \"humble\", \"humble\", \"humility\", \"humor\", \"humor\", \"humor\", \"humor\", \"humour\", \"humour\", \"humour\", \"humour\", \"humphrey\", \"identical\", \"identical\", \"ideological\", \"idiotic\", \"illusion\", \"illusion\", \"imagery\", \"immaculate\", \"impactful\", \"implausible\", \"implement\", \"implication\", \"important\", \"important\", \"important\", \"important\", \"important\", \"important\", \"impossibly\", \"imprison\", \"inaccuracy\", \"inaccuracy\", \"inaccuracy\", \"incoherent\", \"increasingly\", \"india\", \"indulgent\", \"inept\", \"infectious\", \"information\", \"information\", \"information\", \"information\", \"informative\", \"inmate\", \"innate\", \"inner\", \"inner\", \"innocence\", \"innocent\", \"innocent\", \"innocent\", \"innuendo\", \"inspirational\", \"intercept\", \"international\", \"international\", \"internet\", \"internet\", \"internet\", \"interview\", \"interview\", \"intimate\", \"invasion\", \"invasion\", \"invasion\", \"invoke\", \"ireland\", \"irene\", \"iris\", \"ironic\", \"ironically\", \"ironically\", \"isif\", \"islam\", \"jack\", \"jacket\", \"jackie\", \"jackson\", \"jackson\", \"jackson\", \"jackson\", \"jake\", \"jake\", \"jake\", \"jake\", \"jam\", \"jane\", \"jane\", \"jane\", \"janitor\", \"japan\", \"jarring\", \"jasenovac\", \"jason\", \"jason\", \"jay\", \"jazz\", \"jean\", \"jedi\", \"jerome\", \"jerry\", \"jerry\", \"jessica\", \"jesus\", \"jewel\", \"jewish\", \"jewish\", \"jews\", \"jews\", \"jim\", \"jim\", \"jo\", \"joanna\", \"joel\", \"john\", \"john\", \"john\", \"john\", \"john\", \"john\", \"johnny\", \"johnson\", \"johnson\", \"johnson\", \"joker\", \"joker\", \"jolie\", \"jordan\", \"jovovich\", \"juan\", \"julie\", \"juror\", \"jury\", \"jury\", \"justin\", \"k\", \"karen\", \"karin\", \"kat\", \"kathy\", \"kaufman\", \"keanu\", \"keanu\", \"khan\", \"kicking\", \"kill\", \"kill\", \"kill\", \"kill\", \"kill\", \"kill\", \"killer\", \"killer\", \"killer\", \"killer\", \"kilmer\", \"kinetic\", \"knight\", \"knight\", \"know\", \"know\", \"know\", \"know\", \"know\", \"know\", \"know\", \"korean\", \"korean\", \"kristanna\", \"kubrick\", \"kubrick\", \"kung\", \"kung\", \"kurosawa\", \"kurosawa\", \"kurt\", \"kurt\", \"kurtz\", \"kylo\", \"lad\", \"lad\", \"lamb\", \"lame\", \"lampoon\", \"lara\", \"larry\", \"las\", \"lastly\", \"lastly\", \"later\", \"later\", \"later\", \"later\", \"later\", \"later\", \"later\", \"laugh\", \"laugh\", \"laugh\", \"laura\", \"lauren\", \"lawsuit\", \"lazy\", \"lead\", \"lead\", \"lead\", \"lead\", \"lead\", \"lead\", \"lead\", \"leave\", \"leave\", \"leave\", \"leave\", \"leave\", \"leave\", \"leave\", \"lebron\", \"ledger\", \"leftist\", \"legola\", \"leia\", \"leo\", \"leo\", \"let\", \"let\", \"let\", \"let\", \"let\", \"let\", \"let\", \"letdown\", \"leto\", \"lewis\", \"lewis\", \"lex\", \"liam\", \"liberal\", \"liberation\", \"license\", \"life\", \"life\", \"life\", \"life\", \"life\", \"lifetime\", \"lifetime\", \"lifetime\", \"lightsaber\", \"like\", \"like\", \"like\", \"like\", \"like\", \"like\", \"like\", \"likeable\", \"lilly\", \"liner\", \"lisbeth\", \"little\", \"little\", \"little\", \"little\", \"little\", \"little\", \"live\", \"live\", \"live\", \"live\", \"live\", \"lloyd\", \"loathe\", \"lock\", \"lock\", \"log\", \"loken\", \"lol\", \"look\", \"look\", \"look\", \"look\", \"look\", \"look\", \"look\", \"loser\", \"lot\", \"lot\", \"lot\", \"lot\", \"lot\", \"lot\", \"love\", \"love\", \"love\", \"love\", \"love\", \"love\", \"love\", \"lowbudget\", \"lucas\", \"luke\", \"lukewarm\", \"lunatic\", \"lycan\", \"lynch\", \"lynch\", \"lyric\", \"ma\", \"magnetic\", \"magnificently\", \"maid\", \"mail\", \"make\", \"make\", \"make\", \"make\", \"make\", \"make\", \"make\", \"man\", \"man\", \"man\", \"man\", \"man\", \"man\", \"man\", \"management\", \"manga\", \"manmohan\", \"marcus\", \"maria\", \"marine\", \"marine\", \"marine\", \"marion\", \"marisa\", \"matrix\", \"matthew\", \"maurice\", \"mccarthy\", \"mccarthy\", \"mcconaughey\", \"mcgregor\", \"mechanical\", \"mediocre\", \"mediocre\", \"meet\", \"meet\", \"meet\", \"meet\", \"meet\", \"mel\", \"merchandise\", \"mermaid\", \"meryl\", \"mesmerize\", \"mess\", \"mess\", \"message\", \"message\", \"message\", \"michael\", \"michael\", \"michael\", \"michael\", \"michael\", \"michael\", \"michael\", \"michael\", \"migraine\", \"mike\", \"mike\", \"mike\", \"military\", \"military\", \"military\", \"milk\", \"milk\", \"min\", \"minded\", \"minister\", \"ministry\", \"minute\", \"minute\", \"minute\", \"minute\", \"minute\", \"minute\", \"minute\", \"miscarriage\", \"misconception\", \"miserably\", \"miserably\", \"misery\", \"misfortune\", \"misrepresentation\", \"mission\", \"mission\", \"mission\", \"mixture\", \"miyazaki\", \"mockingbird\", \"mode\", \"modern\", \"modern\", \"modern\", \"modern\", \"modern\", \"modern\", \"moment\", \"moment\", \"moment\", \"moment\", \"moment\", \"moment\", \"monetary\", \"money\", \"money\", \"money\", \"money\", \"moody\", \"moore\", \"moore\", \"mormon\", \"morning\", \"morning\", \"moron\", \"morricone\", \"morris\", \"mortensen\", \"mother\", \"mother\", \"mother\", \"mountain\", \"mountain\", \"mountain\", \"movie\", \"movie\", \"movie\", \"movie\", \"movie\", \"movie\", \"moviegoe\", \"mrs\", \"mrs\", \"muddle\", \"mum\", \"mum\", \"murder\", \"murder\", \"murder\", \"murder\", \"murphy\", \"muslim\", \"mutant\", \"myers\", \"mythology\", \"mythology\", \"nader\", \"naive\", \"nana\", \"naomi\", \"narcissist\", \"nate\", \"nathan\", \"native\", \"native\", \"nature\", \"nature\", \"nature\", \"nauseating\", \"navy\", \"nazi\", \"nazi\", \"nazi\", \"neeson\", \"neighbour\", \"neil\", \"neil\", \"nerd\", \"nest\", \"new\", \"new\", \"new\", \"new\", \"new\", \"new\", \"new\", \"new\", \"newman\", \"newt\", \"nick\", \"nick\", \"nick\", \"nicolas\", \"nicolas\", \"ninety\", \"noah\", \"noah\", \"noisy\", \"nolan\", \"nolan\", \"nolte\", \"norton\", \"noteworthy\", \"novel\", \"novel\", \"novel\", \"novel\", \"novel\", \"novelist\", \"nurture\", \"nurture\", \"nut\", \"nut\", \"octopus\", \"officer\", \"officer\", \"ohh\", \"old\", \"old\", \"old\", \"old\", \"old\", \"old\", \"old\", \"oldboy\", \"oldman\", \"oliver\", \"omar\", \"opera\", \"opera\", \"order\", \"order\", \"order\", \"order\", \"original\", \"original\", \"original\", \"original\", \"original\", \"original\", \"original\", \"originality\", \"originality\", \"originality\", \"oscarwinne\", \"oskar\", \"osment\", \"overacting\", \"overall\", \"overall\", \"overall\", \"overall\", \"overall\", \"overblown\", \"overdramatic\", \"overrated\", \"overuse\", \"overview\", \"owen\", \"p\", \"pacino\", \"package\", \"package\", \"pad\", \"painfully\", \"painfully\", \"palme\", \"palpatine\", \"paltrow\", \"pan\", \"pan\", \"pan\", \"pan\", \"pan\", \"panda\", \"pandemic\", \"panic\", \"paranoid\", \"parent\", \"parent\", \"parent\", \"paris\", \"paris\", \"paris\", \"paris\", \"park\", \"park\", \"park\", \"part\", \"part\", \"part\", \"part\", \"pattinson\", \"patton\", \"paul\", \"paul\", \"paul\", \"paul\", \"paul\", \"pea\", \"peck\", \"peek\", \"penelope\", \"penguin\", \"penns\", \"people\", \"people\", \"people\", \"people\", \"people\", \"percy\", \"performance\", \"performance\", \"performance\", \"permanently\", \"perry\", \"person\", \"person\", \"person\", \"person\", \"person\", \"peter\", \"peter\", \"peter\", \"peter\", \"peter\", \"pierce\", \"pierce\", \"pill\", \"pine\", \"piss\", \"pistol\", \"pixar\", \"pixar\", \"planet\", \"planet\", \"planet\", \"plantation\", \"platoon\", \"platoon\", \"play\", \"play\", \"play\", \"play\", \"play\", \"play\", \"play\", \"pleasantly\", \"pleasantly\", \"plot\", \"plot\", \"plot\", \"plot\", \"plot\", \"plot\", \"plot\", \"poe\", \"poetic\", \"point\", \"point\", \"point\", \"point\", \"point\", \"point\", \"point\", \"pointless\", \"pointless\", \"police\", \"police\", \"policy\", \"polish\", \"politician\", \"politician\", \"poor\", \"poor\", \"poor\", \"poor\", \"poorly\", \"poorly\", \"pope\", \"popularity\", \"population\", \"porn\", \"porn\", \"porno\", \"portrayal\", \"portrayal\", \"portrayal\", \"portrayal\", \"power\", \"power\", \"power\", \"power\", \"power\", \"powerful\", \"powerful\", \"powerful\", \"powerful\", \"powerful\", \"predator\", \"predator\", \"predominantly\", \"prequel\", \"prescreening\", \"present\", \"present\", \"present\", \"present\", \"present\", \"present\", \"preserve\", \"prevail\", \"prison\", \"prison\", \"prison\", \"prison\", \"prison\", \"privilege\", \"product\", \"product\", \"product\", \"product\", \"product\", \"profession\", \"progressively\", \"project\", \"project\", \"project\", \"project\", \"project\", \"propel\", \"prophet\", \"propose\", \"proposition\", \"protocol\", \"punk\", \"purge\", \"purity\", \"question\", \"question\", \"question\", \"question\", \"question\", \"quit\", \"rabbit\", \"rabbit\", \"race\", \"race\", \"race\", \"race\", \"race\", \"race\", \"racism\", \"rail\", \"rambo\", \"rape\", \"rape\", \"raptor\", \"read\", \"read\", \"read\", \"read\", \"real\", \"real\", \"real\", \"real\", \"real\", \"real\", \"reali\", \"reality\", \"reality\", \"reality\", \"reboot\", \"reboot\", \"reckless\", \"recommend\", \"recommend\", \"recommend\", \"recommend\", \"redeem\", \"redemption\", \"redemption\", \"redford\", \"rediscover\", \"redmayne\", \"reeve\", \"refreshing\", \"refreshing\", \"refreshing\", \"regime\", \"rehash\", \"reid\", \"relatable\", \"relatable\", \"relatable\", \"relationship\", \"relationship\", \"relationship\", \"relationship\", \"release\", \"release\", \"release\", \"release\", \"release\", \"release\", \"release\", \"release\", \"relevance\", \"religion\", \"religion\", \"religious\", \"remade\", \"remainder\", \"remake\", \"remake\", \"remake\", \"remake\", \"remake\", \"ren\", \"represent\", \"represent\", \"reproduction\", \"resilience\", \"resistance\", \"resistance\", \"resistant\", \"retelling\", \"revenge\", \"revenge\", \"revenge\", \"revenge\", \"revenge\", \"revert\", \"review\", \"review\", \"review\", \"revisionist\", \"revolution\", \"rey\", \"reynold\", \"ridiculous\", \"ridiculous\", \"ridiculous\", \"ring\", \"ring\", \"ring\", \"ring\", \"ring\", \"ripley\", \"rise\", \"rise\", \"rise\", \"rise\", \"rise\", \"rise\", \"ritual\", \"river\", \"river\", \"river\", \"river\", \"riveting\", \"robber\", \"robin\", \"robin\", \"robinson\", \"rock\", \"rock\", \"rock\", \"rock\", \"rock\", \"rock\", \"rocky\", \"roger\", \"role\", \"role\", \"role\", \"role\", \"role\", \"role\", \"role\", \"rollercoaster\", \"rom\", \"romania\", \"romanian\", \"romp\", \"ron\", \"ron\", \"ronin\", \"room\", \"room\", \"room\", \"room\", \"room\", \"roughly\", \"rubbish\", \"rubbish\", \"run\", \"run\", \"run\", \"run\", \"rupert\", \"rural\", \"rural\", \"russell\", \"russell\", \"russell\", \"russian\", \"russian\", \"russian\", \"russians\", \"ryder\", \"s\", \"s\", \"s\", \"saber\", \"sack\", \"safety\", \"saint\", \"saint\", \"salman\", \"samurai\", \"samurai\", \"sand\", \"sandler\", \"sant\", \"santa\", \"sara\", \"saturday\", \"sawyer\", \"scandinavian\", \"scare\", \"scare\", \"scarlet\", \"scary\", \"scary\", \"scary\", \"scene\", \"scene\", \"scene\", \"scene\", \"scene\", \"scene\", \"scene\", \"schumacher\", \"schwarzenegger\", \"sci\", \"scientist\", \"scientist\", \"scientist\", \"scientology\", \"scifi\", \"scifi\", \"scifi\", \"scifi\", \"scifi\", \"scooby\", \"score\", \"score\", \"score\", \"score\", \"score\", \"score\", \"scotland\", \"scott\", \"scott\", \"scott\", \"scott\", \"scottish\", \"screen\", \"screen\", \"screen\", \"screen\", \"screen\", \"screen\", \"screen\", \"screenshot\", \"screw\", \"seal\", \"seal\", \"secretary\", \"see\", \"see\", \"see\", \"see\", \"see\", \"see\", \"see\", \"seed\", \"selfdiscovery\", \"semi\", \"sequence\", \"sequence\", \"sequence\", \"sequence\", \"sequence\", \"sequence\", \"sequence\", \"serb\", \"serbian\", \"serbs\", \"serial\", \"set\", \"set\", \"set\", \"set\", \"set\", \"set\", \"seth\", \"sexually\", \"shack\", \"shark\", \"shaw\", \"shawshank\", \"shelby\", \"shia\", \"shoot\", \"shoot\", \"shoot\", \"shoot\", \"shoot\", \"shoot\", \"show\", \"show\", \"show\", \"show\", \"show\", \"show\", \"showdown\", \"shyamalan\", \"siberia\", \"sienna\", \"sierra\", \"signal\", \"signature\", \"sigourney\", \"simba\", \"singh\", \"sith\", \"skeptical\", \"skywalker\", \"slater\", \"slim\", \"slop\", \"slut\", \"smart\", \"smart\", \"smart\", \"smart\", \"smart\", \"smith\", \"smoke\", \"smoke\", \"smolder\", \"smug\", \"snap\", \"snipe\", \"soap\", \"soap\", \"sofia\", \"soldier\", \"soldier\", \"solo\", \"son\", \"son\", \"son\", \"song\", \"song\", \"song\", \"song\", \"soviet\", \"soviet\", \"spanglish\", \"spartan\", \"spear\", \"special\", \"special\", \"special\", \"special\", \"special\", \"special\", \"spiderman\", \"spiritual\", \"splash\", \"spoof\", \"spoof\", \"spoof\", \"sport\", \"sport\", \"sport\", \"sport\", \"spotlight\", \"spout\", \"stack\", \"stain\", \"stair\", \"stallone\", \"star\", \"star\", \"star\", \"star\", \"star\", \"star\", \"star\", \"star\", \"start\", \"start\", \"start\", \"start\", \"start\", \"start\", \"stateoftheart\", \"statham\", \"steady\", \"stern\", \"stern\", \"stevens\", \"stigma\", \"stimulate\", \"sting\", \"stinker\", \"stoddard\", \"stoner\", \"stoop\", \"story\", \"story\", \"story\", \"story\", \"story\", \"story\", \"story\", \"story\", \"strangeness\", \"streep\", \"streetsmart\", \"stuart\", \"stunt\", \"stunt\", \"stupid\", \"stupid\", \"stupid\", \"style\", \"style\", \"style\", \"style\", \"style\", \"style\", \"style\", \"subconscious\", \"subtly\", \"suburban\", \"success\", \"success\", \"success\", \"superlative\", \"superman\", \"superman\", \"supernatural\", \"supernatural\", \"suppose\", \"suppose\", \"suppose\", \"suppose\", \"suppose\", \"sure\", \"sure\", \"sure\", \"sure\", \"sure\", \"surrogate\", \"surrounding\", \"suspense\", \"suspense\", \"suspense\", \"suspense\", \"suspenseful\", \"suspenseful\", \"suspenseful\", \"sweden\", \"sweden\", \"switch\", \"switch\", \"switch\", \"switch\", \"switch\", \"sync\", \"t2\", \"tackle\", \"tackle\", \"take\", \"take\", \"take\", \"take\", \"take\", \"take\", \"take\", \"take\", \"tall\", \"tall\", \"tauriel\", \"taxi\", \"team\", \"team\", \"team\", \"team\", \"tearjerking\", \"teaser\", \"ted\", \"tell\", \"tell\", \"tell\", \"tell\", \"tell\", \"tell\", \"tension\", \"tension\", \"tension\", \"tension\", \"terminator\", \"terrible\", \"terrible\", \"terrible\", \"terry\", \"tesla\", \"testify\", \"testimony\", \"thano\", \"thati\", \"theater\", \"theater\", \"theater\", \"theater\", \"theater\", \"theater\", \"theater\", \"thing\", \"thing\", \"thing\", \"thing\", \"thing\", \"think\", \"think\", \"think\", \"think\", \"think\", \"think\", \"thor\", \"threaten\", \"threaten\", \"thrill\", \"thrill\", \"thrill\", \"thrill\", \"thrill\", \"thriller\", \"thriller\", \"thriller\", \"thriller\", \"thriller\", \"tiana\", \"tiger\", \"tiger\", \"tiger\", \"tim\", \"tim\", \"time\", \"time\", \"time\", \"time\", \"time\", \"time\", \"time\", \"timely\", \"timethe\", \"timothy\", \"tina\", \"titanic\", \"titanic\", \"titanic\", \"todd\", \"todd\", \"toe\", \"toe\", \"tolerance\", \"tolkien\", \"tom\", \"tom\", \"tom\", \"tom\", \"tomei\", \"topic\", \"topic\", \"topic\", \"topic\", \"toro\", \"toronto\", \"torturous\", \"tourist\", \"toy\", \"toy\", \"toy\", \"trainwreck\", \"tramp\", \"trash\", \"trash\", \"travis\", \"trilogy\", \"trilogy\", \"trilogy\", \"trooper\", \"troy\", \"true\", \"true\", \"true\", \"true\", \"true\", \"truly\", \"truly\", \"truly\", \"truly\", \"truly\", \"truly\", \"truth\", \"truth\", \"truth\", \"truth\", \"try\", \"try\", \"try\", \"try\", \"try\", \"tuck\", \"tuco\", \"tunnel\", \"turd\", \"turmoil\", \"twilight\", \"uh\", \"uk\", \"uk\", \"uk\", \"uk\", \"unaware\", \"uncle\", \"undeniably\", \"understate\", \"underway\", \"underwhelme\", \"undoubtedly\", \"undoubtedly\", \"unemployed\", \"unexpectedly\", \"unexpectedly\", \"unforgiving\", \"unimaginative\", \"unimportant\", \"united\", \"united\", \"universal\", \"unrelated\", \"unsatisfye\", \"unstoppable\", \"unstoppable\", \"unsympathetic\", \"unusually\", \"unwanted\", \"upper\", \"use\", \"use\", \"use\", \"use\", \"use\", \"use\", \"useless\", \"vacation\", \"vacation\", \"vader\", \"vagabond\", \"val\", \"valance\", \"vampire\", \"van\", \"van\", \"van\", \"van\", \"van\", \"van\", \"van\", \"vanilla\", \"vapid\", \"varun\", \"vengeance\", \"vengeance\", \"veronica\", \"version\", \"version\", \"version\", \"version\", \"version\", \"version\", \"version\", \"viable\", \"victim\", \"victim\", \"victim\", \"video\", \"video\", \"video\", \"video\", \"video\", \"video\", \"vietnam\", \"vietnam\", \"viewer\", \"viewer\", \"viewer\", \"viewer\", \"viewer\", \"viggo\", \"vignette\", \"vile\", \"vile\", \"villain\", \"villain\", \"villain\", \"villain\", \"villain\", \"villain\", \"vin\", \"vince\", \"vincent\", \"visual\", \"visual\", \"visual\", \"visual\", \"visual\", \"visual\", \"vladimir\", \"voight\", \"von\", \"voyage\", \"vulgar\", \"wahlberg\", \"walken\", \"walker\", \"wallace\", \"wallach\", \"walle\", \"wan\", \"wan\", \"want\", \"want\", \"want\", \"want\", \"want\", \"want\", \"war\", \"war\", \"war\", \"war\", \"war\", \"war\", \"warming\", \"warrior\", \"warrior\", \"warrior\", \"wartime\", \"waste\", \"wasteland\", \"watch\", \"watch\", \"watch\", \"watch\", \"watch\", \"watch\", \"watch\", \"watt\", \"wave\", \"way\", \"way\", \"way\", \"way\", \"way\", \"way\", \"wear\", \"wear\", \"wear\", \"weaver\", \"well\", \"well\", \"well\", \"well\", \"well\", \"well\", \"well\", \"welldone\", \"welle\", \"wes\", \"west\", \"western\", \"western\", \"western\", \"western\", \"whistle\", \"wicked\", \"widow\", \"widow\", \"wilder\", \"willard\", \"willis\", \"wilson\", \"wilson\", \"wine\", \"wink\", \"wolf\", \"wolf\", \"woman\", \"woman\", \"woman\", \"woman\", \"woman\", \"woman\", \"wonderful\", \"wonderful\", \"wonderful\", \"wonderful\", \"wooden\", \"work\", \"work\", \"work\", \"work\", \"work\", \"work\", \"work\", \"world\", \"world\", \"world\", \"world\", \"world\", \"world\", \"world\", \"world\", \"worth\", \"worth\", \"worth\", \"worth\", \"worth\", \"worth\", \"worth\", \"write\", \"write\", \"write\", \"write\", \"write\", \"write\", \"write\", \"ww2\", \"ww2\", \"wwii\", \"year\", \"year\", \"year\", \"year\", \"year\", \"year\", \"year\", \"yesterday\", \"yoda\", \"young\", \"young\", \"young\", \"young\", \"young\", \"young\", \"z\", \"zack\", \"zambia\", \"zambian\", \"zemeckis\", \"zombie\", \"zombie\", \"zulu\"]}, \"R\": 30, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [9, 20, 22, 28, 13, 2, 12, 25, 6, 23, 14, 21, 18, 7, 26, 11, 24, 17, 3, 1, 16, 27, 8, 10, 5, 19, 29, 4, 30, 15]};\n",
              "\n",
              "function LDAvis_load_lib(url, callback){\n",
              "  var s = document.createElement('script');\n",
              "  s.src = url;\n",
              "  s.async = true;\n",
              "  s.onreadystatechange = s.onload = callback;\n",
              "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
              "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
              "}\n",
              "\n",
              "if(typeof(LDAvis) !== \"undefined\"){\n",
              "   // already loaded: just create the visualization\n",
              "   !function(LDAvis){\n",
              "       new LDAvis(\"#\" + \"ldavis_el5521406547846652963392946508\", ldavis_el5521406547846652963392946508_data);\n",
              "   }(LDAvis);\n",
              "}else if(typeof define === \"function\" && define.amd){\n",
              "   // require.js is available: use it to load d3/LDAvis\n",
              "   require.config({paths: {d3: \"https://d3js.org/d3.v5\"}});\n",
              "   require([\"d3\"], function(d3){\n",
              "      window.d3 = d3;\n",
              "      LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.3.1/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
              "        new LDAvis(\"#\" + \"ldavis_el5521406547846652963392946508\", ldavis_el5521406547846652963392946508_data);\n",
              "      });\n",
              "    });\n",
              "}else{\n",
              "    // require.js not available: dynamically load d3 & LDAvis\n",
              "    LDAvis_load_lib(\"https://d3js.org/d3.v5.js\", function(){\n",
              "         LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.3.1/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
              "                 new LDAvis(\"#\" + \"ldavis_el5521406547846652963392946508\", ldavis_el5521406547846652963392946508_data);\n",
              "            })\n",
              "         });\n",
              "}\n",
              "</script>"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.4 Model 3: Word2vec vectorization using pre-trained model with 25 features \n"
      ],
      "metadata": {
        "id": "UWHRomKJqFHa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#model = api.load(\"glove-twitter-50\")\n",
        "#applying larger (size 50) model did not improve f1 score\n",
        "model_25 = api.load(\"glove-twitter-25\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YXzPxtEgi7_T",
        "outputId": "a7236388-f876-46c4-ab49-74a9cc4f0779"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 104.8/104.8MB downloaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#A function to vectorize reviews by summing W2V vectors of words\n",
        "def embedding_twitter(df_text,model):\n",
        "    tkn_reviews = [wordpunct_tokenize(rev.lower()) for rev in df_text] \n",
        "    vectors =[]\n",
        "    out=0\n",
        "    in_=0\n",
        "    for rev in tkn_reviews:\n",
        "      vector=np.zeros(25)\n",
        "      in_this = 0\n",
        "      for tkn in rev: \n",
        "        try:\n",
        "          in_+=1\n",
        "          in_this+=1\n",
        "          vector+=model[tkn]\n",
        "        except:\n",
        "          out+=1\n",
        "          continue\n",
        "      vectors.append(vector / max(in_this,1))\n",
        "    print('out of voc',out)\n",
        "    print('in voc',in_)\n",
        "    return vectors"
      ],
      "metadata": {
        "id": "pW8m7-Kri8B0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_reviews"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 763
        },
        "id": "fuAMhUF1i8EZ",
        "outputId": "566b51a7-a0f3-4888-9ae5-0582d30f3219"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       helpfulness_cat  \\\n",
              "0                  1.0   \n",
              "1                  1.0   \n",
              "2                  1.0   \n",
              "3                  1.0   \n",
              "4                  1.0   \n",
              "...                ...   \n",
              "10750              0.0   \n",
              "10751              0.0   \n",
              "10752              0.0   \n",
              "10753              0.0   \n",
              "10754              0.0   \n",
              "\n",
              "                                                                                                                                                                                              imdb_user_review  \\\n",
              "0      It is hard to find such delightful and adorable movies these days as \\\"The Kid\\\". It is a silent movie but so rich, winning and beautiful that you don\\u00b4t nead words. Chaplin and 6-year old Coo...   \n",
              "1      They don't make films like this faded, haunting masterpiece of silent cinema anymore.When Dracula was first put on sale for movie rights; the one of the first men to grab it was F.W.Murnau one of ...   \n",
              "2      I first viewed this movie in 1924 at age 6 yrs--probably the first movie I ever saw.  I thought it was terrific then, and after viewing it again now(2003) I still think it is an exceptionally fine...   \n",
              "3      I doubt that I'd ever seen anything resembling a \\\"complete\\\" version of METROPOLIS before, though certain of its scenes were familiar to me, if only as used and abused in such films as Diane Keat...   \n",
              "4      I was shocked to find myself riveted to this movie. This is without a doubt the best sci-fi movie I've ever seen! Let me explain my position. We have all seen modern sci-fi movies, and argued over...   \n",
              "...                                                                                                                                                                                                        ...   \n",
              "10750  The makers of this movie really touched a sore spot in the grumpy old mans mind, namely the semioffficiaal corrupt system of professional legal guardianship in the united states of america, wher t...   \n",
              "10751  I Care A Lot is an exhilarating black comedy with a head-turning performance from Rosamund Pike. If I had to, I would pay 10 dollars for a ticket to see this in the theaters. Also, expect the unex...   \n",
              "10752                         Really loved this. This film is masterful in the way it makes you feel about all characters and the message about our westernised systems with plenty of laugh out loud moments.   \n",
              "10753                                                   The story, direction and acting across the board are superb. BUT Peter Dinklage is THE absolute star here IMHO.\\nLoved every minute, scene and nuance.   \n",
              "10754  This movie ruled! It had such a unique premise. The acting was awesome. Rosemund Pike channeled her inner Jordan Belforte throughout the whole movie. Dinklage was amazing. It's dark and funny. Hig...   \n",
              "\n",
              "                                                                                                                                                                                                 reviews_clean  \n",
              "0      it is hard to find such delightful and adorable movies these days as the kid it is a silent movie but so rich winning and beautiful that you do not nead words chaplin and 6year old coogan are poss...  \n",
              "1      they do not make films like this faded haunting masterpiece of silent cinema anymorewhen dracula was first put on sale for movie rights the one of the first men to grab it was fwmurnau one of the ...  \n",
              "2      i first viewed this movie in 1924 at age 6 yrsprobably the first movie i ever saw i thought it was terrific then and after viewing it again now2003 i still think it is an exceptionally fine movie ...  \n",
              "3      i doubt that i would ever seen anything resembling a complete version of metropolis before though certain of its scenes were familiar to me if only as used and abused in such films as diane keaton...  \n",
              "4      i was shocked to find myself riveted to this movie this is without a doubt the best scifi movie i have ever seen let me explain my position we have all seen modern scifi movies and argued over whi...  \n",
              "...                                                                                                                                                                                                        ...  \n",
              "10750  the makers of this movie really touched a sore spot in the grumpy old mans mind namely the semioffficiaal corrupt system of professional legal guardianship in the united states of america wher the...  \n",
              "10751  i care a lot is an exhilarating black comedy with a headturning performance from rosamund pike if i had to i would pay 10 dollars for a ticket to see this in the theaters also expect the unexpecte...  \n",
              "10752                           really loved this this film is masterful in the way it makes you feel about all characters and the message about our westernised systems with plenty of laugh out loud moments  \n",
              "10753                                                         the story direction and acting across the board are superb but peter dinklage is the absolute star here imho loved every minute scene and nuance  \n",
              "10754  this movie ruled it had such a unique premise the acting was awesome rosemund pike channeled her inner jordan belforte throughout the whole movie dinklage was amazing it is dark and funny highly r...  \n",
              "\n",
              "[10755 rows x 3 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-bc130643-36f9-4e59-9c89-2fe26918c94b\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>helpfulness_cat</th>\n",
              "      <th>imdb_user_review</th>\n",
              "      <th>reviews_clean</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.0</td>\n",
              "      <td>It is hard to find such delightful and adorable movies these days as \\\"The Kid\\\". It is a silent movie but so rich, winning and beautiful that you don\\u00b4t nead words. Chaplin and 6-year old Coo...</td>\n",
              "      <td>it is hard to find such delightful and adorable movies these days as the kid it is a silent movie but so rich winning and beautiful that you do not nead words chaplin and 6year old coogan are poss...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.0</td>\n",
              "      <td>They don't make films like this faded, haunting masterpiece of silent cinema anymore.When Dracula was first put on sale for movie rights; the one of the first men to grab it was F.W.Murnau one of ...</td>\n",
              "      <td>they do not make films like this faded haunting masterpiece of silent cinema anymorewhen dracula was first put on sale for movie rights the one of the first men to grab it was fwmurnau one of the ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.0</td>\n",
              "      <td>I first viewed this movie in 1924 at age 6 yrs--probably the first movie I ever saw.  I thought it was terrific then, and after viewing it again now(2003) I still think it is an exceptionally fine...</td>\n",
              "      <td>i first viewed this movie in 1924 at age 6 yrsprobably the first movie i ever saw i thought it was terrific then and after viewing it again now2003 i still think it is an exceptionally fine movie ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.0</td>\n",
              "      <td>I doubt that I'd ever seen anything resembling a \\\"complete\\\" version of METROPOLIS before, though certain of its scenes were familiar to me, if only as used and abused in such films as Diane Keat...</td>\n",
              "      <td>i doubt that i would ever seen anything resembling a complete version of metropolis before though certain of its scenes were familiar to me if only as used and abused in such films as diane keaton...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.0</td>\n",
              "      <td>I was shocked to find myself riveted to this movie. This is without a doubt the best sci-fi movie I've ever seen! Let me explain my position. We have all seen modern sci-fi movies, and argued over...</td>\n",
              "      <td>i was shocked to find myself riveted to this movie this is without a doubt the best scifi movie i have ever seen let me explain my position we have all seen modern scifi movies and argued over whi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10750</th>\n",
              "      <td>0.0</td>\n",
              "      <td>The makers of this movie really touched a sore spot in the grumpy old mans mind, namely the semioffficiaal corrupt system of professional legal guardianship in the united states of america, wher t...</td>\n",
              "      <td>the makers of this movie really touched a sore spot in the grumpy old mans mind namely the semioffficiaal corrupt system of professional legal guardianship in the united states of america wher the...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10751</th>\n",
              "      <td>0.0</td>\n",
              "      <td>I Care A Lot is an exhilarating black comedy with a head-turning performance from Rosamund Pike. If I had to, I would pay 10 dollars for a ticket to see this in the theaters. Also, expect the unex...</td>\n",
              "      <td>i care a lot is an exhilarating black comedy with a headturning performance from rosamund pike if i had to i would pay 10 dollars for a ticket to see this in the theaters also expect the unexpecte...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10752</th>\n",
              "      <td>0.0</td>\n",
              "      <td>Really loved this. This film is masterful in the way it makes you feel about all characters and the message about our westernised systems with plenty of laugh out loud moments.</td>\n",
              "      <td>really loved this this film is masterful in the way it makes you feel about all characters and the message about our westernised systems with plenty of laugh out loud moments</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10753</th>\n",
              "      <td>0.0</td>\n",
              "      <td>The story, direction and acting across the board are superb. BUT Peter Dinklage is THE absolute star here IMHO.\\nLoved every minute, scene and nuance.</td>\n",
              "      <td>the story direction and acting across the board are superb but peter dinklage is the absolute star here imho loved every minute scene and nuance</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10754</th>\n",
              "      <td>0.0</td>\n",
              "      <td>This movie ruled! It had such a unique premise. The acting was awesome. Rosemund Pike channeled her inner Jordan Belforte throughout the whole movie. Dinklage was amazing. It's dark and funny. Hig...</td>\n",
              "      <td>this movie ruled it had such a unique premise the acting was awesome rosemund pike channeled her inner jordan belforte throughout the whole movie dinklage was amazing it is dark and funny highly r...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10755 rows × 3 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-bc130643-36f9-4e59-9c89-2fe26918c94b')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-bc130643-36f9-4e59-9c89-2fe26918c94b button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-bc130643-36f9-4e59-9c89-2fe26918c94b');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Split to test and training to  \n",
        "#nltk vectorication\n",
        "x=df_reviews.reviews_clean\n",
        "y=df_reviews.helpfulness_cat\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, \n",
        "                                                    y, stratify =y,\n",
        "                                                    test_size=0.20, \n",
        "                                                    random_state=0)          "
      ],
      "metadata": {
        "id": "LdjTcCCqi8H0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Check if the stratification worked fine (divided each subset with proportional number of classes)\n",
        "print(np.average(y_test))\n",
        "print (np.average(y_train))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4uI_MojgceeA",
        "outputId": "0a6787aa-02ee-4470-9e3b-faab928cc01f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.7638307763830776\n",
            "0.7637145513714552\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_vecs=embedding_twitter(x_test,model_25)\n",
        "train_vecs=embedding_twitter(x_train,model_25)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EPza_S6gaC3_",
        "outputId": "57cb3a2a-9855-4e80-f4b5-bf79a9175224"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "out of voc 9213\n",
            "in voc 380985\n",
            "out of voc 36472\n",
            "in voc 1526928\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "logisticRegr = LogisticRegression()\n",
        "logisticRegr.fit(train_vecs, y_train)\n",
        "y_pred=logisticRegr.predict(test_vecs)\n",
        "score = f1_score(y_pred, y_test)\n",
        "score"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qwmWkAPeXDhm",
        "outputId": "f3ddbac8-13b6-4cd9-8f16-9a52ca5834ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8613490364025694"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Check overfitting\n",
        "print(logisticRegr.score(train_vecs, y_train))\n",
        "print(logisticRegr.score(test_vecs, y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mHQf7RzgXDku",
        "outputId": "6b4f1690-989b-4ed8-a782-3b166060480c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.7645281264528127\n",
            "0.7591817759181776\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Deep learning models"
      ],
      "metadata": {
        "id": "ZZf3Se-CrAJh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.1 KERAS"
      ],
      "metadata": {
        "id": "HrW7pgOKrEfi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DL Model 1: TF-DF vectorization approach as an input for our Keras NN"
      ],
      "metadata": {
        "id": "2LvDk6FrrI5r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#create train,test,val from tokenized nltk df\n",
        "y=df_reviews.helpfulness_cat\n",
        "x=x_nltk\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, \n",
        "                                                    y, \n",
        "                                                    stratify=y,\n",
        "                                                    test_size=0.20, \n",
        "                                                    random_state=0)"
      ],
      "metadata": {
        "id": "w-xUH1ngrMC5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train, x_val, y_train, y_val = train_test_split(x_train, \n",
        "                                                    y_train, \n",
        "                                                    stratify=y_train,\n",
        "                                                    test_size=0.10, \n",
        "                                                    random_state=0) "
      ],
      "metadata": {
        "id": "bUngyZc1rMFe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#retreive vectorized models\n",
        "tf_idf=TfidfVectorizer(max_features=100)\n",
        "X_train=tf_idf.fit_transform(x_train)\n",
        "X_test=tf_idf.transform(x_test)\n",
        "X_val=tf_idf.transform(x_val)"
      ],
      "metadata": {
        "id": "PW98utOErMIF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#try a simple 4 layers NN\n",
        "model = Sequential()\n",
        "\n",
        "model.add(layers.Dense(input_shape = X_train.shape[1:],units=10, activation='relu'))\n",
        "model.add(layers.Dense(10, activation='relu'))\n",
        "model.add(layers.Dense(10, activation='relu'))\n",
        "\n",
        "model.add(layers.Dense(1, activation='sigmoid'))\n",
        "opt = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "model.compile(optimizer=opt,\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PH1w8kkKrMKq",
        "outputId": "47d7dd84-76e0-4a6c-bf53-236cba009d4c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense (Dense)               (None, 10)                1010      \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 10)                110       \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 10)                110       \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 1)                 11        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,241\n",
            "Trainable params: 1,241\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(X_train.toarray(), y_train,\n",
        "                    epochs=20,\n",
        "                    verbose=True,\n",
        "                    validation_data=(X_val.toarray(), y_val)\n",
        "                    ,batch_size=16)\n",
        "                    \n",
        "loss, accuracy = model.evaluate(X_train.toarray(), y_train, verbose=False)\n",
        "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
        "loss, accuracy = model.evaluate(X_test.toarray(), y_test, verbose=False)\n",
        "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MSM2-j54rMNj",
        "outputId": "6633718d-f4e9-4286-e2b2-2f660413ac4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "484/484 [==============================] - 6s 4ms/step - loss: 0.5575 - accuracy: 0.7575 - val_loss: 0.5400 - val_accuracy: 0.7642\n",
            "Epoch 2/20\n",
            "484/484 [==============================] - 2s 3ms/step - loss: 0.5233 - accuracy: 0.7637 - val_loss: 0.5347 - val_accuracy: 0.7642\n",
            "Epoch 3/20\n",
            "484/484 [==============================] - 2s 3ms/step - loss: 0.5146 - accuracy: 0.7635 - val_loss: 0.5338 - val_accuracy: 0.7607\n",
            "Epoch 4/20\n",
            "484/484 [==============================] - 2s 3ms/step - loss: 0.5100 - accuracy: 0.7662 - val_loss: 0.5324 - val_accuracy: 0.7596\n",
            "Epoch 5/20\n",
            "484/484 [==============================] - 2s 3ms/step - loss: 0.5077 - accuracy: 0.7687 - val_loss: 0.5328 - val_accuracy: 0.7491\n",
            "Epoch 6/20\n",
            "484/484 [==============================] - 1s 3ms/step - loss: 0.5051 - accuracy: 0.7675 - val_loss: 0.5322 - val_accuracy: 0.7468\n",
            "Epoch 7/20\n",
            "484/484 [==============================] - 2s 3ms/step - loss: 0.5029 - accuracy: 0.7706 - val_loss: 0.5373 - val_accuracy: 0.7573\n",
            "Epoch 8/20\n",
            "484/484 [==============================] - 2s 3ms/step - loss: 0.5008 - accuracy: 0.7732 - val_loss: 0.5330 - val_accuracy: 0.7538\n",
            "Epoch 9/20\n",
            "484/484 [==============================] - 2s 3ms/step - loss: 0.4977 - accuracy: 0.7758 - val_loss: 0.5341 - val_accuracy: 0.7526\n",
            "Epoch 10/20\n",
            "484/484 [==============================] - 1s 3ms/step - loss: 0.4957 - accuracy: 0.7764 - val_loss: 0.5338 - val_accuracy: 0.7515\n",
            "Epoch 11/20\n",
            "484/484 [==============================] - 1s 3ms/step - loss: 0.4926 - accuracy: 0.7781 - val_loss: 0.5412 - val_accuracy: 0.7515\n",
            "Epoch 12/20\n",
            "484/484 [==============================] - 1s 3ms/step - loss: 0.4893 - accuracy: 0.7807 - val_loss: 0.5394 - val_accuracy: 0.7410\n",
            "Epoch 13/20\n",
            "484/484 [==============================] - 2s 3ms/step - loss: 0.4868 - accuracy: 0.7806 - val_loss: 0.5385 - val_accuracy: 0.7387\n",
            "Epoch 14/20\n",
            "484/484 [==============================] - 2s 3ms/step - loss: 0.4835 - accuracy: 0.7839 - val_loss: 0.5390 - val_accuracy: 0.7433\n",
            "Epoch 15/20\n",
            "484/484 [==============================] - 2s 3ms/step - loss: 0.4807 - accuracy: 0.7860 - val_loss: 0.5448 - val_accuracy: 0.7445\n",
            "Epoch 16/20\n",
            "484/484 [==============================] - 2s 3ms/step - loss: 0.4777 - accuracy: 0.7877 - val_loss: 0.5466 - val_accuracy: 0.7433\n",
            "Epoch 17/20\n",
            "484/484 [==============================] - 2s 3ms/step - loss: 0.4754 - accuracy: 0.7872 - val_loss: 0.5472 - val_accuracy: 0.7491\n",
            "Epoch 18/20\n",
            "484/484 [==============================] - 1s 3ms/step - loss: 0.4717 - accuracy: 0.7912 - val_loss: 0.5521 - val_accuracy: 0.7468\n",
            "Epoch 19/20\n",
            "484/484 [==============================] - 1s 3ms/step - loss: 0.4696 - accuracy: 0.7937 - val_loss: 0.5548 - val_accuracy: 0.7480\n",
            "Epoch 20/20\n",
            "484/484 [==============================] - 1s 3ms/step - loss: 0.4664 - accuracy: 0.7930 - val_loss: 0.5545 - val_accuracy: 0.7445\n",
            "Training Accuracy: 0.7990\n",
            "Testing Accuracy:  0.7480\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_predicted = model.predict(X_test.toarray())\n",
        "y_predicted_labels=np.where(y_predicted > 0.5, 1,0)\n",
        "score = f1_score(y_predicted_labels, y_test)\n",
        "score"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9JpSyBuyffwh",
        "outputId": "adb54ac6-36cc-42c4-b7cd-01b2c8730bf0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8501106194690267"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DL Model 2: Pre-trained Word2vec model (gensim twitter 25)  as an input with LSTM layer"
      ],
      "metadata": {
        "id": "GBPi9ukosKTi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Some crafted functions to apply pre-trained Gensim Twitter 25 W2V model"
      ],
      "metadata": {
        "id": "DmreQywisT9T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_train_vocabulary(train_docs,model):\n",
        "  voc={}\n",
        "  idx_=0\n",
        "  for doc in train_docs:\n",
        "    for word in doc:\n",
        "      if word not in voc and word in model.vocab.keys():\n",
        "        voc[word]=idx_\n",
        "        idx_+=1\n",
        "  return voc"
      ],
      "metadata": {
        "id": "A0ycZeNysD5g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def text_to_seq(docs,vocabulary):\n",
        "  seq_list=[]\n",
        "  for sent in docs:\n",
        "    idx=[]\n",
        "    for word in sent:\n",
        "      if word in vocabulary:\n",
        "        idx.append(vocabulary[word])\n",
        "      else:\n",
        "        continue\n",
        "    seq_list.append(idx)\n",
        "  return seq_list"
      ],
      "metadata": {
        "id": "tZsqFPpIsD8I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Split the train dataset, apply custom functions and perform padding"
      ],
      "metadata": {
        "id": "R-GrRnpzkkQE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y=df_reviews.helpfulness_cat\n",
        "x=df_reviews['imdb_user_review']\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, \n",
        "                                                    y, \n",
        "                                                    stratify=y,\n",
        "                                                    test_size=0.20, \n",
        "                                                    random_state=0)      \n",
        "x_train, x_val, y_train, y_val = train_test_split(x_train, \n",
        "                                                    y_train, \n",
        "                                                    stratify=y_train,\n",
        "                                                    test_size=0.10, \n",
        "                                                    random_state=0) "
      ],
      "metadata": {
        "id": "f0WfAKmekQCe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train=tokenize_wordpunkt(x_train,string=False)\n",
        "X_test=tokenize_wordpunkt(x_test,string=False)\n",
        "X_val=tokenize_wordpunkt(x_val,string=False)"
      ],
      "metadata": {
        "id": "6N9YQrqnkR__"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#get vocabulary\n",
        "train_voc=get_train_vocabulary(X_train,model_25)"
      ],
      "metadata": {
        "id": "e-d_GMu4kT9O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#aply custom function to get sequences of numbers from sentences\n",
        "X_train_seq=text_to_seq(X_train,train_voc)\n",
        "X_test=text_to_seq(X_test,train_voc)\n",
        "X_val=text_to_seq(X_val,train_voc)"
      ],
      "metadata": {
        "id": "9cav3dKAkViZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "voc_size = len(train_voc)"
      ],
      "metadata": {
        "id": "ibui9seikXaR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Perform padding\n",
        "max_len=300\n",
        "padded_train = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "    text_to_seq(X_train,train_voc), padding=\"post\",maxlen=max_len, value=voc_size)\n",
        "X_test = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "    text_to_seq(X_test,train_voc), padding=\"post\",maxlen=max_len, value=voc_size)\n",
        "X_val = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "    text_to_seq(X_val,train_voc), padding=\"post\",maxlen=max_len, value=voc_size)"
      ],
      "metadata": {
        "id": "v9TqcIvpkZIV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating weights matrix (matching sequences with vector representations from pre-trained model)\n",
        "EMBEDDING_DIM=25\n",
        "embedding_matrix = np.zeros((len(train_voc) + 1, EMBEDDING_DIM))\n",
        "for word, i in train_voc.items():\n",
        "    embedding_vector = model_25[word]\n",
        "    if embedding_vector is not None:\n",
        "        # words not found in embedding index will be all-zeros.\n",
        "        embedding_matrix[i] = embedding_vector"
      ],
      "metadata": {
        "id": "X6eA-_chkdBC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Training the model and assessing performance"
      ],
      "metadata": {
        "id": "b44ThqdziGom"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(layers.Embedding(input_dim=len(train_voc)+1, \n",
        "                           output_dim=EMBEDDING_DIM, \n",
        "                           weights=[embedding_matrix]\n",
        "                           ,trainable=False))\n",
        "model.add(layers.Masking(mask_value=voc_size, input_shape=(max_len,)))\n",
        "model.add(layers.LSTM(24))\n",
        "#model.add(layers.Dense(units=vocab_size))\n",
        "model.add(layers.Dense(1, activation='sigmoid'))\n",
        "opt = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "model.compile(optimizer=opt,\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "_172-9yxk8PO",
        "outputId": "fcf5b4da-19dc-4375-ec8e-4afe400126bd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, None, 25)          826375    \n",
            "                                                                 \n",
            " masking (Masking)           (None, None, 25)          0         \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 24)                4800      \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 1)                 25        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 831,200\n",
            "Trainable params: 4,825\n",
            "Non-trainable params: 826,375\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(padded_train, y_train,\n",
        "                    epochs=20,\n",
        "                    verbose=True,\n",
        "                    validation_data=(X_val, y_val)\n",
        "                    ,batch_size=128)"
      ],
      "metadata": {
        "id": "Gh95BqlxlDcE",
        "outputId": "db2f4db2-fe4d-407a-8895-29e42c69d639",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "61/61 [==============================] - 8s 40ms/step - loss: 0.6012 - accuracy: 0.7488 - val_loss: 0.5501 - val_accuracy: 0.7642\n",
            "Epoch 2/20\n",
            "61/61 [==============================] - 1s 15ms/step - loss: 0.5434 - accuracy: 0.7637 - val_loss: 0.5478 - val_accuracy: 0.7642\n",
            "Epoch 3/20\n",
            "61/61 [==============================] - 1s 16ms/step - loss: 0.5429 - accuracy: 0.7637 - val_loss: 0.5464 - val_accuracy: 0.7642\n",
            "Epoch 4/20\n",
            "61/61 [==============================] - 1s 15ms/step - loss: 0.5425 - accuracy: 0.7637 - val_loss: 0.5462 - val_accuracy: 0.7642\n",
            "Epoch 5/20\n",
            "61/61 [==============================] - 1s 15ms/step - loss: 0.5418 - accuracy: 0.7637 - val_loss: 0.5464 - val_accuracy: 0.7642\n",
            "Epoch 6/20\n",
            "61/61 [==============================] - 1s 16ms/step - loss: 0.5413 - accuracy: 0.7637 - val_loss: 0.5464 - val_accuracy: 0.7642\n",
            "Epoch 7/20\n",
            "61/61 [==============================] - 1s 15ms/step - loss: 0.5405 - accuracy: 0.7637 - val_loss: 0.5492 - val_accuracy: 0.7642\n",
            "Epoch 8/20\n",
            "61/61 [==============================] - 1s 15ms/step - loss: 0.5405 - accuracy: 0.7637 - val_loss: 0.5475 - val_accuracy: 0.7642\n",
            "Epoch 9/20\n",
            "61/61 [==============================] - 1s 15ms/step - loss: 0.5399 - accuracy: 0.7637 - val_loss: 0.5469 - val_accuracy: 0.7642\n",
            "Epoch 10/20\n",
            "61/61 [==============================] - 1s 15ms/step - loss: 0.5391 - accuracy: 0.7637 - val_loss: 0.5462 - val_accuracy: 0.7642\n",
            "Epoch 11/20\n",
            "61/61 [==============================] - 1s 15ms/step - loss: 0.5384 - accuracy: 0.7637 - val_loss: 0.5464 - val_accuracy: 0.7642\n",
            "Epoch 12/20\n",
            "61/61 [==============================] - 1s 15ms/step - loss: 0.5381 - accuracy: 0.7637 - val_loss: 0.5478 - val_accuracy: 0.7642\n",
            "Epoch 13/20\n",
            "61/61 [==============================] - 1s 15ms/step - loss: 0.5377 - accuracy: 0.7638 - val_loss: 0.5475 - val_accuracy: 0.7642\n",
            "Epoch 14/20\n",
            "61/61 [==============================] - 1s 15ms/step - loss: 0.5367 - accuracy: 0.7638 - val_loss: 0.5463 - val_accuracy: 0.7642\n",
            "Epoch 15/20\n",
            "61/61 [==============================] - 1s 15ms/step - loss: 0.5374 - accuracy: 0.7639 - val_loss: 0.5489 - val_accuracy: 0.7642\n",
            "Epoch 16/20\n",
            "61/61 [==============================] - 1s 15ms/step - loss: 0.5356 - accuracy: 0.7640 - val_loss: 0.5484 - val_accuracy: 0.7642\n",
            "Epoch 17/20\n",
            "61/61 [==============================] - 1s 15ms/step - loss: 0.5348 - accuracy: 0.7644 - val_loss: 0.5485 - val_accuracy: 0.7642\n",
            "Epoch 18/20\n",
            "61/61 [==============================] - 1s 15ms/step - loss: 0.5344 - accuracy: 0.7646 - val_loss: 0.5475 - val_accuracy: 0.7642\n",
            "Epoch 19/20\n",
            "61/61 [==============================] - 1s 15ms/step - loss: 0.5344 - accuracy: 0.7655 - val_loss: 0.5462 - val_accuracy: 0.7642\n",
            "Epoch 20/20\n",
            "61/61 [==============================] - 1s 15ms/step - loss: 0.5326 - accuracy: 0.7653 - val_loss: 0.5466 - val_accuracy: 0.7642\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss, accuracy = model.evaluate(X_test, y_test, verbose=False)\n",
        "accuracy"
      ],
      "metadata": {
        "id": "PsOKa2NDlFjd",
        "outputId": "671eea24-c29b-4f70-cd83-950f0d448bc7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7638307809829712"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_predicted = model.predict(X_test)\n",
        "y_predicted_labels=np.where(y_predicted > 0.5, 1,0)\n",
        "score = f1_score(y_predicted_labels, y_test)\n",
        "score"
      ],
      "metadata": {
        "id": "hGpLxO6WlP3y",
        "outputId": "19b08d81-a3dd-4c92-ba22-33de192f87ac",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8661043753294676"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DL Model 3: Applying BERT layers"
      ],
      "metadata": {
        "id": "l7NriOvbtiKc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y=df_reviews.helpfulness_cat\n",
        "x=df_reviews.reviews_clean\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, \n",
        "                                                    y, \n",
        "                                                    stratify=y,\n",
        "                                                    test_size=0.20, \n",
        "                                                    random_state=0)      \n",
        "x_train, x_val, y_train, y_val = train_test_split(x_train, \n",
        "                                                    y_train, \n",
        "                                                    stratify=y_train,\n",
        "                                                    test_size=0.10, \n",
        "                                                    random_state=0) "
      ],
      "metadata": {
        "id": "Y2Epwna2sEMF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow_hub as hub\n",
        "#it may take some while to download\n",
        "!pip install \"tensorflow-text==2.8.*\"\n",
        "import tensorflow_text as text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DrNoNFuqsEO9",
        "outputId": "744dd315-1bbd-4bb8-afed-db0f18dc9258"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorflow-text==2.8.*\n",
            "  Downloading tensorflow_text-2.8.2-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (4.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.9 MB 3.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorflow<2.9,>=2.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-text==2.8.*) (2.8.2+zzzcolab20220527125636)\n",
            "Requirement already satisfied: tensorflow-hub>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-text==2.8.*) (0.12.0)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (1.21.6)\n",
            "Requirement already satisfied: keras<2.9,>=2.8.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (2.8.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (1.1.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (3.3.0)\n",
            "Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (2.0)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (3.17.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (57.4.0)\n",
            "Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (0.5.3)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (1.6.3)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (0.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (1.15.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (0.26.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (2.8.0)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (14.0.1)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (3.1.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (1.1.2)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (1.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (4.1.1)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (1.14.1)\n",
            "Requirement already satisfied: tensorboard<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (2.8.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (1.47.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (0.37.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (1.5.2)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (2.23.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (3.3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (0.6.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (1.8.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (0.4.6)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (1.35.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (1.0.1)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (4.8)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (4.12.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (3.8.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (0.4.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (2022.6.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (3.0.4)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (3.2.0)\n",
            "Installing collected packages: tensorflow-text\n",
            "Successfully installed tensorflow-text-2.8.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#retrieve BERT layers\n",
        "bert_preprocess = hub.KerasLayer('https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3')\n",
        "bert_encoder = hub.KerasLayer('https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/4')"
      ],
      "metadata": {
        "id": "2sq4HWc_sERX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#lets have a look at how the pre-processing layer works\n",
        "check_preprocess=bert_preprocess([x_train[0]])\n",
        "check_preprocess['input_word_ids']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d_DKLzYPsETr",
        "outputId": "7f3fa6b9-ddd6-4ebe-8246-833196e3255a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1, 128), dtype=int32, numpy=\n",
              "array([[  101,  2009,  2003,  2524,  2000,  2424,  2107, 26380,  1998,\n",
              "        23677,  5691,  2122,  2420,  2004,  1996,  4845,  2009,  2003,\n",
              "         1037,  4333,  3185,  2021,  2061,  4138,  3045,  1998,  3376,\n",
              "         2008,  2017,  2079,  2025, 11265,  4215,  2616, 23331,  1998,\n",
              "         1020, 29100,  2214,  2522, 21131,  2024,  4298,  1996,  2087,\n",
              "        11951,  3940,  1045,  2031,  2412,  2464,  1999,  2026,  2166,\n",
              "         1996,  2143, 10514,  9468, 10285,  2000,  2022, 19957,  2422,\n",
              "         1998,  2440,  1997,  6569,  2021,  2036, 24783,  6517,  1998,\n",
              "        23069,  1045,  2467,  2131,  2026,  2159,  2440,  2007,  4000,\n",
              "         1997,  6517,  2094,  2791,  1998,  8404,  1998,  1045,  2428,\n",
              "         2196,  5390,  1999,  5691,  1998,  1996,  2189, 12762,  3432,\n",
              "         7436, 20414,  2884,  1996,  3185,  2003, 13026,  2440,  1997,\n",
              "         3110,  7244,  2135,  6057,  5312,  2009,  2003,  5621,  1037,\n",
              "        17743,  4760,  2129,  9313, 10904,  2711,  4918, 23331,  2001,\n",
              "         6195,   102]], dtype=int32)>"
            ]
          },
          "metadata": {},
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#check how encoder works\n",
        "check_encoder=bert_encoder(check_preprocess)\n",
        "check_encoder['sequence_output']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LoJtGAsssEWU",
        "outputId": "1d967c5d-85da-4198-b0fb-0eb7a5e3f6ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1, 128, 768), dtype=float32, numpy=\n",
              "array([[[-0.09483425,  0.02509628,  0.00794085, ..., -0.49546272,\n",
              "          0.45173013,  0.20836335],\n",
              "        [ 0.00378387,  0.20399034, -0.28273895, ...,  0.13478795,\n",
              "          1.3335385 ,  0.38007602],\n",
              "        [-0.00591481,  0.12706122,  0.15976521, ..., -0.71001875,\n",
              "          0.41201642,  0.04064867],\n",
              "        ...,\n",
              "        [ 0.45924836,  0.13019066, -0.25051722, ..., -0.1414549 ,\n",
              "          0.21332017, -0.20304096],\n",
              "        [-0.26811668,  0.00673912, -0.02975496, ...,  0.21424296,\n",
              "          0.47883397, -0.2404381 ],\n",
              "        [ 0.4136543 ,  0.37176904,  0.14373316, ..., -0.16149329,\n",
              "         -0.3951578 , -0.08344617]]], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Bert layers\n",
        "text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\n",
        "preprocessed_text = bert_preprocess(text_input)\n",
        "outputs = bert_encoder(preprocessed_text)\n",
        "# Neural network layers\n",
        "l = tf.keras.layers.Dropout(0.1, name=\"dropout\")(outputs['pooled_output'])\n",
        "l = tf.keras.layers.Dense(1, activation='sigmoid', name=\"output\")(l)\n",
        "# Use inputs and outputs to construct a final model\n",
        "model = tf.keras.Model(inputs=[text_input], outputs = [l])"
      ],
      "metadata": {
        "id": "epyG-orHsEY2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8_d_JyF5sEb_",
        "outputId": "435dce9d-cfab-4174-8396-82cc962a7ad9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " text (InputLayer)              [(None,)]            0           []                               \n",
            "                                                                                                  \n",
            " keras_layer (KerasLayer)       {'input_word_ids':   0           ['text[0][0]']                   \n",
            "                                (None, 128),                                                      \n",
            "                                 'input_type_ids':                                                \n",
            "                                (None, 128),                                                      \n",
            "                                 'input_mask': (Non                                               \n",
            "                                e, 128)}                                                          \n",
            "                                                                                                  \n",
            " keras_layer_1 (KerasLayer)     {'sequence_output':  109482241   ['keras_layer[0][0]',            \n",
            "                                 (None, 128, 768),                'keras_layer[0][1]',            \n",
            "                                 'default': (None,                'keras_layer[0][2]']            \n",
            "                                768),                                                             \n",
            "                                 'pooled_output': (                                               \n",
            "                                None, 768),                                                       \n",
            "                                 'encoder_outputs':                                               \n",
            "                                 [(None, 128, 768),                                               \n",
            "                                 (None, 128, 768),                                                \n",
            "                                 (None, 128, 768),                                                \n",
            "                                 (None, 128, 768),                                                \n",
            "                                 (None, 128, 768),                                                \n",
            "                                 (None, 128, 768),                                                \n",
            "                                 (None, 128, 768),                                                \n",
            "                                 (None, 128, 768),                                                \n",
            "                                 (None, 128, 768),                                                \n",
            "                                 (None, 128, 768),                                                \n",
            "                                 (None, 128, 768),                                                \n",
            "                                 (None, 128, 768)]}                                               \n",
            "                                                                                                  \n",
            " dropout (Dropout)              (None, 768)          0           ['keras_layer_1[0][13]']         \n",
            "                                                                                                  \n",
            " output (Dense)                 (None, 1)            769         ['dropout[0][0]']                \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 109,483,010\n",
            "Trainable params: 769\n",
            "Non-trainable params: 109,482,241\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "model.fit(np.array(x_train),\n",
        "          np.array(y_train),\n",
        "          epochs=2,\n",
        "          validation_data=(np.array(x_val), y_val),\n",
        "          batch_size = 32)\n",
        "y_predicted = model.predict(np.array(x_test))\n",
        "y_predicted_labels=np.where(y_predicted > 0.5, 1,0)\n",
        "score = f1_score(y_predicted_labels, y_test)\n",
        "score"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vLt09dnbsEej",
        "outputId": "5053d0e0-0f59-47f7-9fd5-6f95e22b688c"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            "242/242 [==============================] - 3829s 16s/step - loss: 0.5506 - accuracy: 0.7599 - val_loss: 0.5278 - val_accuracy: 0.7642\n",
            "Epoch 2/2\n",
            "242/242 [==============================] - 3818s 16s/step - loss: 0.5331 - accuracy: 0.7585 - val_loss: 0.5215 - val_accuracy: 0.7642\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8658922914466737"
            ]
          },
          "metadata": {},
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preparing the test set and getting the final prediction"
      ],
      "metadata": {
        "id": "0Pt5Wd1lBxQf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_test=pd.read_csv(test_data,encoding='utf-8',)"
      ],
      "metadata": {
        "id": "LrViUPavsEj7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Running the function\n",
        "pd.options.display.max_colwidth = 200\n",
        "df_test[\"reviews_clean\"] = df_test[\"imdb_user_review\"].apply(lambda x: unicoderemoval(x))\n",
        "df_test.head()"
      ],
      "metadata": {
        "id": "TZ-ps6MWsEmf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "outputId": "8d6a0a3f-0a22-48ae-c326-b8b03a28a8df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                        _id  \\\n",
              "0  610d01fe9a63eb113d2235ac   \n",
              "1  610d01fe9a63eb113d224536   \n",
              "2  610d01fe9a63eb113d224d14   \n",
              "3  610d01fe9a63eb113d225f17   \n",
              "4  610d01fe9a63eb113d225f1a   \n",
              "\n",
              "                                                                                                                                                                                          imdb_user_review  \\\n",
              "0  This is basically a German takeoff on a Sherlock Holmes story. Now, if told in a competent way, Sherlock Holmes-inspired tales can be entertaining, even if of secondary heritage. But this abominat...   \n",
              "1  In January of 1924, director Erich von Stroheim gathered a small group of friends to view his magnum opus, a film which would become legendary. This was GREED &amp; when the experience was over, a...   \n",
              "2  Silent movies are not for everyone. Neither are subtitles. Those brave enough to view a movie with no sound and words that are far and few between should definitely enjoy this silent masterpiece. ...   \n",
              "3  It so often happens that some films take the long way to achieve their status of classics and worthy of being studied, frame by frame, by movie lovers who believe in the power of raw performance a...   \n",
              "4  I saw this film for the very first time last week and was so tremendously captivated by it that I needed to share this rapture. The innovative camera-angles, the close-ups revealing pain and spiri...   \n",
              "\n",
              "                                                                                                                                                                                             reviews_clean  \n",
              "0  This is basically a German takeoff on a Sherlock Holmes story. Now, if told in a competent way, Sherlock Holmes-inspired tales can be entertaining, even if of secondary heritage. But this abominat...  \n",
              "1  In January of 1924, director Erich von Stroheim gathered a small group of friends to view his magnum opus, a film which would become legendary. This was GREED & when the experience was over, all t...  \n",
              "2  Silent movies are not for everyone. Neither are subtitles. Those brave enough to view a movie with no sound and words that are far and few between should definitely enjoy this silent masterpiece. ...  \n",
              "3  It so often happens that some films take the long way to achieve their status of classics and worthy of being studied, frame by frame, by movie lovers who believe in the power of raw performance a...  \n",
              "4  I saw this film for the very first time last week and was so tremendously captivated by it that I needed to share this rapture. The innovative camera-angles, the close-ups revealing pain and spiri...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-04612b13-3cf2-4c77-9f36-7856c9b011b0\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>_id</th>\n",
              "      <th>imdb_user_review</th>\n",
              "      <th>reviews_clean</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>610d01fe9a63eb113d2235ac</td>\n",
              "      <td>This is basically a German takeoff on a Sherlock Holmes story. Now, if told in a competent way, Sherlock Holmes-inspired tales can be entertaining, even if of secondary heritage. But this abominat...</td>\n",
              "      <td>This is basically a German takeoff on a Sherlock Holmes story. Now, if told in a competent way, Sherlock Holmes-inspired tales can be entertaining, even if of secondary heritage. But this abominat...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>610d01fe9a63eb113d224536</td>\n",
              "      <td>In January of 1924, director Erich von Stroheim gathered a small group of friends to view his magnum opus, a film which would become legendary. This was GREED &amp;amp; when the experience was over, a...</td>\n",
              "      <td>In January of 1924, director Erich von Stroheim gathered a small group of friends to view his magnum opus, a film which would become legendary. This was GREED &amp; when the experience was over, all t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>610d01fe9a63eb113d224d14</td>\n",
              "      <td>Silent movies are not for everyone. Neither are subtitles. Those brave enough to view a movie with no sound and words that are far and few between should definitely enjoy this silent masterpiece. ...</td>\n",
              "      <td>Silent movies are not for everyone. Neither are subtitles. Those brave enough to view a movie with no sound and words that are far and few between should definitely enjoy this silent masterpiece. ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>610d01fe9a63eb113d225f17</td>\n",
              "      <td>It so often happens that some films take the long way to achieve their status of classics and worthy of being studied, frame by frame, by movie lovers who believe in the power of raw performance a...</td>\n",
              "      <td>It so often happens that some films take the long way to achieve their status of classics and worthy of being studied, frame by frame, by movie lovers who believe in the power of raw performance a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>610d01fe9a63eb113d225f1a</td>\n",
              "      <td>I saw this film for the very first time last week and was so tremendously captivated by it that I needed to share this rapture. The innovative camera-angles, the close-ups revealing pain and spiri...</td>\n",
              "      <td>I saw this film for the very first time last week and was so tremendously captivated by it that I needed to share this rapture. The innovative camera-angles, the close-ups revealing pain and spiri...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-04612b13-3cf2-4c77-9f36-7856c9b011b0')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-04612b13-3cf2-4c77-9f36-7856c9b011b0 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-04612b13-3cf2-4c77-9f36-7856c9b011b0');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 113
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_test[\"imdb_user_review_lang\"] = df_test[\"reviews_clean\"].apply(lambda x: lng(x))\n",
        "df_test[\"imdb_user_review_lang_valn\"] = df_test[\"reviews_clean\"].apply(lambda x: lng1(x))\n",
        "pd.options.display.max_colwidth = 50\n",
        "df_test[df_test[\"imdb_user_review_lang\"]!='en']\n",
        "\n"
      ],
      "metadata": {
        "id": "2OLCCS1fKe_S",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 348
        },
        "outputId": "3df47098-c01e-4aad-8869-621231d8cb1f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                           _id  \\\n",
              "510   610d02089a63eb113d2fe99a   \n",
              "1741  610d02169a63eb113d453312   \n",
              "1766  610d02169a63eb113d456a92   \n",
              "3594  610d02219a63eb113d548bd0   \n",
              "4206  610d02249a63eb113d594b34   \n",
              "4672  610d02259a63eb113d5c2237   \n",
              "4951  610d02269a63eb113d5e1bf8   \n",
              "\n",
              "                                       imdb_user_review  \\\n",
              "510   This movie, singlehandedly, saved my marriage....   \n",
              "1741  Worst worst Comedy , NOT ONE TIME WATCH TOO , ...   \n",
              "1766  Fake movie h. Kanchana best movie h original. ...   \n",
              "3594  KAKAV JE OVO TSHRO OD USRANOG FILMA JEBO VAS H...   \n",
              "4206  Mmm mmMM mmmmmmm Mmmmmm. Mmmmmmm Mm m MMM! MMM...   \n",
              "4672  South Indian most expected movie kurukshetra D...   \n",
              "4951  Best inspired movie 2018\\n\\\"I Never fear Lose ...   \n",
              "\n",
              "                                          reviews_clean imdb_user_review_lang  \\\n",
              "510   This movie, singlehandedly, saved my marriage....                    it   \n",
              "1741  Worst worst Comedy , NOT ONE TIME WATCH TOO , ...                    nl   \n",
              "1766  Fake movie h. Kanchana best movie h original. ...                    af   \n",
              "3594  KAKAV JE OVO TSHRO OD USRANOG FILMA JEBO VAS H...                    rw   \n",
              "4206  Mmm mmMM mmmmmmm Mmmmmm. Mmmmmmm Mm m MMM! MMM...                    mt   \n",
              "4672  South Indian most expected movie kurukshetra D...                    id   \n",
              "4951  Best inspired movie 2018 \"I Never fear Lose bu...                    no   \n",
              "\n",
              "      imdb_user_review_lang_valn  \n",
              "510                 -1298.407276  \n",
              "1741                  -29.460060  \n",
              "1766                  -36.975652  \n",
              "3594                  -69.382356  \n",
              "4206                 -135.474604  \n",
              "4672                 -212.293078  \n",
              "4951                  -79.404977  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c338559c-d453-450d-9ed3-5f285e4228c9\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>_id</th>\n",
              "      <th>imdb_user_review</th>\n",
              "      <th>reviews_clean</th>\n",
              "      <th>imdb_user_review_lang</th>\n",
              "      <th>imdb_user_review_lang_valn</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>510</th>\n",
              "      <td>610d02089a63eb113d2fe99a</td>\n",
              "      <td>This movie, singlehandedly, saved my marriage....</td>\n",
              "      <td>This movie, singlehandedly, saved my marriage....</td>\n",
              "      <td>it</td>\n",
              "      <td>-1298.407276</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1741</th>\n",
              "      <td>610d02169a63eb113d453312</td>\n",
              "      <td>Worst worst Comedy , NOT ONE TIME WATCH TOO , ...</td>\n",
              "      <td>Worst worst Comedy , NOT ONE TIME WATCH TOO , ...</td>\n",
              "      <td>nl</td>\n",
              "      <td>-29.460060</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1766</th>\n",
              "      <td>610d02169a63eb113d456a92</td>\n",
              "      <td>Fake movie h. Kanchana best movie h original. ...</td>\n",
              "      <td>Fake movie h. Kanchana best movie h original. ...</td>\n",
              "      <td>af</td>\n",
              "      <td>-36.975652</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3594</th>\n",
              "      <td>610d02219a63eb113d548bd0</td>\n",
              "      <td>KAKAV JE OVO TSHRO OD USRANOG FILMA JEBO VAS H...</td>\n",
              "      <td>KAKAV JE OVO TSHRO OD USRANOG FILMA JEBO VAS H...</td>\n",
              "      <td>rw</td>\n",
              "      <td>-69.382356</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4206</th>\n",
              "      <td>610d02249a63eb113d594b34</td>\n",
              "      <td>Mmm mmMM mmmmmmm Mmmmmm. Mmmmmmm Mm m MMM! MMM...</td>\n",
              "      <td>Mmm mmMM mmmmmmm Mmmmmm. Mmmmmmm Mm m MMM! MMM...</td>\n",
              "      <td>mt</td>\n",
              "      <td>-135.474604</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4672</th>\n",
              "      <td>610d02259a63eb113d5c2237</td>\n",
              "      <td>South Indian most expected movie kurukshetra D...</td>\n",
              "      <td>South Indian most expected movie kurukshetra D...</td>\n",
              "      <td>id</td>\n",
              "      <td>-212.293078</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4951</th>\n",
              "      <td>610d02269a63eb113d5e1bf8</td>\n",
              "      <td>Best inspired movie 2018\\n\\\"I Never fear Lose ...</td>\n",
              "      <td>Best inspired movie 2018 \"I Never fear Lose bu...</td>\n",
              "      <td>no</td>\n",
              "      <td>-79.404977</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c338559c-d453-450d-9ed3-5f285e4228c9')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-c338559c-d453-450d-9ed3-5f285e4228c9 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-c338559c-d453-450d-9ed3-5f285e4228c9');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 114
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_test = df_test.drop(['imdb_user_review_lang', 'imdb_user_review_lang_valn', 'imdb_user_review' ], axis=1)\n",
        "df_test.head()"
      ],
      "metadata": {
        "id": "QVADZR2jKfJI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "e4ddff6d-3652-4d07-a274-111389d51e24"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                        _id                                      reviews_clean\n",
              "0  610d01fe9a63eb113d2235ac  This is basically a German takeoff on a Sherlo...\n",
              "1  610d01fe9a63eb113d224536  In January of 1924, director Erich von Strohei...\n",
              "2  610d01fe9a63eb113d224d14  Silent movies are not for everyone. Neither ar...\n",
              "3  610d01fe9a63eb113d225f17  It so often happens that some films take the l...\n",
              "4  610d01fe9a63eb113d225f1a  I saw this film for the very first time last w..."
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c3e5a629-c44b-4352-b8e6-f46e2b227f70\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>_id</th>\n",
              "      <th>reviews_clean</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>610d01fe9a63eb113d2235ac</td>\n",
              "      <td>This is basically a German takeoff on a Sherlo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>610d01fe9a63eb113d224536</td>\n",
              "      <td>In January of 1924, director Erich von Strohei...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>610d01fe9a63eb113d224d14</td>\n",
              "      <td>Silent movies are not for everyone. Neither ar...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>610d01fe9a63eb113d225f17</td>\n",
              "      <td>It so often happens that some films take the l...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>610d01fe9a63eb113d225f1a</td>\n",
              "      <td>I saw this film for the very first time last w...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c3e5a629-c44b-4352-b8e6-f46e2b227f70')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-c3e5a629-c44b-4352-b8e6-f46e2b227f70 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-c3e5a629-c44b-4352-b8e6-f46e2b227f70');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 115
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Running the cleaning process\n",
        "df_test[\"reviews_clean\"] = df_test[\"reviews_clean\"].apply(lambda x: datacleaning(x))\n",
        "pd.options.display.max_colwidth = 200\n",
        "df_test.head()"
      ],
      "metadata": {
        "id": "JYdlp2cOKfPn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337
        },
        "outputId": "38560671-f895-4a12-c191-506528bb6c78"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                        _id  \\\n",
              "0  610d01fe9a63eb113d2235ac   \n",
              "1  610d01fe9a63eb113d224536   \n",
              "2  610d01fe9a63eb113d224d14   \n",
              "3  610d01fe9a63eb113d225f17   \n",
              "4  610d01fe9a63eb113d225f1a   \n",
              "\n",
              "                                                                                                                                                                                             reviews_clean  \n",
              "0  this is basically a german takeoff on a sherlock holmes story now if told in a competent way sherlock holmesinspired tales can be entertaining even if of secondary heritage but this abomination is...  \n",
              "1  in january of 1924 director erich von stroheim gathered a small group of friends to view his magnum opus a film which would become legendary this was greed  when the experience was over all there ...  \n",
              "2  silent movies are not for everyone neither are subtitles those brave enough to view a movie with no sound and words that are far and few between should definitely enjoy this silent masterpiece one...  \n",
              "3  it so often happens that some films take the long way to achieve their status of classics and worthy of being studied frame by frame by movie lovers who believe in the power of raw performance and...  \n",
              "4  i saw this film for the very first time last week and was so tremendously captivated by it that i needed to share this rapture the innovative cameraangles the closeups revealing pain and spiritual...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-30a293e6-4ce8-43f5-9cab-fe542de739bb\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>_id</th>\n",
              "      <th>reviews_clean</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>610d01fe9a63eb113d2235ac</td>\n",
              "      <td>this is basically a german takeoff on a sherlock holmes story now if told in a competent way sherlock holmesinspired tales can be entertaining even if of secondary heritage but this abomination is...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>610d01fe9a63eb113d224536</td>\n",
              "      <td>in january of 1924 director erich von stroheim gathered a small group of friends to view his magnum opus a film which would become legendary this was greed  when the experience was over all there ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>610d01fe9a63eb113d224d14</td>\n",
              "      <td>silent movies are not for everyone neither are subtitles those brave enough to view a movie with no sound and words that are far and few between should definitely enjoy this silent masterpiece one...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>610d01fe9a63eb113d225f17</td>\n",
              "      <td>it so often happens that some films take the long way to achieve their status of classics and worthy of being studied frame by frame by movie lovers who believe in the power of raw performance and...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>610d01fe9a63eb113d225f1a</td>\n",
              "      <td>i saw this film for the very first time last week and was so tremendously captivated by it that i needed to share this rapture the innovative cameraangles the closeups revealing pain and spiritual...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-30a293e6-4ce8-43f5-9cab-fe542de739bb')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-30a293e6-4ce8-43f5-9cab-fe542de739bb button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-30a293e6-4ce8-43f5-9cab-fe542de739bb');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 116
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_final=df_test.reviews_clean"
      ],
      "metadata": {
        "id": "OUMXhRgWKfTH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_final = model.predict(np.array(x_final))\n",
        "y_final_labels=np.where(y_final > 0.5, 1,0)\n",
        "\n"
      ],
      "metadata": {
        "id": "GZ6LaCenKfWj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results =[i[0] for i in y_final_labels]\n",
        "df_test['Y'] = results"
      ],
      "metadata": {
        "id": "0ExqaC_1d_LZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_test.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337
        },
        "id": "mdflHfP3je3b",
        "outputId": "ecea2301-3198-4f7d-ca50-c7e3e3d429e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                        _id  \\\n",
              "0  610d01fe9a63eb113d2235ac   \n",
              "1  610d01fe9a63eb113d224536   \n",
              "2  610d01fe9a63eb113d224d14   \n",
              "3  610d01fe9a63eb113d225f17   \n",
              "4  610d01fe9a63eb113d225f1a   \n",
              "\n",
              "                                                                                                                                                                                             reviews_clean  \\\n",
              "0  this is basically a german takeoff on a sherlock holmes story now if told in a competent way sherlock holmesinspired tales can be entertaining even if of secondary heritage but this abomination is...   \n",
              "1  in january of 1924 director erich von stroheim gathered a small group of friends to view his magnum opus a film which would become legendary this was greed  when the experience was over all there ...   \n",
              "2  silent movies are not for everyone neither are subtitles those brave enough to view a movie with no sound and words that are far and few between should definitely enjoy this silent masterpiece one...   \n",
              "3  it so often happens that some films take the long way to achieve their status of classics and worthy of being studied frame by frame by movie lovers who believe in the power of raw performance and...   \n",
              "4  i saw this film for the very first time last week and was so tremendously captivated by it that i needed to share this rapture the innovative cameraangles the closeups revealing pain and spiritual...   \n",
              "\n",
              "   Y  \n",
              "0  1  \n",
              "1  1  \n",
              "2  1  \n",
              "3  1  \n",
              "4  1  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-0d6add51-9c85-41a6-9b68-be82133a594e\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>_id</th>\n",
              "      <th>reviews_clean</th>\n",
              "      <th>Y</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>610d01fe9a63eb113d2235ac</td>\n",
              "      <td>this is basically a german takeoff on a sherlock holmes story now if told in a competent way sherlock holmesinspired tales can be entertaining even if of secondary heritage but this abomination is...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>610d01fe9a63eb113d224536</td>\n",
              "      <td>in january of 1924 director erich von stroheim gathered a small group of friends to view his magnum opus a film which would become legendary this was greed  when the experience was over all there ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>610d01fe9a63eb113d224d14</td>\n",
              "      <td>silent movies are not for everyone neither are subtitles those brave enough to view a movie with no sound and words that are far and few between should definitely enjoy this silent masterpiece one...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>610d01fe9a63eb113d225f17</td>\n",
              "      <td>it so often happens that some films take the long way to achieve their status of classics and worthy of being studied frame by frame by movie lovers who believe in the power of raw performance and...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>610d01fe9a63eb113d225f1a</td>\n",
              "      <td>i saw this film for the very first time last week and was so tremendously captivated by it that i needed to share this rapture the innovative cameraangles the closeups revealing pain and spiritual...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0d6add51-9c85-41a6-9b68-be82133a594e')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-0d6add51-9c85-41a6-9b68-be82133a594e button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-0d6add51-9c85-41a6-9b68-be82133a594e');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 157
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_test.to_csv('/content/drive/MyDrive/MSC/FCP.csv')\n",
        "\n"
      ],
      "metadata": {
        "id": "KwFolq48sEpa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.2 Pytorch"
      ],
      "metadata": {
        "id": "Pl5WQPPMqQAq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import time\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from torch.utils.data.dataset import random_split\n",
        "from torchtext.data.functional import to_map_style_dataset"
      ],
      "metadata": {
        "id": "QHzWqWmNqH1e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x=df_reviews.imdb_user_review\n",
        "y=df_reviews.helpfulness_cat\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(x, \n",
        "                                                    y, \n",
        "                                                    stratify=y,\n",
        "                                                    test_size=0.20, \n",
        "                                                    random_state=0) "
      ],
      "metadata": {
        "id": "c8ciJNYXqWUq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class IMDB_helpful_reviews(Dataset):\n",
        "\n",
        "    def __init__(self,x,y):\n",
        "        \n",
        "        self.x = x\n",
        "        self.y = y\n",
        "       \n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.x)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "\n",
        "        review = self.x.iloc[idx, ]\n",
        "        label= self.y.iloc[idx, ]\n",
        "\n",
        "        #sample = {'label': label, 'review': review}\n",
        "        sample = (label, review)\n",
        "        return sample"
      ],
      "metadata": {
        "id": "mGTZ_qbNqYHN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_set=IMDB_helpful_reviews(X_train,Y_train)\n",
        "test_set=IMDB_helpful_reviews(X_test,Y_test)"
      ],
      "metadata": {
        "id": "NGyQD38iuye5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenization\n",
        "tokenizer = get_tokenizer('basic_english')\n",
        "#train_iter = train_set\n",
        "# apply the tokenizer\n",
        "# --+ custom function\n",
        "def yield_tokens(data_iter):\n",
        "    for _, text in data_iter:\n",
        "        yield tokenizer(text)\n",
        "# --+ deploy the tokenizer to get the vocabulary\n",
        "vocab = build_vocab_from_iterator(yield_tokens(train_set), specials=[\"<unk>\"])\n",
        "vocab.set_default_index(vocab[\"<unk>\"])\n",
        "# tokenize reviews \n",
        "text_pipeline = lambda x: vocab(tokenizer(x))\n",
        "# encode review labels\n",
        "label_pipeline = lambda x: int(x)"
      ],
      "metadata": {
        "id": "AHIKzIMfu1kU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# custom function\n",
        "def collate_batch(batch):\n",
        "    label_list, text_list, offsets = [], [], [0]\n",
        "    for (_label, _text) in batch:\n",
        "         label_list.append(label_pipeline(_label))\n",
        "         processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)\n",
        "         text_list.append(processed_text)\n",
        "         offsets.append(processed_text.size(0))\n",
        "    label_list = torch.tensor(label_list, dtype=torch.int64)\n",
        "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
        "    text_list = torch.cat(text_list)\n",
        "    return label_list.to(device), text_list.to(device), offsets.to(device)\n",
        "# deploy the function\n",
        "dataloader = DataLoader(train_set, batch_size=8, shuffle=False, collate_fn=collate_batch)"
      ],
      "metadata": {
        "id": "b615E99fu4Na"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "# the class associated with the module\n",
        "class TextClassificationModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, num_class):\n",
        "        super(TextClassificationModel, self).__init__()\n",
        "        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=True)\n",
        "        self.fc = nn.Linear(embed_dim, num_class)\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        initrange = 0.5\n",
        "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
        "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
        "        self.fc.bias.data.zero_()\n",
        "\n",
        "    def forward(self, text, offsets):\n",
        "        embedded = self.embedding(text, offsets)\n",
        "        return self.fc(embedded)"
      ],
      "metadata": {
        "id": "zqvBU3YKu6Tg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_class = len(set([label for (label, text) in train_set]))\n",
        "# vocabulary size\n",
        "vocab_size = len(vocab)\n",
        "# ebedding bag size\n",
        "emsize = 64\n",
        "# model instantiation\n",
        "model = TextClassificationModel(vocab_size, emsize, num_class).to(device)\n"
      ],
      "metadata": {
        "id": "aH3LQb8mu8S8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(dataloader):\n",
        "    model.train()\n",
        "    total_acc, total_count = 0, 0\n",
        "    log_interval = 500\n",
        "    start_time = time.time()\n",
        "\n",
        "    for idx, (label, text, offsets) in enumerate(dataloader):\n",
        "        optimizer.zero_grad()\n",
        "        predicted_label = model(text, offsets)\n",
        "        loss = criterion(predicted_label, label)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
        "        optimizer.step()\n",
        "        total_acc += (predicted_label.argmax(1) == label).sum().item()\n",
        "        total_count += label.size(0)\n",
        "        if idx % log_interval == 0 and idx > 0:\n",
        "            elapsed = time.time() - start_time\n",
        "            print('| epoch {:3d} | {:5d}/{:5d} batches '\n",
        "                  '| accuracy {:8.3f}'.format(epoch, idx, len(dataloader),\n",
        "                                              total_acc/total_count))\n",
        "            total_acc, total_count = 0, 0\n",
        "            start_time = time.time()"
      ],
      "metadata": {
        "id": "kS_vSuQtu-a0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# custom function for evaluation\n",
        "def evaluate(dataloader):\n",
        "    model.eval()\n",
        "    total_acc, total_count = 0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for idx, (label, text, offsets) in enumerate(dataloader):\n",
        "            predicted_label = model(text, offsets)\n",
        "            loss = criterion(predicted_label, label)\n",
        "            total_acc += (predicted_label.argmax(1) == label).sum().item()\n",
        "            total_count += label.size(0)\n",
        "    return total_acc/total_count"
      ],
      "metadata": {
        "id": "xQysEdjdvAM7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 10 \n",
        "# --+ learning rate\n",
        "LR = 0.01  \n",
        "# --+ batch size for training\n",
        "BATCH_SIZE = 64 \n",
        "# optimization\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=LR)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)\n",
        "total_accu = None\n",
        "# data handling\n",
        "\n",
        "train_dataset = to_map_style_dataset(train_set)\n",
        "test_dataset = to_map_style_dataset(test_set)\n",
        "num_train = int(len(train_dataset) * 0.95)\n",
        "split_train_, split_valid_ = \\\n",
        "    random_split(train_dataset, [num_train, len(train_dataset) - num_train])\n"
      ],
      "metadata": {
        "id": "4PYawt2EvCaa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataloader = DataLoader(split_train_, batch_size=BATCH_SIZE,\n",
        "                              shuffle=True, collate_fn=collate_batch)\n",
        "valid_dataloader = DataLoader(split_valid_, batch_size=BATCH_SIZE,\n",
        "                              shuffle=True, collate_fn=collate_batch)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE,\n",
        "                             shuffle=True, collate_fn=collate_batch)"
      ],
      "metadata": {
        "id": "MwvyqyCJvEKw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(1, EPOCHS + 1):\n",
        "    epoch_start_time = time.time()\n",
        "    train(train_dataloader)\n",
        "    accu_val = evaluate(valid_dataloader)\n",
        "    if total_accu is not None and total_accu > accu_val:\n",
        "      scheduler.step()\n",
        "    else:\n",
        "       total_accu = accu_val\n",
        "    print('-' * 59)\n",
        "    print('| end of epoch {:3d} | time: {:5.2f}s | '\n",
        "          'valid accuracy {:8.3f} '.format(epoch,\n",
        "                                           time.time() - epoch_start_time,\n",
        "                                           accu_val))\n",
        "    print('-' * 59)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wcbjE_D6vGUC",
        "outputId": "f7f44f32-9c2e-448c-d2f2-c67a3e131dd3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----------------------------------------------------------\n",
            "| end of epoch   1 | time:  3.96s | valid accuracy    0.738 \n",
            "-----------------------------------------------------------\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   2 | time:  1.21s | valid accuracy    0.738 \n",
            "-----------------------------------------------------------\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   3 | time:  1.20s | valid accuracy    0.738 \n",
            "-----------------------------------------------------------\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   4 | time:  1.19s | valid accuracy    0.738 \n",
            "-----------------------------------------------------------\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   5 | time:  1.18s | valid accuracy    0.738 \n",
            "-----------------------------------------------------------\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   6 | time:  1.17s | valid accuracy    0.738 \n",
            "-----------------------------------------------------------\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   7 | time:  1.18s | valid accuracy    0.738 \n",
            "-----------------------------------------------------------\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   8 | time:  1.18s | valid accuracy    0.738 \n",
            "-----------------------------------------------------------\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   9 | time:  1.19s | valid accuracy    0.738 \n",
            "-----------------------------------------------------------\n",
            "-----------------------------------------------------------\n",
            "| end of epoch  10 | time:  1.20s | valid accuracy    0.738 \n",
            "-----------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('Checking the results of test dataset.')\n",
        "accu_test = evaluate(test_dataloader)\n",
        "print('test accuracy {:8.3f}'.format(accu_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cNrxwU8tvNsb",
        "outputId": "5a504cbf-bb06-46d5-8ac8-29048f1b2bfd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checking the results of test dataset.\n",
            "test accuracy    0.764\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The accuracy score of 76% does not show any progression comparing to all previously described models"
      ],
      "metadata": {
        "id": "4L3NBQG34uSw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Further context in classification"
      ],
      "metadata": {
        "id": "w04NJabnp6U0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluating the sentiment for the reviews (unlabelled)\n",
        "!pip install flair\n",
        "from flair.models import TextClassifier\n",
        "from flair.data import Sentence\n",
        "sia = TextClassifier.load('en-sentiment')\n",
        "\n",
        "def flair_prediction(x):\n",
        "  sentence = Sentence(x)\n",
        "  sia.predict(sentence)\n",
        "  score = sentence.labels[0]\n",
        "\n",
        "  if \"POSITIVE\" in str(score):\n",
        "    return \"pos\"\n",
        "\n",
        "  elif \"NEGATIVE\" in str(score):\n",
        "    return \"neg\"\n",
        "\n",
        "  else:\n",
        "    return \"neu\""
      ],
      "metadata": {
        "id": "UvbH0cGBp7yx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "titanicdf = df_reviews.iloc[1448:(1448+19)]\n",
        "godfatherdf = df_reviews[386:(386+15)]\n",
        "df = pd.concat([titanicdf, godfatherdf], ignore_index=True)\n",
        "df[\"sentiment\"] = df[\"reviews_clean\"].apply(flair_prediction)\n",
        "# Manual sentiment classification\n",
        "neg = np.repeat('neg', len(titanicdf[\"reviews_clean\"])-5)\n",
        "pos = np.repeat('pos', 5)\n",
        "neg1 = np.repeat('neg', len(godfatherdf[\"reviews_clean\"])-6)\n",
        "pos1 = np.repeat('pos', 6)\n",
        "df[\"sentiment_manual\"] = np.concatenate((pos, neg, pos1, neg1))\n",
        "# Separating movies\n",
        "ttc = np.repeat('titanic', len(titanicdf))\n",
        "gft = np.repeat('godfather', len(godfatherdf))\n",
        "df[\"Filter\"] = np.repeat(\"Counts\", len(df))\n",
        "df[\"movie\"] = np.concatenate((ttc, gft))\n",
        "df"
      ],
      "metadata": {
        "id": "_MTN8oKJqEP8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pivot_table = pd.pivot_table(df, \n",
        "                             columns='helpfulness_cat', \n",
        "                             index=['movie', 'sentiment_manual'],\n",
        "                             values='Filter',\n",
        "                             aggfunc='count')\n",
        "pivot_table = pivot_table.fillna(\"-\")\n",
        "pivot_table"
      ],
      "metadata": {
        "id": "oIPW624AqHvz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6.  Classification based on our subjective judgement"
      ],
      "metadata": {
        "id": "d3277lNv-CUZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To understand if our team's judgement could overperform the one obtained by the machine, we classified 80 reviews after reading 20 labeled reviews. Then, we measured the corresponding F1 score for our predictions."
      ],
      "metadata": {
        "id": "AAckFVGI-HEF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_human_test=pd.read_csv('test_human.csv')\n",
        "y_human_true=pd.read_csv('test_human_labels.csv')\n",
        "score = f1_score(y_human_test['Your prediction'], y_human_true.helpfulness_cat)\n",
        "score"
      ],
      "metadata": {
        "id": "NQLkVfaf-Gum"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The resulting F1 score is 80%. Hence, we couldn't gain a better F1 score than the one produced by NLP models."
      ],
      "metadata": {
        "id": "LtPMZMet-TBe"
      }
    }
  ]
}